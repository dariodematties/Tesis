\chapter{Introducción}

\section{Motivación}

\subsection{Adquisición del Lenguaje por Humanos}

El lenguaje es el elemento de la cognición por excelencia que nos hace realmente humanos.
Mientras otras especies se comunican con habilidades innatas para producir un número limitado de vocalizaciones significantes--por ejemplo, los bonobos--o aún con sistemas parcialmente adquiridos ontogénicamente--como es el caso de las aves--al día no se conocen otras especies que puedan expresar ideas combinatorialmente--oraciones--con un conjunto limitado de símbolos--los sonidos del habla y las palabras.

Es una habilidad notable en sí misma. Pero lo que la hace aún más notable es el hecho de que se está hallando evidencia de que el dominio de esta habilidad tan compleja se da en infantes cada vez más jóvenes.

En tal sentido, infantes de tan solo 12 meses reportan tener sensibilidad a la gramática necesaria para entender oraciones causales--quién le hizo qué a quién; por ejemplo, el conejito empujó la rana \cite{doi:10.1080/15475441003769411}.

Después de más de 60 años de investigación dentro del desarrollo del lenguaje infantil, el mecanismo que habilita a los infantes a segmentar sílabas desde las cadenas de sonidos que oyen y adquirir la gramática para entender y producir lenguaje configura todavía un enigma.

Una de las primeras explicaciones científicas de la adquisición del lenguaje fue realizada por Skinner \cite{skinner1957verbal}.
Como uno de los pioneros del Behaviorismo, Skinner concibió al desarrollo del lenguaje a través de la influencia del medio.

Él afirmaba que los infantes aprendían el lenguaje basándose en principios de refuerzo comportamentales asociando palabras con sus significados.
Los enunciados correctos son reforzados de manera positiva cuando el infante se da cuenta del valor comunicativo de la palabras y de las frases.

Por ejemplo, cuando el infante dice 'leche' y la madre ríe y le da leche como resultados a su enunciado verbalizado, el infante encontrará dicho resultado gratificante, mejorando el desarrollo del lenguaje del infante \cite{doi:10.1111/j.1460-6984.2011.00086.x}. 

Sin embargo el enfoque de Skinner fue criticado rápidamente por Noam Chomsky, el lingüista más popular hoy. 
En el espíritu de la revolución cognitiva de la década del 1950, Chomsky alegaba que los infantes nunca adquirirían las herramientas necesarias para procesar un número infinito de oraciones si el mecanismo de adquisición del lenguaje dependía sólo de la entrada lingüística.

Consecuentemente Chomsky propuso la teoría de la Gramática Universal: una idea de categorías gramaticales biológicas innatas, tales como una categoría sustantivo y una categoría verbo que facilitan el desarrollo completo del lenguaje en el infante y el posterior procesamiento del lenguaje en adultos.

Se considera que la Gramática Universal contiene toda la información gramatical necesaria para combinar esas categorías, por ejemplo, sustantivos y verbos en las frases.
La tarea del infante es sólo la de aprender las palabras de su lengua \cite{doi:10.1111/j.1460-6984.2011.00086.x}.

Por ejemplo, de acuerdo con la teoría de la Gramática Universal, los infantes conocen instintivamente cómo combinar un sustantivo--por ejemplo chico--y un verbo--comer--dentro de una frase correcta con significado--Un chico come.

Este enfoque Chomskiano \cite{10.2307/j.ctt17kk81z.1} para la adquisición del lenguaje ha inspirado a cientos de expertos a investigar la naturaleza de esas categorías gramaticales asumidas y la investigación aún continúa.

Una o dos décadas después, algunos psico-lingüistas comenzaron a cuestionar la existencia de la Gramática Universal.
Ellos alegaban que categorías como sustantivo y verbo son biológicamente, evolutivamente y psicológicamente implausibles y que dicho campo de estudio necesitaba un enfoque que pudiera explicar procesos de adquisición sin categorías innatas.

Los investigadores comenzaron a sugerir que en vez de tener mecanismos específicos para el procesamiento del lenguaje, los infantes utilizaban principios cognitivos y de aprendizaje generales. 

Mientras los investigadores que abordaban el problema de la adquisición del lenguaje desde la perspectiva de la Gramática Universal bregaban por una productividad completa y temprana, esto es, un conocimiento similar al adulto pero temprano, los investigadores constructivistas opositores bregaban por un proceso de desarrollo más gradual.
Se sugiere que los infantes son sensibles a los patrones en el lenguaje lo cual habilita el proceso de adquisición.

Un ejemplo de este patrón de aprendizaje gradual es la adquisición de la morfología.
Los morfemas son los marcadores gramaticales más pequeños, o unidades, en el lenguaje que modifican las palabras.
En Inglés, los plurales regulares son marcados con una $s$ como morfema--por ejemplo $dog-s$.

Similarmente, la conjugación de verbos en tercera persona del singular en Inglés--she eat+s, a boy kick+s--se marcan con el morfema $-s$.
Se considera que los infantes adquieren los primeros ejemplos de las formas de tercera persona del singular como segmentos frasales completos--Daddy kicks, a girl eats, a dog barks--sin la habilidad de separar los componentes gramaticales más finos.

Cuando el infante oye un número suficiente de ejemplos de una construcción lingüística--como por ejemplo la tercera persona del singular--detectará los patrones en las construcciones escuchadas.
En este caso, el patrón repetido es el marcador $-s$ en esta forma particular verbal.

Como resultado de muchas repeticiones y ejemplos del marcador $-s$ en los diferentes verbos, el infante adquirirá un conocimiento sofisticado de que en Inglés, los verbos deben ser marcados con el morfema $-s$ en la tercera persona del singular \cite{doi:10.1111/j.1460-6984.2011.00086.x,pine_conti-ramsden_joseph_lieven_serratrice_2008,theakston_lieven_2005}.

Abordar la adquisición del lenguaje desde una perspectiva del procesamiento cognitivo general es un enfoque económico de cómo los infantes pueden aprender su primer lenguaje sin un mecanismo bio-lingüista excesivo. 

Sin embargo, la comunidad científica está lejos de hallar una respuesta sólida al problema de la adquisición del lenguaje.
Nuestro entendimiento actual del proceso de desarrollo es aún inmaduro.

Los investigadores de la Gramática Universal están aún tratando de convencer que el lenguaje es una tarea demasiado demandante para ser adquirida sin un equipamiento innato, mientras que los investigadores constructivistas discuten ferozmente acerca de la importancia de la entrada lingüística.

Las mayores preguntas, sin embargo, aún están sin respuesta.
¿Cuál es el proceso exacto que transforma los enunciados infantiles en oraciones gramaticalmente correctas como las verbalizadas por adultos?
¿A cuánta información lingüística necesita ser expuesto el infante para alcanzar un estado similar al de un adulto?
¿Bajo qué enfoque se puede explicar la variación entre lenguas y el proceso de adquisición en infantes adquiriendo lenguajes muy diferentes?
El misterio de la adquisición del lenguaje está destinado a mantener el asombro de psicólogos y lingüistas década tras década.











\subsection{Importancia del Estudio de la Corteza Cerebral}

Existen rezones filosóficas y antropológicas para estudiar el neocortex; el entendimiento del neocrtex es en parte
entender qué significa ser humano.
Sin embargo también existen razones computacionales e ingenieriles importantes para estudiar el neocortex.
El entendimiento de los principios de operación del neocortex podría configurar un paso necesario para construir máquinas inteligentes.
La necesidad de estudiar el cerebro se puede discutir desde un punto de vista del \gls{ml}.
En el \gls{ml} lo primero que un investigador hace es construir un modelo adaptativo parametrizado para los datos a ser modelados.
Este modelo inicial, construido en base al conocimiento del dominio tendrá varios parámetro que pueden ser ajustados basandose en los datos para el entrenamiento.
El investigador luego desarrolla algoritmos que pueden adaptar esos parámetros.
Típicamente , los parámetros son ajustados adaptándolos para minimizar un criterio de error \cite{bishop2007,dudaHart1973}.
Un modelo cuyos parámetro se ajustan puede luego ser utilizado para clasificar nuevos patrones,
predecir futuros patrones, comprimir, reconstruir, etc.
Sin embargo, independientemente de su elegancia matemática, este método esconde algunas dificultades fundamentales asociadas al aprendizaje.
Un aspecto del aprendizaje es la complejidad de la muestra--es decir, el número de ejemplos requeridos para entrenar un modelo
a un nivel de desempeño rasonable.
Si el espacio de hipótesis para un problema de aprendizaje es muy grande,
entonces, la construcción de un modelo entrenado puede tomar un gran número de ejemplos de entrenamiento
y mucho tiempo para ser entrenado.

El teorema de \gls{nfl}, que plantea un dilema fundamental en búsqueda y optimización,
aborda aspectos acerca de la complejidad del aprendizaje \cite{RePEc:eee:renene:v:134:y:2019:i:c:p:1295-1306}.
Intuitivamente el teorema de \gls{nfl} para el aprendizaje alega que 
no existe un algoritmo de aprendizaje que presente una superioridad inherente con respecto a otro algoritmo
para todos los preblemas de aprendizaje.
Si un algoritmo es superior para un probema particular es sólo porque dicho algoritmo explota
supuestos que son adecuados para dicho problema.
El mismo algoritmo no se desmpeñará bien en un problema que tiene diferentes supuestos
a los del problema original.
Esto significa que para construir algoritmos efectivos, el investigador en \gls{ml}
tiene que codificar el conocimiento acerca del dominio del problema dentro de la estructura del modelo inicial.
Mientras más conocimiento previo es introducido, es mismo es adquirido más rápido por el modelo.
Pero, ¿esto en realidad significa que necesitamos crear un nuevo modelo para cada nuevo problema
que intentamos resolver utilizando \gls{ml}?
Claramente, dicha estrategia sería excesivamente humano dependiente.

Por otro lado, los humános y otros mamíferos parecen resolver todos estos problemas de manera diferente.
El hecho de que los humanos puedan aprender y adaptarse a problemas que no existían
cuando el modelo inicial--el neocortex--fue freado por la selección natural
es la pruaba de cierta naturaleza genérica de los mecanismos utilizados por el cerebro humano.
Asimismo, un gran número de investigadores ahora conjeturan que el neocortex está utilizando fundamentalmente el mismo algoritmo
para aprender modelidades diferentes.
Esto significa que el mismo algoritmo está en operación como modelo de aprendizaje para la audición, la visión, la percepción somatosensorial
y el lenguaje.

Aunque todavía no existe evidencia concluyente, hay una cantidad cada vez mayor de evidencia que soporta un algoritmo cortical comun.
La existencia de un algoritmo cortical comun fue primeramente conjeturada po un nerucientífico que estudió el tejido cortical de diferentes mamífersos \cite{mountcastle_1978}.
La similitud sorprendente de la conectividad cortical a través de modalidades diferentes
y aún entre especies diferentes fue la razón para esta conjetura.
Resultados experimentales en hurones proveen apoyo adicional a esta teoría \cite{von2000visual}.
En estos experimentos, las fibras del nervio óptico in el urón se re-rutearon hacia el área auditiva.
Con exposición a patrones visuales el área auditiva del urón desarrolló campos receptivos visuales
y pudo demostrar comportamiento utilizando esta \emph{audio-visión}.
Estudios de sustitución sensorial donde la impresión de patrones visuales en la modalidad somatosensorial
da origen a percepción visual también respaldan la existencia de un algoritmo cortical común \cite{doi:10.1196/annals.1305.006,BachyRita2003SensorySA}.
La combinación de un algoritmo cortical común y del teorema \gls{nfl} produce implicancias importantes para el \gls{ml}
y también para nuestra necesidad de construir máquinas inteligentes.
Superficialmente, el teorema \gls{nfl} parece traerle problemas a la idea de un algoritmo cortical común.
¿Cómo un algoritmo puede desempeñarse tan bien en tareas tan disímiles como
visión, audición y lenguaje?
La respuesta viene desde la parte del teorema \gls{nfl} que habla acerca de los supuestos que se necesitan explotar.
EL algoritmo cortical común y el teorema \gls{nfl} pueden ser consistentes el uno con el otro
debido a que podemos explotar los mismos supuestos básicos para aprender visión, audición,
lenguaje, etc.
Si la corteza es buena aprendiendo una amplia variedad de tareas utilizando un mecanismo común,
debe de haber algo en común entre todas estas tareas--aparentemente diferentes.
La evolución debe haber descubierto estos razgos comunes y el neocortex debe estar esplotándolos. 

Desde el teorema \gls{nfl} concluimos que una teoría universal de aprendizaje es,
en esencia, una teoría universal de supuestos.
El conjunto de supuestos que un aprendiz utiliza para predecir las salidas, dadas ciertas entradas
que este aún no ha recibido se conoce como el sesgo inductivo del algoritmo de aprendizaje \cite{Mitchell80theneed,10.1007/BF00993472}.
Mientras más supuestos se adoptan, más fácil es aprender.
Sin embargo, mientras más supuestos se adoptan, también se reduce más el número de problemas que podemos resolver.
Si necesitamos diseñar un algoritmo que se pueda aplicar a un gran espectro con una amplia clase de problemas,
la pregunta que necesitamos hacernos es:
¿Cuál es el conjunto de supuesto básicos que son suficientemente específicos para que el aprendizaje sea posible
en una cantidad razonable de tiempo mientras que sea suficientemente general para ser aplicable a una
amplia clase de problemas?
Tendremos suerte si el mismo conjunto de supuestos funciona para una amplia clase de problemas.

Todo esto nos trae denuevo a la pregunta formulada al principio de esta sección.
Debemos estudiar la corteza ya que necesitamos hallar el conjunto preciso de supuestos
acerca del mundo que deben ser codificados en nuestros algoritmos de aprendizaje.
Primero, el neocortex consittuye una prueba en acción de que tal conjunto de supuestos esiste.
Segundo, estudiando el neocortex de manera apropiada nos dará más información sobre dichos supuestos.





















\section{Objetivos}

El cerebro humano es el objeto más complejo creado por al evolución en el universo conocido.
Sin embargo, ¿cuánta de esa complejidad está dedicada exclusivamente a sustentar sus capacidades algorítmicas
y cuanta ha sido heredada desde rutas biológicas evolutivas para poder funcionar de manera apropiada en su
entorno físico?
¿Qué pasaría si las propiedades de procesamiento de la información del cerebro se pudieran reducir a unas pocas
reglas--quizás columnares--repetidas a través de toda la corteza?

Realizar la ingeniería inversa de la corteza constituye un problema de larga data.
¿Cómo deberíamos buscar estos supuestos universales?
Existen detalles anatómicos y fisiológicos en el cerebro; ¿cómo saber qué es importante y qué no lo es?
¿Cómo detectar lo que es un mero detalle de implementación que la biología tiene que emplear ya que esta sólo cuenta con neuronas
para realizar su trabajo, y lo que es un principio computacional importante que no se debería soslayar?
Una buena estrategia sería la de etudiar el neocortex y el entorno--el mundo que lo rodea--al mismo tiempo.
Deberíamos de hallar principios generales que se perfilen como relevantes desde el punto de vista del aprendizaje.
Deberíamos seleccionar sólo aquellos principios para los cuales podamos hallar contrapartes en el entorno.

En nuestra investigación tratamos de hallar tales principios por medio de la elaboración de modelos computacionales inspirados en la corteza de los mamíferos.
Si una propiedad de organización en el neocortex va a la par con cierta propiedad de organización del entorno, podemos contar con cierta certesa
de que hemos hallado un principio que es relevante desde el punto de vista del aprendizaje.

Por una parte, mientras más simples sean los principios o reglas halladas, más repeticiones de tales reglas necesitarán ser implementadas
para conseguir comportamientos interesantes--similares a los evidenciados por humanos--desde nuestros modelos--tales como invarianza en la percepción o la adquisición de fenómenos cognitivos emergentes más abstractos.
El cerebro de los mamíferos--con sus estados combinatorios posibles que ván más allá del número de átomos en
el universo conocido--nos provee con indicios categóricos a favor de tal posición.
Por otro lado, a mayor número de repeticiones de aquellas reglas simples, mayor cantidad de recursos computacionales
se necesitan para implementar los modelos.



En tal sentido creemos que la  \glsfirst{hpc} recibirá contribuciones importantes a partir de trabajos con características similares a las aquí desarrolladas.
Más allá del gran progreso evidenciado por la \gls{hpc} en los últimos años, lo cierto es que las supercomputadoras más poderosas en el mundo
siguen basándose en los mismos principios arquitectónicos descriptos por John Von Neumann en 1945 \cite{10.1109/85.238389}.
Aunque con sus limitaciones, el \glsfirst{dl}--por ejemplo--ha mostrado logros incomparables
que sugieren rutas alternativas en el futuro del diseño de la arquitectura para las nuevas computadoras.
Investigadores de renombre alrededor del mundo advierten sobre futuros cambios en la arquitectura de los
sistemas de \gls{hpc} a los fines de correr algoritmos de \gls{ai} y de \gls{dl} de manera más eficiente.

Sin embargo, en relación a las limitaciones exchibidas por el \gls{dl} existe motivación en prestar más atención
a ciertas propiedades claves del tejido cortical para el diseño de nuevas arquitecturas
basadas en la corteza cerebral de los mamíferos.
De hecho, algunas organizaciónes han tomado estas ideas y las están incorporando de manera parsimoniosa.
Companías como IBM y organizaciones federales como DARPA implementan estas ideas en silicio,
allanando el camino para los procesadores neuromórficos que procesan información 
construyendo representaciones de patrones y realizando predicciones casi en tiempo real.




Este trabajo comparte con tal línea de investigación la idea de que para
obtener máquinas que realmente reflejen la inteligencia humana no es necesario
copiar toda la complejidad del cerebro; sólo se necesita agrupar
los rasgos neuro-fisiológicos clasves para el procesamiento de la información en el tejido cortical.




En tal sentido, emprendemos este desafío tratando de replicar ciertas capacidades de adquisición del lenguaje en infantes.





\subsection{Objetivos Generales}

En este trabajo nos proponemos detectar y agrupar características neurofisiológicas relevantes teneiendo en cuenta un entorno configurado por rasgos restringidos de la adquisición del lenguaje en humanos.
A tales fines utilizamos modelos computacionales bio-inspirados cuyas hipótesis estan basadas en los principios fisiológicos receptados como relevantes para el entorno considerado en el trabajo.
El objetivo general es el de impulsar el desarrollo de máquinas inteligentes que reflejen más fielmente el comportamiento humano sin utilizar hipótesis computacionales que se encuentren alejadas de la neurofisiología y anatomía cerebral.





\subsection{Objetivos Específicos}

Las teorías computacionales en el \glsfirst{ml} en general--y los enfoques de \glsfirst{dl} en particular--no toman en cuenta algunos hechos neurofsiológicos persuacivos descubiertos recientemente.
Algunos de estos elementos incluyen las activaciones \gls{nmda} de ramas dendríticas independientes producidas por la excitación de cierta cantidad de sinapsis distales, la adaptación a estímulos contextuales en el tejido cortical para mejorar la eficiencia en la codificación de información sensorial, y el hecho de que el cerebro utiliza \gls{sdr_pl} para procesar la información.

Más allá de su gran éxito \cite{lecun_deep_2015}, los enfoques tradicionales en el \gls{ml} podrían estar alcanzando una meceta difícil de revertir.
Si bien los algoritmos de dichos enfoque avanzan día tras día de manera vertiginosa, se observa una clara tendencia que atribuye cada vez más importancia al volumen de los datos más allá del tipo de algoritmo a utilizar \cite{PPR:PPR91722}.
Creemos que métodos alternativos más inspirados en la neurofisiología actual deberían ser desarrollados para obtener mejores técnicas computacionales de aprendizaje.

Aquí, nos proponemos tomar una ruta alternativa al \gls{ml} tradicional abordando dichos inconvenientes por medio de la observación de las propiedades dinámicas y estructurales de la corteza de los mamíferos.


\subsubsection{Adquisición de Fonética en Dinámica Cortical}

Consideramos importante investigar los rasgos fisiológicos y anatómicos en el tejido cortical,
basándonos en el hecho de que (i) los humanos y varias especies animales pueden clasificar unidades fonéticas de manera robusta independientemente de las perturbaciones importantes sufridas por los estímulos; y (ii) la activación cortical en algunos animales revela un ajuste espectro-temporal suficientemente fuerte como para sostener discriminación fonética, aún cuando los estímulos son distorcionados por ruido aditivo y reverberación.

A tal efecto, en nuestro primer trabajo nos proponemos identificar propiedades neurofisiológicas y anatómicas precisas en el cerebro de los mamíferos que consideramos potencialmente relevantes para la adquisición de las reglas fonotácticas del lenguaje.
Proponemos desarrollar un modelo computacional que incorpore tales propiedades, las que deberemos testear en términos de invarianza y generalización fonética contra ruido adiferentes tipos de perturbaciones.

Los resultados a presentar en este trabajo deberían sugerir más investigación a los efectos de entender y modelar las características corticales en aplicaciones de adquisición de fonética.




\subsubsection{Escalabilidad de Alta Gama en Modelos Biológicamente Inspirados}

Nos proponemos desarrollar un enfoque relevante hacia el desarrollo de estrategias de paralelización masivas enfocadas en la implementación de modelos computacionales biológicamente inspirados que incluyan perfiles de conectividad altamente dispersos y aleatorios--similares a los hallados en la corteza de los mamíferos--en equipos de \gls{hpc}.
Se medirá la eficiencia en escalabilidad en nuestros modelos, lo que nos permitirá augurar una utilización eficiente de recursos computacionales con un gran número de núcleos por nodo \footnote{\url{https://www.alcf.anl.gov/support-center/cooley/cooley-system-overview}}.
En este trabajo nos proponemos mostrar una estratégia viable de paralelización para implementaciones altamente flexibles de modelos de la corteza cerebral en la que la cohalescencia de los datos no configura un requisito \footnote{La coalesencia enel acceso a la memoria hace referencia a combinar multiples accesos a la misma dentro de una única transacción}.
También nos proponemos mostrar la versatilidad de este esquema de paralelización en un modelo computacional--inspirado en el cerebro--de la dinámica cortical con un esquema de conección altamente disperso y aleatório.
Por lo tanto, esperamos que esta estratégia de paralelización pueda resultar en un enfoque para implementaciones computacionales novedosas, con más plausibilidad biológica.




\subsubsection{Categorías Gramaticales Emergentes en Dinámica Cortical}

Finalmente proponemos traer nuevos modelos computacionales bio-inspirados para el procesamiento de aspectos más abstractos del lenguaje humano,
lo que configura un gran desafío en el \gls{ml}.
Desarrollaremos un modelo computacional que también introduzca hipótesis pertinentes acerca de la dinámica cortical.
Respaldamos nuestras hipótesis citando investigación neurofisiológica relevante y situando nuestras postulaciones entro de la literatura psico-lingüística actual.
Compararemos nuestros algoritmos con aquellos utilizados en otros modelos y métodos psico-lingüísticos comparables, algunos de los cuales aún empleando enfoques de \gls{ml} predominantes. 

Proponemos testear nuestro modelo en taréas de clasificación de categorías gramaticales para palabras individuales en un contexto oracional y demostrar como sus activaciones dispersas bio-inspiradas se apoyan en representaciones de \gls{ds} (\emph{Text Embedding}) para conseguir su desempeño.
De esta forma, nos proponemos desarrollar una investigación que promueva aplicaciones de \gls{ml} mas inspiradas en el cerebro para el \gls{nlp}.
En tal sentido este trabajo también podría proveer una validación computacional para teorías psico-lingüísticas del procesamiento del lenguaje.
Dicha validación deberá estar inspirada en datos neurofisiológicos desde la corteza de los mamíferos.




















%\section{Motivación y Objetivos}

%Evidencia comportamental muestra que humanos y otros animales pueden clasificar fonemas como así también otras unidades lingüísticas de manera categórica. También pueden generalizar en presencia de la variabilidad impuesta por diferentes hablantes con diferentes tonos de voz y prosodia, aún en ambientes ruidosos y con reverberación~\cite{kuhl_1975, kuhl_1983, kluender_1998, pons_2006, hienz_1996, dent_1997, lotto_1997}.

%%Behavioral evidence shows that humans and other animals can classify phonemes as well as other linguistic units categorically. They can also generalize in the presence of the variability imposed by different speakers with different pitches and prosody, even in noisy and reverberant environments~\cite{kuhl_1975, kuhl_1983, kluender_1998, pons_2006, hienz_1996, dent_1997, lotto_1997}.

%Evidencia neurofisiológica sostiene tal comportamiento revelando la existencia de respuestas espectro-temporales en \gls{a1} en hurones. Las activaciones corticales medidas en tales animales sostienen la discriminación fonética~\cite{mesgarani_2008}, aún cuando el estímulo se distorsiona con ruido aditivo y reverberación \cite{mesgarani_2014A}.

%%Neurophysiological evidence support such behavior revealing the existence of spectro-temporal tuning in naive ferrets' \gls{a1}. Cortical activations measured in such animals support phonetic discrimination~\cite{mesgarani_2008}, even when stimuli is distorted by additive noise and reverberation \cite{mesgarani_2014A}.

%%Which are the neuroanatomical and neurophysiological features in cortical tissue that support such evidence? Although many computational theories have been developed, they only explain relevant aspects of phonetic acquisition~\cite{rasanen_2012}. Others are mainly proposed to improve audio classification accuracy using \glspl{cdbn} \cite{Lee:2009:UFL:2984093.2984217} and \glspl{dmn}~\cite{silos_2016}. Such approaches neglect important biological properties recently found in cortical tissue.

%%Instead, we implemented a computational theory which incorporates specific properties present in the mammalian cortex. We foresee such properties as relevant for the acquisition of the sequential phonotactic rules of a language. 

%Para entender como se adquieren las distintas categorías fonéticas como así también las unidades fonotácticas similares a palabras se han desarrollado diversas teorías computacionales \cite{rasanen_2012}. En el contexto de tales teorías, la idea principal ha sido la de explicar aspectos relevantes de la adquisición fonética apelando al uso de la ingeniería y la experiencia humana de expertos y profesionales en la materia. De todas formas, dada la gran cantidad de variables interrelacionadas en los procesos de categorización fonética, algunos fenómenos como la \emph{ausencia de invarianza} en la percepción del habla \cite{appelbaum_1996} parecen posicionarse como uno de esos problemas científicos de resolución imposible para el razonamiento humano espontáneo. En este sentido, las arquitecturas de aprendizaje profundo--dentro del aprendizaje de representaciones--han mostrado niveles sin precedente en la asistencia a técnicas convencionales de aprendizaje supervisado de máquinas, las que por décadas se han valido de altos niveles de ingeniería y de dominio experto para obtener diseños efectivos en la extracción de características desde los datos crudos de entrada \cite{lecun_2015}.

%%To understand how phonetic categories and word-like units are acquired, several computational theories have been developed \cite{rasanen_2012}. In the context of such theories, the main idea has been to explain relevant aspects of phonetic acquisition by means of human engineered features. Nevertheless, \emph{lack of invariance} phenomenon in speech perception \cite{appelbaum_1996} seems to be one of those scientific problems which cannot be solved by spontaneous human reasoning, given the immense amount of interrelated variables involved in phonetic categorization processes. In that sense, \emph{deep learning} architectures--as a subfield inside of \emph{representation learning}--have shown unprecedented performance in the assistance of conventional machine learning techniques, which for decades  required careful engineering in order to reach an effective feature extraction design \cite{lecun_2015}.


%\subsection{Características Diseñadas por Humanos}

%Entre las teorías computacionales desarrolladas para entender la adquisición fonética en humanos, algunos modelos pasan por alto el procesamiento inicial de la señal del habla y, en lugar de lidiar con la complejidad y variabilidad de la señal de habla real, utilizan representaciones pre-léxicas discretas e idealizadas de la señal acústica como entrada al nivel léxico \cite{scharenborg_2010}. Dichas representaciones son diseñadas artificialmente por medio de la intervención humana. Aunque en otros trabajos se hacen algunas observaciones a nivel biológico \cite{dominey_2000}, las componentes de entrada no son más que representaciones silábicas extraídas de corpus específicos. 

%%Among the computational theories developed to understand human phonetic acquisition, some models bypass the initial speech signal processing and, instead of dealing with the complexity and variability of real speech at the prelexical level, they use an artificial, often hand-crafted,  idealized discrete (prelexical) representation of the acoustic signal as an input to the lexical level \cite{scharenborg_2010}. In other works \cite{dominey_2000}, although some biological observations are made, the input components are syllable representations from specific corpora.

%En los trabajos de Boer y Kuhl \cite{boer_2003} y Vallabha, McLelland, Pons, Werker y Amano \cite{vallabha_2007}, los modelos clasifican algunas vocales por medio de mecanismos estadísticos que toman en consideración componentes formantes y \gls{vl}. En Toscano y McMurray \cite{toscano_2010}, se utilizan métodos estadísticos para clasificar características fonéticas consonánticas, por medio de \gls{vot}, \gls{vl}, tono y primera frecuencia de inicio del formante.

%%In the works by de Boer and Kuhl \cite{boer_2003} and Vallabha, McLelland, Pons, Werker and Amano \cite{vallabha_2007}, the models classify some vowels through statistical mechanisms which consider formant components and \gls{vl}. In Toscano and McMurray \cite{toscano_2010}, statistical methods are used to classify consonantal phonetic characteristics, by means of \gls{vot}, \gls{vl}, pitch and first formant onset frequency.

%En Kouki et al. \cite{kouki_2010}, la utilización de \gls{mfcc} supone un flujo de entrada más biológicamente plausible. En un trabajo posterior, Kouki et al. \cite{kouki_2011}, diseñaron un método para separar representaciones “estables” y “dinámicas” desde patrones del habla.

%%In Kouki et al. \cite{kouki_2010}, the use of \gls{mfcc} strategy presupposes a more biologically accurate input stream. In a subsequent work, Kouki et al. \cite{kouki_2011}, designed a method to separate “stable” and “dynamic” speech patterns.

%Los métodos estadísticos utilizados en estos trabajos interrelacionan diferentes características extraídas desde las señales acústicas del habla. Dichas características son sopesadas cuidadosamente por medio de ingeniería y alto nivel de experiencia y dominio humano los que evalúan su relevancia a la hora de incluirlos en las computaciones. Algunas características como \gls{vl} y \gls{vot} son altamente dinámicas y abstractas y se toman como características ampliamente disponibles sin ningún procesamiento neuronal previo. 

%%The statistical methods used in such works make different features extracted from acoustic speech signals interrelate. Those features are carefully weighted by means of human engineering and domain expertise, which evaluate their relevance in order to include them in the computations. Some features, as \gls{vl} and \gls{vot}, refer to highly abstract dynamic characteristics which are taken as available parameters without any previous natural processing.

%\subsection{Características Diseñadas por Máquinas}

%La posibilidad de que existan otras características que puedan escapar a la experiencia humana más el hecho de que algunas características ocultas podrían ser partes constitutivas de aquellas características de alta abstracción evaluadas por humanos permitió el advenimiento de los distintos enfoques del aprendizaje profundo. Dichos enfoques han ganado un extraordinario interés como medio para la construcción automática y jerárquica de de representaciones extraídas desde datos tanto etiquetados como no etiquetados.  

%%The possibility of the existence of other features which can escape from human expertise and the fact that some hidden features could be a constituent part of those abstract features evaluated by humans allowed the advent of deep learning approaches which have fairly gained outstanding interest as a way of building hierarchical representations from unlabeled and labeled data.

%En referencia al aprendizaje supervisado en múltiples capas, tales arquitecturas pueden ser entrenadas a través de descenso de gradiente estocástico, y las mismas funcionan sorprendentemente bien. Esto fue descubierto de manera independiente por diferentes grupos durante las décadas de 1970 y 1980 \cite{noauthor_9_nodate,noauthor_parker_nodate,lecun_procedure_1985,rumelhart_learning_1986}.  

%%Referring to multilayer supervised learning, the fact that such architectures can be trained by simple stochastic gradient descent, and that they work remarkably well, was discovered independently by different groups during the 1970s and 1980s \cite{noauthor_9_nodate,noauthor_parker_nodate,lecun_procedure_1985,rumelhart_learning_1986}.

%Sin embargo se tuvo que esperar hasta el año 2006 para que dichas arquitecturas tuvieran su primer impacto de aceptación \cite{hinton_what_2005,hinton_fast_2006,bengio_greedy_2006,ranzato_efficient_2006} cuando un grupo de investigadores introdujo procedimientos de aprendizaje profundo no-supervisados en los que inicialmente se pre-entrenaba una red para finalmente ajustar la misma por medio de algoritmos backpropagation \cite{bengio_greedy_2006,ranzato_efficient_2006,hinton_reducing_2006}. Tales procedimientos rompieron marcas en bancos de prueba estandardizados como parte constitutiva en un sistema completo para reconocimiento de habla para clasificación en vocabularios pequeños \cite{mohamed_acoustic_2012} y grandes \cite{dahl_context-dependent_2012}.

%%However, a first widespread acceptance hit of these architectures had to wait until 2006 \cite{hinton_what_2005,hinton_fast_2006,bengio_greedy_2006,ranzato_efficient_2006} when a group of researchers introduced deep unsupervised learning procedures which initially pre-train the network for finally fine-tune it by means of standard backpropagation \cite{bengio_greedy_2006,ranzato_efficient_2006,hinton_reducing_2006}. It returned record-breaking results--as a constituent part of a complete system--on a standard speech recognition benchmark for small \cite{mohamed_acoustic_2012} and then for large \cite{dahl_context-dependent_2012} vocabulary tasks.

%Más allá del éxito obtenido en la aplicación de \glspl{cnn}--desde el año 2000--a reconocimiento de señales de tránsito \cite{ciresan_multi-column_2012}, segmentación de imágenes biológicas \cite{ning_toward_2005,turaga_convolutional_2010}, detección de rostros, texto, peatones y cuerpos humanos en imágenes naturales \cite{sermanet_pedestrian_2012,vaillant_original_1994,nowlan_convolutional_1995,garcia_convolutional_2004,osadchy_synergistic_2007,tompson_efficient_2014}, reconocimiento de rostros \cite{taigman_deepface:_2014} e independientemente de su aplicación exitosa en tecnología incluyendo robots móviles autónomos y vehículos auto-conducidos \cite{hadsell_learning_nodate,farabet_scene_2012} así como entendimiento de lenguaje natural \cite{collobert_natural_2011} y reconocimiento de habla \cite{noauthor_deep_nodate}, tales enfoques tuvieron que esperar hasta el año 2012 para ser ampliamente aceptados por la corriente principal en la comunidad de visión computacional y aprendizaje de máquinas. En la competición ImagNet--celebrada ese año--las redes neuronales convolucionales alcanzaron resultados sorprendentes, casi duplicando las performances de sus mejores rivales \cite{krizhevsky_imagenet_2012}.

%%Despite of the success in the application of \glspl{cnn}--since 2000--to  traffic sign recognition \cite{ciresan_multi-column_2012}, segmentation of biological images \cite{ning_toward_2005,turaga_convolutional_2010}, detection of faces, text, pedestrians and human bodies in natural images \cite{sermanet_pedestrian_2012,vaillant_original_1994,nowlan_convolutional_1995,garcia_convolutional_2004,osadchy_synergistic_2007,tompson_efficient_2014}, face recognition  \cite{taigman_deepface:_2014} and despite of its successful applications in technology, including autonomous mobile robots and self-driving cars \cite{hadsell_learning_nodate,farabet_scene_2012} as well as natural language understanding  \cite{collobert_natural_2011} and speech recognition \cite{noauthor_deep_nodate}, those approaches had to wait until 2012 to be widely accepted by the mainstream computer-vision and machine-learning communities, when in the ImageNet competition in 2012 deep convolutional networks achieved spectacular results, almost doubling the accuracy rates respecting to the best competing approaches \cite{krizhevsky_imagenet_2012}.

%En referencia a las representaciones distribuidas en el procesamiento del lenguaje, y en el contexto de corpus grandes extraídos de texto real, se entrenan redes neuronales profundas para predecir la siguiente palabra desde una cadena de palabras teniendo en cuenta el conjunto de palabras más recientes en cierta vecindad dentro de la secuencia. En ciertos casos, la única premisa es tomar en cuenta las palabras previa y siguiente en relación a la palabra que se quiere predecir. Es sorprendente que de tan simple premisas puedan emerger representaciones distribuidas semánticas tan perspicaces en las que los vectores que representan las palabras Jueves y Miércoles--por ejemplo--son muy similares, así como los vectores que representan las palabras Suecia y Noruega \cite{bengio_neural_2003}. Las características semánticas aprendidas por tales redes no las determinan humanos expertos por adelantado, por el contrario, son descubiertas de manera automática por la red neuronal.

%%In reference to distributed representations in language processing, and in the context of very large corpora in real text, deep neural networks are trained to predict the next word from a chain of words by taking into account the most recent words in the neighborhood inside the sequence. Some times the only premise is to take into account the previous and the next words in relation to the word that want to be predicted. It is striking that from too simple premises can arise insightful distributed semantic representations in which the learned word vectors for Tuesday and Wednesday--for example--are very similar, as are the word vectors for Sweden and Norway  \cite{bengio_neural_2003}. Semantic features learned by such networks are not determined by experts in advance, instead, they are automatically discovered by the neural network.

%Mecanismos muy sencillos implementados por \glspl{rnn} para la traducción de oraciones y la descripción automática de imágenes generan serias dudas en cuanto a la necesidad de expresiones simbólicas internas manipuladas por reglas de inferencia. En tal sentido, se resalta el hecho de que el razonamiento cotidiano podría involucrar una gran cantidad de analogía simultáneas, cada una contribuyendo con cierta medida de plausibilidad a la conclusión \cite{noauthor_metaphors_nodate,rogers_precis_2008}.

%%Very simple mechanisms carried out by \glspl{rnn} for sentence translation and automatic image caption generation raised incriminating doubts about the need of internal symbolic expressions manipulated by inference rules. In that sense, it is highlighted that everyday reasoning might involve many simultaneous analogies, each contributing with plausibility to a conclusion \cite{noauthor_metaphors_nodate,rogers_precis_2008}.

%\gls{lstm} es una versión mejorada de la \gls{rnn} convencional que puede ser aplicada en la implementación de sistemas de reconocimiento del habla completos \cite{graves_speech_2013} y que ha sido utilizada también en las etapas de codificación y decodificación para la traducción realizada por máquinas \cite{sutskever_sequence_2014,cho_learning_2014,bahdanau_neural_2014}. Las máquinas de Turing y las redes de memoria son versiones mejoradas de las \gls{lstm} que han sido utilizadas para tareas que normalmente requerirían razonamiento y manipulación simbólica \cite{graves_neural_2014,weston_memory_2014,weston_towards_2015}.

%%\gls{lstm}--an improved version of conventional \glspl{rnn}--can be applied in the implementation of complete speech recognition systems  \cite{graves_speech_2013} and has also been used for the encoder and decoder networks at machine translation  \cite{sutskever_sequence_2014,cho_learning_2014,bahdanau_neural_2014}. Turing machines and memory networks are \gls{lstm} improvements that have been used for tasks that would normally require reasoning and symbol manipulation  \cite{graves_neural_2014,weston_memory_2014,weston_towards_2015}.

%\subsection{Plausibilidad Biológica}

%Aunque las \glspl{cnn} están inspiradas en ciertos fenómenos como los desarrollados por neuronas simples y complejas en la ruta visual y presentan reminiscencias de la jerarquía LGN–V1–V2–V4–IT en la ruta ventral de la corteza visual \cite{felleman_distributed_1991}, las principales líneas de desarrollo de tales herramientas han virtualmente ignorado sus contrapartes biológicas con las que se podría encontrar respaldo explicativo--al menos en parte--a los excepcionales niveles de performance obtenidos por tales mecanismos. En tal sentido y en términos de la enorme influencia generada durante los últimos años por las redes neuronales profundas supervisadas, hay investigadores que sostienen la hipótesis de que las técnicas de backpropagation podrían ser llevadas a cabo por el tejido neuronal \cite{guerguiev_towards_2017}. En dichos trabajos, el problema de asignación de crédito es explicado en términos biológicos implementando una red que--capa por capa--clusteriza información creando representaciones abstractas de categorías de dígitos numéricos. 

%%Even though \glspl{cnn} are inspired by the phenomena developed by simple and complex cells in the visual pathway and is reminiscent of the LGN–V1–V2–V4–IT hierarchy in the visual cortex ventral pathway  \cite{felleman_distributed_1991}, the mainstreams in the development of such tools has virtually ignored biological counterparts as to find support which explains--at least in part--the outstanding performance of such approaches. In that sense,  and in terms of the enormous influence of deep supervised neural networks during the last years, there are researchers who support the hypothesis that backpropagation could be carried out in neural tissue \cite{guerguiev_towards_2017}. In that work, the credit assignment problem is explained in terms of biological plausibility implementing a network which--layer by layer--clusters visual information with increasing abstract representations of digit categories. 

%En otra propuesta \cite{hinton_matrix_2018,sabour_dynamic_2017}, Geoffrey Hinton aborda la plausibilidad biológica con una impronta top-down resaltando algunos inconvenientes importantes en las \glspl{cnn} como la ausencia de relación de posición (traslación y rotación) de características simples que configuran características de más alto nivel y el hecho de que el mecanismo de max pooling provee una invarianza artificialmente forzada a la red, a expensas de información valiosa. Inspirado en como los gráficos de computadora construyen una imagen visual desde sus representaciones jerárquicas internas de datos geométricos, Hinton en su lugar alega que nuestro cerebro ejecuta una especie de proceso de renderizado inverso--gráficos inversos como él les llama--desde la información visual recibida por los ojos. Para Hinton nuestro cerebro extrae una representación jerárquica del mundo y trata de compararla con patrones ya aprendidos y relaciones previamente almacenadas en el cerebro. La idea principal detrás de esta teoría es que la representación de objetos en el cerebro es independiente del ángulo con el que se mire al objeto. De esta manera el cerebro cuenta con una propiedad de invarianza más naturalmente fundamentada. Esta red ha alcanzado performances a niveles de vanguardia en conjuntos de datos simples utilizando tan sólo una fracción de los ejemplos de entrenamiento que necesitaría una \gls{cnn}. En este sentido, la teoría de cápsulas se encuentra mucho más cerca de lo que el cerebro humano realiza en la práctica. Más allá de estos resultados prometedores, la implementación actual de dichas tecnología es más lenta que los modelos de aprendizaje profundo a la hora de ser entrenados. 

%%In another proposal \cite{hinton_matrix_2018,sabour_dynamic_2017}, Geoffrey Hinton tries to tackle biological plausibility in a top-down fashion highlighting important drawbacks in \glspl{cnn} as the lack of pose (translational and rotational) relationship between simpler features that make up a higher level feature and the fact that max pooling provides an artificially forced invariance to the network at expenses of valuable information. Inspired by how computer graphics construct a visual image from its internal hierarchical representation of geometric data, Hinton instead alleges that our brain do kind of a reverse rendering process--inverse graphics as he calls it--from visual information received by eyes. For Hinton, our brain extracts a hierarchical representation of the world and tries to match it with already learned patterns and relationships previously stored in the brain. The main idea behind this theory is that representation of objects in the brain does not depend on view angle and in this way they have a more naturally sourced invariance property. This network has reached state-of-the-art performance in a simple data set using just a fraction of the training examples that would need a \gls{cnn}. In this sense, the capsule theory is much closer to what the human brain does in practice.  Beyond this encouraging outcome, current implementations are much slower than other modern deep learning models in order to be trained.

%Continuando en esta línea de búsqueda de plausibilidad biológica, las entidades biológicas descubren la estructura del mundo sin que ningún agente externo les diga en detalle cómo hacerlo, interactuando con el mismo y recibiendo premios o castigos en el mejor de los casos--aprendizaje reforzado--mientras que en otros casos sin pista alguna más allá de la estructura estadística inherente a los datos de manera completamente no-supervisada. El aprendizaje reforzado parece conformar las condiciones a las cuales los animales se encuentran más frecuentemente expuestos. En tal sentido, para entrenar AlphaGo, una \gls{dnn} \cite{silver_mastering_2016} se utilizó una combinación de aprendizaje supervisado asistido por experiencia humana en juegos y de aprendizaje reforzado desde juegos de auto-juego. Dicha red alcanzó un 99.8\% de tasa de triunfos contra otros programas jugadores de Go, y venció al campeón europeo--jugador humano-- de Go por 5 juegos a 0. Luego, en \cite{silver_mastering_2017} AlphaGo se convirtió en su propio maestro por medio de un algoritmo basado sólo en aprendizaje reforzado. Sin datos provistos por humanos, conocimiento, guía o dominio alguno más allá de las reglas del juego, esta red neuronal fue entrenada para predecir sus propios movimientos como así también los movimientos de los AlphaGo ganadores en juegos progresivos. Esta red neuronal mejora la fuerza en la búsqueda de árbol resultando en una más alta calidad en la selección de movimientos y--por lo tanto--en un jugador más experimentado contra si mismo en cada iteración. Comenzando desde una tabla rasa, el programa Alpha Go Zero alcanzó una performance super-humana, ganando 100-0 contra el programa publicado anteriormente vencedor del campeón de Go. 

%%Biological entities discover the structure of the world without being told how, interacting with it and receiving reward and punishment in the best cases--reinforcement learning--while in other cases learning without any clue beyond the statistical structure in the data itself in a completely unsupervised fashion. Reinforcement learning seems to be the conditions under which animals are most often exposed to.  In that sense, a combination of supervised learning from human expert games, and reinforcement learning from games of self-play was used to train AlphaGo, a \glspl{dnn} \cite{silver_mastering_2016} that achieved a 99.8\% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. Then, in \cite{silver_mastering_2017} AlphaGo became its own teacher by means of an algorithm based solely on reinforcement learning. Without human data, guidance or domain knowledge beyond game rules, this neural network was trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in each iteration. Starting tabula rasa, the new program AlphaGo Zero achieved superhuman performance, winning 100–0 against the previously published, champion-defeating AlphaGo.

%Más allá del hecho de que el aprendizaje reforzado se pueda ver como un mecanismo ubicuo para los sistemas biológicos complejos, es importante resaltar que las técnicas de aprendizaje no supervisado conducen los procesos de aprendizaje de agentes biológicos en muchos casos también. En relación con esto, Jeff Hawkins et al. \cite{noauthor_why_nodate} ha desarrollado un enfoque computacional que reúne propiedades biológicas precisas desde el tejido cortical. Dicho trabajo muestra como un modelo biológico detallado de neuronas piramidales con miles de sinapsis y \gls{sdr} puede aprender transiciones de patrones y puede formar una memoria secuencial robusta \cite{noauthor_htm_nodate}. En esta red se propone que tal memoria secuencial podría configurar una propiedad universal de todo el tejido cortical. El mismo grupo se encuentra actualmente desarrollando un enfoque convincente \cite{noauthor_theory_nodate} por medio de un modelo de red que aprende la estructura de objetos a través de movimiento. En tal investigación se prevé que las células de grilla podrían conducir la adquisición de la estructura del mudo externo en la corteza de mamíferos.

%%Beyond the fact that reinforcement learning could be thought as an ubiquitous mechanism in complex biological systems, it is important to highlight that unsupervised techniques could lead the learning processes in many cases in biological agents too. Regarding this, Jeff Hawkins et al. \cite{noauthor_why_nodate} have developed a computational approach which gather precise biological properties from cortical tissue. This work shows how a biologically detailed model of pyramidal neurons with thousands of synapses and \glspl{sdr} can learn transitions of patterns and can form a powerful and robust sequence memory \cite{noauthor_htm_nodate}. They propose that such form of sequence memory could be a universal property of all neocortical tissue. The same group is currently developing a compelling approach \cite{noauthor_theory_nodate} by means of a network model that learns the structure of objects through movement. In such research it is foreseen that grid cells could lead that acquisition of the structure of the world in the mammalian cortex. 

%\subsection{Evidencia Comportamental de Aprendizaje Incidental en Humanos}

%Una tarea fundamental para la adquisición del lenguaje, como lo es la segmentación de palabras desde el flujo acústico del habla, la pueden llevar a cabo infantes de 8 meses de edad basándose sólo en las relaciones estadísticas entre sonidos contiguos del habla. Es más, dicha segmentación de palabras se basa en aprendizaje estadístico de tan sólo 2 minutos de exposición, esto sugiere que los infantes tienen acceso a mecanismos poderosos para la computación de las propiedades estadísticas presentes en la fonética del lenguaje de entrada\cite{saffran_statistical_1996}. En este caso los infantes recibieron estímulos acústicos continuos sin pausa ni prosodia alguna de la que se pudiera inferir los límites de las palabras. La única pista fue que la probabilidad transicional entre sílabas de una misma palabra es mayor que entre sílabas pertenecientes a palabras distintas. Con tan sólo eso, los infantes fueron capaces de extraer la reglas fonotácticas de las palabras y así demostraron capacidades muy potentes de extracción fonética más allá de la dimensión gramatical y/o semántica del lenguaje humano. Dichas dimensiones estuvieron completamente ausentes para los infantes durante las pruebas, en donde los mismos adquirieron las restricciones fonotácticas del lenguaje de manera incidental.

%Nuestro objetivo principal es probar la capacidad de un modelo computacional que reúne propiedades neurofisiologicas y anatómicas precisas para obtener abstracción fonética de manera completamente no supervisada. Este es un punto fundamental que demuestra la plausibilidad biológica de nuestra implementación debido a que las restricciones fonotácticas de un lenguaje humano son adquiridas de manera incidental \cite{BRENT199693,saffran_1997} y por lo tanto--bajo tales circunstancias--no se podría aceptar supervisión alguna. Nuestro modelo computacional está inspirado en la biología de la corteza de los mamíferos pero el mismo no trata de reflejar fielmente su complejidad biológica. De la misma manera en que no es necesario que un avión aletee para poder volar, ni es necesario copiar meticulosamente la biología de un ave para crear un sistema artificial altamente eficiente desde el punto de vista aerodinámico, nosotros creemos que no es necesario copiar la complejidad biológica del cerebro para crear un sistema artificial inteligente. Basta sólo con recoger aquellos aspectos neurofisiológicos y anatómicos relevantes para el procesamiento de información en el mismo.





\section{Aportes}

Comenzamos nuestro trabajo con la adquisición temprana de reglas fonotácticas, específicas a un lenguaje particular \cite{10.1371/journal.pone.0217966}.
Implementamos un modelo neurocomputacional inspirado en la biología de la corteza que nos permite testear hipótesis neurofisiologicas incorporadas en los algoritmos.
Luego, dada la necesidad de la utilización de recursos de \gls{hpc} demandados por estos modelos, analizamos la eficiencia en escalabilidad paralela de los mismos \cite{}.
Finalmente, basados en la uniformidad de la corteza cerebral de los mamiferos, aplicamos principios computacionales similares al estudio de adquisición de propiedades cognitivas más abstractas como la adquisición de ciertos aspectos gramaticales del lenguaje desde la confluencia de información de \glsfirst{ds} y sintaxis básica ayudada desde la fonología \cite{}.

\subsection{Adquisición Fonética en Dinámica Cortical, Un Enfoque Computacional}

Muchas teorías computacionales han sido desarrolladas para mejorar el desempeño en clasificación fonética desde flujos auditivos lingüísticos.
Sin embargo creemos que no se le ha prestado la atención suficiente a los datos psico-lingüísticos ni a las propiedades neurofisiológicas halladas en el tejido cortical.
Nos basamos en un contexto en el que ciertas unidades lingüísticas básicas--como los fonemas--son extraidos y clasificados de manera robusta por humanos y otros animales desde flujos acústicos complejos en el habla.
Este trabajo está específicamente motivado en el hecho de que infantes humanos de 8 meses de edad pueden alcanzar segmentación de palabras desde una cadena de audio fluida 
basándose de manera exclusiva en las relaciones estadísticas entre sonidos colindantes sin ningún tipo de supervisión. 
En este trabajo introducimos un enfoque neurocomputacional biológicamente inspirado no-supervisado que incorpora propiedades corticales neurofisiológicas y anatómicas claves, incluyendo organización columnar, formación microcolumnar espontánea, adaptación a activaciones contextuales y \gls{sdr_pl} producidas por medio de depolarizaciones parciales \gls{nmda}.
Sus capacidades de abstracción de rasgos muestran atributos de invarianza y generalización fonética prometedoras.
Nuestro modelo mejora el desempeño de algoritmos de clasificación de \gls{svm} en taréas de clasificación de palabras monosilábicas, bisilábicas y de más de dos sílabas en presencia de disturbios ambientales como el ruido blanco, la reverberación y las variaciones de tono y voz.
De hecho, nuestro enfoque enfatiza principios corticales potenciales  de auto-organización alcanzando una mejora sin ningún tipo de guia de optimización que pudiera minimizar funciones de pérdida por medio de--por ejemplo--backpropagation.
De esta forma, nuestro modelo computacional supera respresentaciones de rasgos espectro-temporales de múltiple resolución utilizando sólo la estrucutra secuencial estadística inmersa en las reglas fonotácticas del flujo de entrada.




\subsection{Hacia una Escalabilidad de Alta Gama en Modelos Computacionales Biológicamente Inspirados}

El campo interdiciplinario de la neurociencia ha alcanzado un progreso significatico en décadas recientes, proveyendo a la comunidad científica en general con un nuevo nivel de entendimiento en cuanto a cómo funciona el cerebro más allá de los modelos de almacenamiento y disparo (\emph{store-and-fire}) hallados en redes neuronales artificiales tradicionales.
Mientras tanto, el \glsfirst{ml} basado en modelos ya establecidos ha evidenciado un surgimiento del interés por la comunidad de la \glsfirst{hpc}, especialmente a través de la utilización de aceleradores de alta gama como las \glsfirst{gpu_pl}, incluidas en \emph{clusters} para \gls{hpc}. 
En este trabajo, nos motiva explotar dichos desarrollos en \gls{hpc} y entender los desafíos de dados al tratar de escalar nuevos modelos biológicamente inspirados en recursos computacionales de alta gama.
Estos modelos emergentes se caracterizan por tener perfiles de conectividad dispersos y aleatórios que se mapean mejor en arquiecturas paralelas acopladas más holgadamente con un gran número de \gls{cpu_pl} por nodo.
En contraste a los códigos del \gls{ml} tradicional, estos métodos explotan estructuras de datos dispersas, holgadamente vinculadas en oposición a la computación en matrices densas y acopladas de manera rígida, las que se benefician de un paralelismo de estilo \emph{\gls{simd}} hallado en las \gls{gpu_pl}.
En este trabajo introducimos un esquema de paralelización híbrido basado en \emph{\gls{mpi}} y \emph{\gls{omp}} para acelerar y escalar nuestro modelo computacional basado en la dinámica del tejido cortical.
Corremos pruebas computacionales en un \emph{cluster} de alta gama para visualización y análisis en el \gls{anl}.
Incluimos un estudio de escalado fuerte y débil (\emph{strong and weak scaling}) donde obtenemos medidas de eficiencia paralela con un mínimo de 87\% y un máximo de 97\% para simulaciones de nuestra red neuronal biológicamente inspirada en hasta 64 nodos con 8 hilos de ejecución cada uno.
Este estudio muestra al enfoque híbrido \gls{mpi}+\gls{omp} como una estratégia prometedora para sostener esenarios experimentales computacionales flexibles y biológicamente inspirados.
Adicionalmente exponemos la viabilidad en la aplicación de estas estratégias en un futuro en super-computadores de catalogados liderazgo en el ranquing mundial.





\subsection{Una Teoría Computacional para la Emergencia de Categorías Gramaticales en Dinámica Cortical}

Un consenso general en psico-lingüística afirma que la sintáxis y el significado se unifican de manera precisa y muy rápidamente durante el procesamiento oracional en línea.
Aunque muchas teorías han dado argumentos en relación a las bases neurocomputacionales de este fenómeno, creemos que dichas teorías podrían beneficiarse incluyendo datos neurofisiológicos en relación a restricciones desde la dinámica cortical en el tejido cerebral.
Adicionalmente, algunas teorías promueben la integración de métodos de optimización complejos en el tejido cortical.
En este trabajo intentamos subsanar estas brechas introduciendo un modelo computacional inspirado en la dinámica del tejido cortical.
En nuestro enfoque de modelado, las dendritas aferentes próximas producen activaciones celulares estocásticas, mientras que las ramas dendríticas distales--por el otro lado--contribuyen de manera independiente a la depolarización somática por medio de impulsos dendríticos, y finalmente, fallas de predicción producen \gls{mfe_pl} los que comprometen la formación de \gls{sdr_pl}.
El modelo presentado en este trabajo combina restricciones semánticas y sintácticas \emph{de grano grueso} para cada palabra en el contexto oracional hasta que la discriminación de funciones gramaticalmente relacionadas enmergen espontáneamente por la sola correlación de la información léxica desde fuentes diferentes sin la aplicación de métodos de optimización complejos.
Por medio de técnicas de \gls{svm} mostramos que las activaciones dispersas devueltas por nuestro modelo son apropiadas para alcanzar la clasificación de funciones gramaticales de palabras individuales en la oración, posándose en los datos devueltos por mecanismos denominadas \emph{Word Embedding}.
De esta forma desarrollamos una explicación computacional biológicamente guiada para procesos de unificación lingüísticamente relevantes en la corteza que conecta la psico-lingüística con aspecto neurobiológicos del lenguaje.
También podemos decir que las hipótesis computacionales establecidas en esta investigación podrían promover más trabajo en algoritmos de aprendixaje biológicamente inspirados para aplicaciones en el procesamiento del lenguaje natural.







%\begin{itemize}

%\item Implementamos un modelo computacional completamente no supervisado y biológicamente inspirado que incorpora características halladas en la corteza de mamíferos. Nuestro modelo devuelve niveles de clasificación fonética similares a aquellos encontrados en enfoques profundos de clasificación de patrones de primera línea.

%%\item We implemented a completely unsupervised and biologically inspired computational model which incorporates the above mentioned cortical features. Our model returns phonetic classification accuracy levels similar to those of state-of-the-art deep pattern classification approaches.

%\item Implementamos nuestros algoritmos utilizando el estándard \CC14 por medio de contenedores \gls{stl} y el paradigma de orientación a objetos o \gls{oop} en un conjunto de clases interrelacionadas por herencia y composición.

%%\item We implemented our algorithms in standard \CC14 using \gls{stl} containers and the \gls{oop} paradigm in a set of classes interrelated by inheritance and composition.

%\item Paralelizamos el código utilizando un paradigma híbrido \gls{mpi}+\gls{omp} y usamos sistemas de archivo paralelos (\gls{mpi} I/O parallel file system).

%%\item We parallelized the code by means of a hybrid \gls{mpi}+\gls{omp} paradigm and used \gls{mpi} I/O parallel file system.

%\item Implementamos nuestro propio conjunto de librerías para guardar el estado del modelo en formatos de archivo compatibles con Matlab/Octave.
	
%%\item We implemented our own set of libraries in order to save the model status in Matlab/Octave file formats.

%\item Nuestra implementación cuenta con capacidad de reinicio en su etapa de entrenamiento donde existe total flexibilidad en término del número de procesos con los que la aplicación es reiniciada.

%%\item Our implementation has Checkpoint and Restart capacity in its training stage where there is total flexibility in terms of the number of ranks with which the execution is restarted.

%\item Para producir la entrada desde el flujo auditivo, seguimos en líneas generales el modelo desarrollado por Chi T. et al.~\cite{chi_2005}. Implementamos un algoritmo llamado \gls{mrstsa}. Nuestra implementación principalmente se basa en la sección cortical en \cite{chi_2005} en lugar de basarse en su parte subcortical. Implementamos este algoritmo en C y lo paralelizamos por medio de secciones paralelas en \gls{omp}.

%%\item In order to produce the inputs from auditory streams we followed main guidelines from from Chi T. et al.~\cite{chi_2005}. To that end, we implemented an algorithm called \gls{mrstsa}. Our implementation follows primarily the cortical section in~\cite{chi_2005} rather than its sub-cortical counterpart. We implemented the algorithm in C and parallelized it by means of \gls{omp} parallel sections.

%\item Realizamos todos los experimentos en Cooley, un cluster para análisis y visualización provisto por el Laboratorio Nacional de Argonne.

%%\item We performed all computational experiments on Cooley, a visualization and analysis cluster at Argonne National Laboratory.

	%\begin{itemize}
		%\item Corrimos simulaciones para realizar pruebas en ciencia utilizando hasta 25 nodos (un proceso \gls{mpi} por nodo) y 8 hilos \gls{omp} por nodo/proceso. Probamos la capacidad de invarianza en las características fonéticas--en una etapa llamada \gls{el}--en varias tareas de clasificación de palabras.
			
		%%\item We ran science simulations using up to 25 nodes (one \gls{mpi} rank per node) and 8 \gls{omp} threads per node/rank. We tested the phonetic feature invariance capacity of our cortical implementation--in a stage called \gls{el}--in several word classification tasks.
			
		%\item Comparamos el desempeño del \gls{el} con el desempeño de las características devueltas por el algoritmo \gls{mrstsa} por medio de clasificación con \gls{svm} de las características devueltas por cada algoritmo (Fig. \ref{fig:Experiment}).
			
		%%\item We compared the \gls{el} performance with the performance of the features returned by the \gls{mrstsa} algorithm by means of \gls{svm} classification of the features delivered by each algorithm (Fig. \ref{fig:Experiment}).
			
		%\item Corrimos pruebas de desempaño utilizando hasta 12 nodos (un proceso \gls{mpi} por nodo) y 12 hilos \gls{omp} por proceso/nodo. Realizamos pruebas de strong y weak scaling sobre los nodos de Cooley combinando la utilización de marcas temporales y de memoria con Allinea MAP profiler \footnote{\url{https://en.wikipedia.org/wiki/Allinea_MAP}}.

		%%\item We ran performance tests using up to 12 nodes (one \gls{mpi} rank per node) and 12 \gls{omp} threads per node/rank. We performed strong and weak scaling tests on Cooley nodes combining the use of time/memory marks and Allinea MAP profiler \footnote{\url{https://en.wikipedia.org/wiki/Allinea_MAP}}.

	%\end{itemize}

%\end{itemize}



%\section{Declaración de Originalidad}

%%Statement here.


%\section{Publicaciones}

%Para este trabajo se ha enviado un manuscrito \cite{10.1371/journal.pone.0217966} que se encuentra en proceso de revisión.

%%Publications here.
