@misc{audacity,
  title = {Audacity 2.0.5 free, open source, cross-platform software for recording and editing sounds.
Audacity® software is copyright © Audacity Team},
  howpublished = {\url{https://audacityteam.org/}},
  note = {It is free software distributed under the terms of the GNU General Public License.
The name Audacity® is a registered trademark of Dominic Mazzoni.},
  year = {1999-2018}
}



@misc{libsvm,
  title = {libsvm Version 3.22 released on December 22},
  year = {2016}
}



@misc{sable,
  title = {SABLE cross synthesizer standard mark up language},
  howpublished = {\url{https://www.cs.cmu.edu/~awb/festival_demos/sable.html}},
  note = {Accessed: 2018-04-27}
}



@misc{fftw,
  title = {FFTW package},
  howpublished = {\url{http://www.fftw.org/}},
  note = {Accessed: 2018-04-27}
}



@Misc{festival2014,
title = {Festival Speech Synthesis System 2.4: release},
howpublished = {Copyright (C) University of Edinburgh, 1996-2010. All rights reserved.
clunits: Copyright (C) University of Edinburgh and CMU 1997-2010
clustergen\_engine: Copyright (C) Carnegie Mellon University 2005-2014
hts\_engine: 
The HMM-Based Speech Synthesis Engine "hts\_engine API"
hts\_engine API version 1.07 (\url{http://hts-engine.sourceforge.net/})
Copyright (C) The HMM-Based Speech Synthesis Engine "hts\_engine API"
Version 1.07 (\url{http://hts-engine.sourceforge.net/})
Copyright (C) 2001-2012 Nagoya Institute of Technology
              2001-2008 Tokyo Institute of Technology
All rights reserved.
\url{http://www.cstr.ed.ac.uk/projects/festival/}},
year = {December 2014}
}





@article{Marques2018,
author = {Marques, Tiago and Nguyen, Julia and Fioreze, Gabriela and Petreanu, Leopoldo},
year = {2018},
month = {04},
pages = {1546-1726},
title = {The functional organization of cortical feedback inputs to primary visual cortex},
journal = {Nature Neuroscience}
}



@article{reiter_1998,
author = {Reiter, H.O. and Stryker, Michael},
year = {1988},
month = {06},
pages = {3623-7},
title = {Neural Plasticity without Postsynaptic Action Potentials: Less-Active Inputs become Dominant When Kitten Visual Cortical Cells are Pharmacologically Inhibited},
volume = {85},
booktitle = {Proceedings of the National Academy of Sciences of the United States of America}
}



@InProceedings{silos_2016,
author="de-la-Calle-Silos, F.
and Gallardo-Antol{\'i}n, A.
and Pel{\'a}ez-Moreno, C.",
editor="Abad, Alberto
and Ortega, Alfonso
and Teixeira, Ant{\'o}nio 
and Garc{\'i}a Mateo, Carmen
and Mart{\'i}nez Hinarejos, Carlos D.
and Perdig{\~a}o, Fernando
and Batista, Fernando
and Mamede, Nuno",
title="An Analysis of Deep Neural Networks in Broad Phonetic Classes for Noisy Speech Recognition",
booktitle="Advances in Speech and Language Technologies for Iberian Languages",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="87--96",
abstract="The introduction of Deep Neural Network (DNN) based acoustic models has produced dramatic improvements in performance. In particular, we have recently found that Deep Maxout Networks, a modification of DNNs' feed-forward architecture that uses a max-out activation function, provides enhanced robustness to environmental noise. In this paper we further investigate how these improvements are translated into the different broad phonetic classes and how does it compare to classical Hidden Markov Models (HMM) based back-ends. Our experiments demonstrate that performance is still tightly related to the particular phonetic class being stops and affricates the least resilient but also that relative improvements of both DNN variants are distributed unevenly across those classes having the type of noise a significant influence on the distribution. A combination of the different systems DNN and classical HMM is also proposed to validate our hypothesis that the traditional GMM/HMM systems have a different type of error than the Deep Neural Networks hybrid models.",
isbn="978-3-319-49169-1"
}


@article {Javitt11962,
	author = {Javitt, D C and Steinschneider, M and Schroeder, C E and Arezzo, J C},
	title = {Role of cortical N-methyl-D-aspartate receptors in auditory sensory memory and mismatch negativity generation: implications for schizophrenia},
	volume = {93},
	number = {21},
	pages = {11962--11967},
	year = {1996},
	publisher = {National Academy of Sciences},
	abstract = {Working memory refers to the ability of the brain to store and manipulate information over brief time periods, ranging from seconds to minutes. As opposed to long-term memory, which is critically dependent upon hippocampal processing, critical substrates for working memory are distributed in a modality-specific fashion throughout cortex. N-methyl-D-aspartate (NMDA) receptors play a crucial role in the initiation of long-term memory. Neurochemical mechanisms underlying the transient memory storage required for working memory, however, remain obscure. Auditory sensory memory, which refers to the ability of the brain to retain transient representations of the physical features (e.g., pitch) of simple auditory stimuli for periods of up to approximately 30 sec, represents one of the simplest components of the brain working memory system. Functioning of the auditory sensory memory system is indexed by the generation of a well-defined event-related potential, termed mismatch negativity (MMN). MMN can thus be used as an objective index of auditory sensory memory functioning and a probe for investigating underlying neurochemical mechanisms. Monkeys generate cortical activity in response to deviant stimuli that closely resembles human MMN. This study uses a combination of intracortical recording and pharmacological micromanipulations in awake monkeys to demonstrate that both competitive and noncompetitive NMDA antagonists block the generation of MMN without affecting prior obligatory activity in primary auditory cortex. These findings suggest that, on a neurophysiological level, MMN represents selective current flow through open, unblocked NMDA channels. Furthermore, they suggest a crucial role of cortical NMDA receptors in the assessment of stimulus familiarity/unfamiliarity, which is a key process underlying working memory performance.},
	issn = {0027-8424},
	URL = {http://www.pnas.org/content/93/21/11962},
	eprint = {http://www.pnas.org/content/93/21/11962.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}


@article{nachum_2003,
author = {Ulanovsky, Nachum and Las, Liora and Nelken, Israel},
year = {2003},
month = {05},
pages = {391-8},
title = {Processing of low-probability sounds by cortical neurons},
volume = {6},
booktitle = {Nature neuroscience}
}


@inproceedings{Natan2015ComplementaryCO,
  title={Complementary control of sensory adaptation by two types of cortical interneurons},
  author={Ryan G Natan and John J Briguglio and Laetitia Mwilambwe-Tshilobo and Sara I Jones and Mark Aizenberg and Ethan M Goldberg and Maria Neimark Geffen},
  booktitle={eLife},
  year={2015}
}


@article{kuhl_1975,
author = {K Kuhl, P and Miller, James},
year = {1975},
month = {11},
pages = {69-72},
title = {Speech Perception by the Chinchilla: Voiced-Voiceless Distinction in Alveolar Plosive Consonants},
volume = {190},
booktitle = {Science (New York, N.Y.)}
}


@article {Meyer19113,
	author = {Meyer, Hanno S. and Egger, Robert and Guest, Jason M. and Foerster, Rita and Reissl, Stefan and Oberlaender, Marcel},
	title = {Cellular organization of cortical barrel columns is whisker-specific},
	volume = {110},
	number = {47},
	pages = {19113--19118},
	year = {2013},
	doi = {10.1073/pnas.1312691110},
	publisher = {National Academy of Sciences},
	abstract = {Cortical columns are thought to be the elementary functional building blocks of sensory cortices. Here we show that the cellular architecture of cortical {\textquotedblleft}barrel{\textquotedblright} columns in rodent somatosensory cortex is not stereotypic, but specific for each whisker on the animals{\textquoteright} snout. Our findings challenge the concepts underlying contemporary simulation efforts that build up large-scale network models of repeatedly occurring identical cortical circuits.The cellular organization of the cortex is of fundamental importance for elucidating the structural principles that underlie its functions. It has been suggested that reconstructing the structure and synaptic wiring of the elementary functional building block of mammalian cortices, the cortical column, might suffice to reverse engineer and simulate the functions of entire cortices. In the vibrissal area of rodent somatosensory cortex, whisker-related {\textquotedblleft}barrel{\textquotedblright} columns have been referred to as potential cytoarchitectonic equivalents of functional cortical columns. Here, we investigated the structural stereotypy of cortical barrel columns by measuring the 3D neuronal composition of the entire vibrissal area in rat somatosensory cortex and thalamus. We found that the number of neurons per cortical barrel column and thalamic {\textquotedblleft}barreloid{\textquotedblright} varied substantially within individual animals, increasing by \~{}2.5-fold from dorsal to ventral whiskers. As a result, the ratio between whisker-specific thalamic and cortical neurons was remarkably constant. Thus, we hypothesize that the cellular architecture of sensory cortices reflects the degree of similarity in sensory input and not columnar and/or cortical uniformity principles.},
	issn = {0027-8424},
	URL = {http://www.pnas.org/content/110/47/19113},
	eprint = {http://www.pnas.org/content/110/47/19113.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}


@article{BRENT199693,
title = "Distributional regularity and phonotactic constraints are useful for segmentation",
journal = "Cognition",
volume = "61",
number = "1",
pages = "93 - 125",
year = "1996",
note = "Compositional Language Acquisition",
issn = "0010-0277",
doi = "https://doi.org/10.1016/S0010-0277(96)00719-6",
url = "http://www.sciencedirect.com/science/article/pii/S0010027796007196",
author = "Michael R. Brent and Timothy A. Cartwright"
}


@article{saffran_1997,
author = {Saffran, Jenny and Newport, Elissa and N. Aslin, Richard and A. Tunick, Rachel and Barrueco, Sandra},
year = {1997},
month = {03},
pages = {101-105},
title = {Incidental Language Learning: Listening (and Learning) Out of the Corner of Your Ear},
volume = {8},
booktitle = {Psychological Science - PSYCHOL SCI}
}


@article{CC01a,
 author = {Chang, Chih-Chung and Lin, Chih-Jen},
 title = {{LIBSVM}: A library for support vector machines},
 journal = {ACM Transactions on Intelligent Systems and Technology},
 volume = {2},
 issue = {3},
 year = {2011},
 pages = {27:1--27:27},
 note =	 {Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}}
}


@Article{FFTW05,
  author = 	 {Frigo, Matteo and Johnson, Steven~G.},
  title = 	 {The Design and Implementation of {FFTW3}},
  journal = 	 {Proceedings of the IEEE},
  year = 	 2005,
  volume =	 93,
  number =	 2,
  pages =	 {216--231},
  note =	 {Special issue on ``Program Generation, Optimization, and Platform Adaptation''}
}


@article{cui_2016,
author = {Yuwei Cui and Subutai Ahmad and Jeff Hawkins},
title = {Continuous Online Sequence Learning with an Unsupervised Neural Network Model},
journal = {Neural Computation},
volume = {28},
number = {11},
pages = {2474-2504},
year = {2016},
doi = {10.1162/NECO\_a\_00893},
    note ={PMID: 27626963},

URL = { 
        https://doi.org/10.1162/NECO_a_00893
    
},
eprint = { 
        https://doi.org/10.1162/NECO_a_00893
    
}
,
    abstract = { The ability to recognize and predict temporal sequences of sensory inputs is vital for survival in natural environments. Based on many known properties of cortical neurons, hierarchical temporal memory (HTM) sequence memory recently has been proposed as a theoretical framework for sequence learning in the cortex. In this letter, we analyze properties of HTM sequence memory and apply it to sequence learning and prediction problems with streaming data. We show the model is able to continuously learn a large number of variable order temporal sequences using an unsupervised Hebbian-like learning rule. The sparse temporal codes formed by the model can robustly handle branching temporal sequences by maintaining multiple predictions until there is sufficient disambiguating evidence. We compare the HTM sequence memory with other sequence learning algorithms, including statistical methods—autoregressive integrated moving average; feedforward neural networks—time delay neural network and online sequential extreme learning machine; and recurrent neural networks—long short-term memory and echo-state networks on sequence prediction problems with both artificial and real-world data. The HTM model achieves comparable accuracy to other state-of-the-art algorithms. The model also exhibits properties that are critical for sequence learning, including continuous online learning, the ability to handle multiple predictions and branching sequences with high-order statistics, robustness to sensor noise and fault tolerance, and good performance without task-specific hyperparameter tuning. Therefore, the HTM sequence memory not only advances our understanding of how the brain may solve the sequence learning problem but is also applicable to real-world sequence learning problems from continuous data streams. }
}


@article{eatock_2000,
	author = {Eatock, R., A.},
	title = {Adaptation in hair cells.},
	journal = {Annu. Rev. Neurosci.},
        year = {2000},
        volume = {23},
	pages = {285–314}
}


@article{holt_2000,
	author = {Holt, J., R, and Corey, D., P.},
	title = {Two mechanisms for transducer adaptation in vertebrate hair cells.},
	journal = {Proc. Natl. Acad. Sci. USA.},
        year = {2000},
        volume = {97},
	pages = {11730–11735}
}


@article{le_goff_2005,
	author = {Le Goff, L and Bozovic, D and Hudspeth, A. J.},
	title = {Adaptive shift in the domain of negative stiffness during spontaneous oscillation by hair bundles from the internal ear.},
	journal = {Proc. Natl. Acad. Sci. USA.},
        year = {2005},
        volume = {102},
	pages = {16996–17001}
}


@article{shamma_1993,
	author = {Shama, S. A. and Fleshman, J. W. and Wiser, P. W. and Versnel, H.},
	title = {Organization of response areas in ferret primary auditory cortex.},
	journal = {Neurophysiol.},
        year = {1993},
        volume = {69},
        month = {Feb.},
	pages = {367-383}
}


@article{schreiner_1990,
	author = {Schreiner, C. E. and Sutler, M. L.},
	title = {Functional topography of cat primary auditory cortex: distribution of integrated excitation.},
	journal = {J.Neurophys.},
        year = {1990},
        volume = {64},
        month = {Nov.},
	pages = {1442-1459}
}


@article{heil_1992,
	author = {Heil, P. and Rajan, R. and Irvine, D. R.},
	title = {Sensitivity of neurons in cat primary auditory cortex to tones and frequency-modulated stimuli 11: Organization of response properties along the ‘isofrequency‘ dimension.},
	journal = {Hearing Res.},
        year = {1992},
        volume = {63},
        month = {Nov.},
	pages = {135-156}
}


@article{mendelson_1985,
	author = {Mendelson, J. R. and Cynader, M. S.},
	title = {Sensitivity of cat primary auditory cortex (AI) neurons to the direction and rate of frequency modulation.},
	journal = {Brain Res.},
        year = {1985},
        volume = {327},
	pages = {331-335}
}


@article{antic_2010,
	author = {Antic, S. D. and Zhou, W. L. and Moore, A. R. and Short, S. M. and Ikonomu, K. D.},
	title = {The decade of the dendritic NMDA spike.},
	journal = {J. Neurosci. Res.},
        year = {2010},
        volume = {88},
	pages = {2991–3001}
}


@article{major_2013,
	author = {Major, G. and Larkum, M. E. and Schiller, J.},
	title = {Active properties of neocortical pyramidal neuron dendrites.},
	journal = {Annu. Rev. Neurosci.},
        year = {2013},
        volume = {36},
	pages = {1–24}
}


@article{wang_1995,
	author = {Kuansan, Wang and Shihab, A., Shamma},
	title = {Spectral Shape Analysis in the Central Auditory System},
	journal = {IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING.},
        year = {1995},
        volume = {3},
	number = {5},
	month = {sept},
	pages = {382-395}
}


@article{ahmad_2015,
	author = {Ahmad, S. and Hawkins, J.},
	title = {Properties of Sparse Distributed Representations and their Application to Hierarchical Temporal Memory},
	journal = {https://arxiv.org/pdf/1503.07469.pdf.},
        year = {2015},
	month = {March}
}


@article{kuhl_1983,
	author = {Kuhl, P. K. and Padden, D. M.},
	title = {Enhanced discriminability at the phonetic boundaries for the place feature in macaques},
	journal = {J. Acoust. Soc. Am.},
        year = {1983},
        volume = {73},
        pages = {1003-1010}
}


@article{kluender_1998,
	author = {Kluender, K. R. and  Lotto, A. J. and  Holt, L. L. and  Bloedel, S. L.},
	title = {Role of experience for language-specific functional mappings of vowel sounds},
	journal = {J. Acoust. Soc. Am.},
        year = {1998},
	volume = {104},
	pages = {3568–3582}
}


@article{pons_2006,
	author = {Pons, F.},
	title = {The effects of distributional learning on rats sensitivity to phonetic information},
	journal = {J. Exp. Psychol. Anim. Behav.},
        year = {2006},
        volume = {32},
        pages = {97–101}
}


@article{hienz_1996,
	author = {Hienz, R. D.and Aleszczyk, C. M.and May, B. J.},
	title = {Vowel discrimination in cats: Acquisition, effects of stimulus level, and performance in noise},
	journal = {J. Acoust. Soc. Am.},
        year = {1996},
        volume = {99},
        pages = {3656–3668}
}


@article{dent_1997,
	author = {Dent, M. L. and Brittan-Powell, E. F. and Dooling, R. J. and Pierce, A.},
	title = {Perception of synthetic /ba/-/wa/ speech continuum by budgerigars (Melopsittacus undulatus)},
	journal = {J. Acoust. Soc. Am.},
        year = {1997},
        volume = {102},
        pages = {1891–1897}
}


@article{lotto_1997,
	author = {Lotto, A. J. and Kluender, K. R. and Holt, L. L.},
	title = {Perceptual compensation for coarticulation by Japanese quail (Coturnix coturnix japonica)},
	journal = {J. Acoust. Soc. Am.},
        year = {1997},
        volume = {102},
        pages = {1134–1140}
}


@article{rasanen_2012,
	author = {R\"{a}s\"{a}nen, O.},
	title = {Computational modeling of phonetic and lexical learning in early language acquisition: Existing models and future directions},
	journal = {Speech Communication.},
        year = {2012},
        volume = {54},
        pages = {975–997}
}


@article{appelbaum_1996,
	author = {Appelbaum, I.},
	title = {The lack of invariance problem and the goal of speech perception},
	journal = {Spoken Language, 1996. ICSLP 96. Proceedings., Fourth International Conference on},
        year = {1996},
        volume = {3}
}


@article{lecun_2015,
	author = {LeCun, Y. and Bengio, Y. and Hinton, G.},
	title = {Deep learning},
	journal = {Nature},
        year = {2015},
        volume = {521},
	pages = {436-444}
}


@unpublished{Hawkins-et-al-2016-Book,
   title={Biological and Machine Intelligence (BAMI)},
   author={Hawkins, J. and Ahmad, S. and Purdy, S. and Lavin, A.},
   note={Initial online release 0.4},
   url={http://numenta.com/biological-and-machine-intelligence/},
   year={2016}
}


@article{scharenborg_2010,
	author = {Scharenborg, O. and Boves, L.},
	title = {Computational modelling of spoken-word recognition processes},
	journal = {John Benjamins Publishing Company.},
        year = {2010}
}


@article{dominey_2000,
	author = {Dominey, P. F. and Ramus, F.},
	title = {Neural network processing of natural language: I. Sensitivity to serial, temporal and abstract structure of language in the infant},
	journal = {Language and Cognitive Processes.},
        year = {2000},
        volume = {15},
        pages = {87-127}
}


@article{boer_2003,
	author = {de Boer, B. and Kuhl, P.},
	title = {Investigating the role of infant-directed speech with a computer model},
	journal = {Acoustics Research Letters Online.},
        year = {2003},
        volume = {4},
        pages = {129–134}
}


@article{vallabha_2007,
	author = {Vallabha, G. K. and McLelland, J. L. and Pons, F. and Werker, J. F. and Amano, S.},
	title = {Unsupervised learning of vowel categories from infant-directed speech},
	journal = {Proceedings of National Academy of Sciences.},
        year = {2007},
        volume = {104},
        pages = {13273–13278}
}


@article{toscano_2010,
	author = {Toscano, J. C. and McMurray, B.},
	title = {Cue Integration With Categories: Weighting Acoustic Cues in Speech Using Unsupervised Learning and Distributional Statistics},
	journal = {Cognitive Scienc.},
        year = {2010},
        volume = {34},
        pages = {434–464}
}


@article{kouki_2010,
	author = {Kouki, M. and Hideaki, K. and Reiko, M.},
	title = {Unsupervised Learning of Vowels from Continuous Speech based on Self-organized Phoneme Acquisition Model},
	journal = {Interspeech.},
        year = {2010}
}


@article{kouki_2011,
	author = {Kouki, M. and Hideaki, M. and Hideaki, K. and Reiko, M.},
	title = {The Multi Timescale Phoneme Acquisition Model of the Self-Organizing Based on the Dynamic Features},
	journal = {Interspeech.},
        year = {2011}
}


@article{hawkins_2004,
	author = {Hawkins, J. and Blakeslee, S},
	title = {On Intelligence},
	journal = {Times Books.},
        year = {2004}
}


@inproceedings{Lee:2009:UFL:2984093.2984217,
 author = {Lee, Honglak and Largman, Yan and Pham, Peter and Ng, Andrew Y.},
 title = {Unsupervised Feature Learning for Audio Classification Using Convolutional Deep Belief Networks},
 booktitle = {Proceedings of the 22Nd International Conference on Neural Information Processing Systems},
 series = {NIPS'09},
 year = {2009},
 isbn = {978-1-61567-911-9},
 location = {Vancouver, British Columbia, Canada},
 pages = {1096--1104},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2984093.2984217},
 acmid = {2984217},
 publisher = {Curran Associates Inc.},
 address = {USA},
} 


@article{hawkins_2016,
	author = {Hawkins, J. and Ahmad, S.},
	title = {Why Neurons Have Thousands of Synapses, a Theory of Sequence Memory in Neocortex},
	journal = {Frontiers in Neural Circuits.},
        year = {2016},
        volume = {10},
}


@article{mountcastle_1955,
	author = {Mountcastle, V. B. and Berman, A. L. and  Davies, P. W.},
	title = {Topographic organization and modality representation in first somatic area of cat’s cerebral cortex by method of single unit analysis},
	journal = {Am. J. Physiol.},
        year = {1955},
        volume = {183}
}


@article{mountcastle_modality_1957,
	title = {{MODALITY} {AND} {TOPOGRAPHIC} {PROPERTIES} {OF} {SINGLE} {NEURONS} {OF} {CAT}'{S} {SOMATIC} {SENSORY} {CORTEX}},
	volume = {20},
	issn = {0022-3077, 1522-1598},
	url = {http://www.physiology.org/doi/10.1152/jn.1957.20.4.408},
	doi = {10.1152/jn.1957.20.4.408},
	language = {en},
	number = {4},
	urldate = {2018-07-10},
	journal = {Journal of Neurophysiology},
	author = {Mountcastle, Vernon B.},
	month = jul,
	year = {1957},
	pages = {408--434},
	file = {Mountcastle - 1957 - MODALITY AND TOPOGRAPHIC PROPERTIES OF SINGLE NEUR.pdf:/home/dario/Zotero/storage/EXXRNZXE/Mountcastle - 1957 - MODALITY AND TOPOGRAPHIC PROPERTIES OF SINGLE NEUR.pdf:application/pdf}
}


@article{hubel_1962,
	author = {Hubel, D. and Wiesel, T.},
	title = {Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex},
	journal = {J Physiol.},
        year = {1962},
        volume = {160},
        pages = {106–154}
}


@article{hubel_1968,
	author = {Hubel, D. and Wiesel, T.},
	title = {Receptive fields and functional architecture of monkey striate cortex},
	journal = {J Physiol.},
        year = {1968},
        volume = {195},
        pages = {215–243}
}


@article{rakic_1995,
	author = {Rakic, P.},
	title = {Radial versus tangential migration of neuronal clones in the developing cerebral cortex},
	journal = {Proc Natl Acad Sci USA.},
        year = {1995},
        volume = {92}
}


@article{mountcastle_1978,
	author = {Mountcastle, V. B.},
	title = {An Organizing Principle for Cerebral Function: The Unit Model and the Distributed System},
	journal = {Cambridge, MA.},
        year = {1978}
}


@article{linden_2003,
author = {Linden, Jennifer F. and Schreiner, Christoph E.},
title = {Columnar Transformations in Auditory Cortex? A Comparison to Visual and Somatosensory Cortices},
journal = {Cerebral Cortex},
volume = {13},
number = {1},
pages = {83-89},
year = {2003},
doi = {10.1093/cercor/13.1.83},
URL = {http://dx.doi.org/10.1093/cercor/13.1.83},
eprint = {/oup/backfile/content_public/journal/cercor/13/1/10.1093_cercor_13.1.83/1/1300083.pdf}
}


@article{mountcastle_1997,
author = {B Mountcastle, V},
year = {1997},
month = {04},
pages = {701-722},
title = {The columnar organization of the neocortex},
volume = {120 ( Pt 4)},
booktitle = {Brain}
}


@article{huang_2000,
	author = {Huang, C. L. and Winer, J. A.},
	title = {Auditory thalamocortical projections in the cat: laminar and areal patterns of input},
	journal = {J. Comp. Neurol.},
        year = {2000},
        volume = {427},
        pages = {302-331}
}


@article{winer_1992,
	author = {Winer, J. A.},
	title = {The functional architecture of the medial geniculate body and the primary auditory cortex,  In: The mammalian auditory pathway: neuroanatomy},
	journal = {New York: Springer-Verlag.},
        year = {1992},
        pages = {222-409}
}


@article{rockel_1980,
	author = {Rockel, A. J. and Hiorns, R. W. and Powell, T. P.},
	title = {The basic uniformity in the structure of the neocortex},
	journal = {Brain},
        year = {1980},
        volume = {103},
        pages = {221-244}
}


@article{mitani_1985,
	author = {Mitani, A. and Shimokouchi, M.},
	title = {Neuronal connections in the primary auditor y cortex: an electrophysiological study in the cat},
	journal = {J. Comp. Neurol.},
        year = {1985},
        volume = {235},
        pages = {417-429}
}


@article{mitani_1985A,
	author = {Mitani, A. and Shimokouchi, M. and Itoh, K. and Nomura, S. and Kudo, M. and Mizuno, N.},
	title = {Morphology and laminar organization of electrophysiologically identified neurons in the primary auditory cortex in the cat},
	journal = {J. Comp. Neurol.},
        year = {1985},
        volume = {235},
        pages = {430-447}
}


@article{sur_1988,
	author = {Sur, M. and Garraghty, P. E. and Roe, A. W.},
	title = {Experimentally induced visual projections into auditory thalamus and cortex},
	journal = {Science.},
        year = {1988},
        volume = {242},
        pages = {1437–1441}
}


@article{angelucci_1998,
	author = {Angelucci, A. and Clasca, F. and Sur, M.},
	title = {Brainstem inputs to the ferret medial geniculate nucleus and the effect of early deafferentation on novel retinal projections to the auditory thalamus},
	journal = {J. Comp. Neurol.},
        year = {1998},
        volume = {400},
        pages = {417–439}
}


@article{roe_1992,
	author = {Roe, A. W. and Pallas, S. L. and Kwon, Y. H. and Sur, M.},
	title = {Visual projections routed to the auditory pathway in ferrets: receptive fields of visual neurons in primary auditory cortex},
	journal = {J. Neurosci.},
        year = {1992},
        volume = {12},
        pages = {3651–3664}
}


@article{roe_1990,
	author = {Roe, A. W. and Pallas, S. L. and Hahm, J. O. and Sur, M.},
	title = {A map of visual space induced in primary auditory cortex},
	journal = {Science.},
        year = {1990},
        volume = {250},
        pages = {818–820}
}


@article{sur_2000,
	author = {Sharma, J. and Angelucci, A. and Sur, M.},
	title = {Induction of visual orientation modules in auditory cortex},
	journal = {Nature.},
        year = {2000},
        volume = {404},
        pages = {841–847}
}


@article{mesgarani_2008,
	author = {Mesgarani, N. and David, S. V. and Fritz, J. B. and Shamma, S. A.},
	title = {Phoneme representation and classification in primary auditory cortex},
	journal = {J. Acoust. Soc. Am.},
        year = {2008},
        volume = {123},
        pages = {899–909}
}


@article{mesgarani_2014A,
	author = {Mesgarani, N. and David, S. V. and Fritz, J. B. and Shamma, S. A.},
	title = {Mechanisms of noise robust representation of speech in primary auditory cortex},
	journal = {PNAS.},
        year = {2014},
        volume = {123},
        pages = {899–909}
}


@article{chi_2005,
	author = {Chi, T. and Ru, P. and Shamma, S.A.},
	title = {Multiresolution spectrotemporal analysis of complex sounds.},
	journal = {J. Acoust. Soc. Am.},
        year = {2005},
        volume = {118},
        pages = {887-906}
}


@article{kohonen_2082,
	author = {Kohonen, T.},
	title = {Self-Organized Formation of Topologically Correct Feature Maps},
	journal = {Biological Cybernetics.},
        year = {1982},
        volume = {43},
        pages = {59-69}
}


@book{Kohonen:1989:SAM:69371,
 author = {Kohonen, T.},
 title = {Self-organization and Associative Memory: 3rd Edition},
 year = {1989},
 isbn = {0-387-51387-6},
 publisher = {Springer-Verlag New York, Inc.},
 address = {New York, NY, USA},
} 


@article{barth_2012,
	author = {Barth, A.L. and Poulet, J.F.},
	title = {Experimental evidence for sparse firing in the neocortex.},
	journal = {Trends Neurosci.},
        year = {2012},
        volume = {35},
        pages = {345-55}
}


@article{turrigiano_2012,
	author = {Turrigiano, G.},
	title = {Homeostatic Synaptic Plasticity: Local and Global Mechanisms for Stabilizing Neuronal Function},
	journal = {Cold Spring Harb Perspect Biol.},
        year = {2012},
        volume = {4}
}


@article{ahmad_2016,
	author = {Ahmad, S. and Hawkins, J.},
	title = {How do neurons operate on sparse distributed representations? A mathematical theory of sparsity, neurons and active dendrites.},
	journal = {arXiv:1601.00720 [q–bio.NC]},
        year = {2016}
}


@article{JMLR:v15:srivastava14a,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  pages   = {1929-1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}


@article{taishih_2005,
	author = {Taishih, Chi and Powen, Ru and Shihab, A., Shamma },
	title = {Multiresulution spectrotemporal analysis of complex sounds},
	journal = {J. Acoust. Soc. Am.},
        year = {2005},
        volume = {118},
        pages = {887-906}
}


@book{Miller_1993,
 author = {Kenneth S. Miller and Bertram Ross},
 title = {An Introduction to the Fractional Calculus and Fractional Differential Equations},
 year = {1993},
 isbn = {0-471-58884-9},
 publisher = {A Wiley-Interscience Publication},
 address = {Printed in the United States of America},
 edition = {First},
 pages = {23}
} 


@article{KRAUSE201436,
title = "Contextual modulation and stimulus selectivity in extrastriate cortex",
journal = "Vision Research",
volume = "104",
pages = "36 - 46",
year = "2014",
note = "The Function of Contextual Modulation",
issn = "0042-6989",
doi = "https://doi.org/10.1016/j.visres.2014.10.006",
url = "http://www.sciencedirect.com/science/article/pii/S0042698914002399",
author = "Matthew R. Krause and Christopher C. Pack",
keywords = "Contextual modulation, Normalization, Surround, Extrastriate cortex, Neurophysiology, Macaque"
}


@article{doi:10.1167/16.13.1,
author = {Snow, Michoel and Coen-Cagli, Ruben and Schwartz, Odelia},
title = {Specificity and timescales of cortical adaptation as inferences about natural movie statistics},
journal = {Journal of Vision},
volume = {16},
number = {13},
pages = {},
year = {2016},
doi = {10.1167/16.13.1},
URL = { + http://dx.doi.org/10.1167/16.13.1},
eprint = {/data/journals/jov/935767/i1534-7362-16-13-1.pdf}
}


@article{guerguiev_towards_2017,
	title = {Towards deep learning with segregated dendrites},
	volume = {6},
	copyright = {© 2017 Guerguiev et al.. This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited.},
	issn = {2050-084X},
	url = {https://elifesciences.org/articles/22901},
	doi = {10.7554/eLife.22901},
	abstract = {A multi-compartment spiking neural network model demonstrates that biologically feasible deep learning can be achieved if sensory inputs and higher-order feedback are received by different dendritic compartments.},
	language = {en},
	urldate = {2018-06-20},
	journal = {eLife},
	author = {Guerguiev, Jordan and Lillicrap, Timothy P. and Richards, Blake A.},
	month = dec,
	year = {2017},
	pages = {e22901},
	file = {Full Text PDF:/home/dario/Zotero/storage/HM44QJKB/Guerguiev et al. - 2017 - Towards deep learning with segregated dendrites.pdf:application/pdf;Snapshot:/home/dario/Zotero/storage/GLAXT948/22901.html:text/html}
}

@article{hinton_matrix_2018,
	title = {{MATRIX} {CAPSULES} {WITH} {EM} {ROUTING}},
	abstract = {A capsule is a group of neurons whose outputs represent different properties of the same entity. Each layer in a capsule network contains many capsules. We describe a version of capsules in which each capsule has a logistic unit to represent the presence of an entity and a 4x4 matrix which could learn to represent the relationship between that entity and the viewer (the pose). A capsule in one layer votes for the pose matrix of many different capsules in the layer above by multiplying its own pose matrix by trainable viewpoint-invariant transformation matrices that could learn to represent part-whole relationships. Each of these votes is weighted by an assignment coefﬁcient. These coefﬁcients are iteratively updated for each image using the Expectation-Maximization algorithm such that the output of each capsule is routed to a capsule in the layer above that receives a cluster of similar votes. The transformation matrices are trained discriminatively by backpropagating through the unrolled iterations of EM between each pair of adjacent capsule layers. On the smallNORB benchmark, capsules reduce the number of test errors by 45\% compared to the state-of-the-art. Capsules also show far more resistance to white box adversarial attacks than our baseline convolutional neural network.},
	language = {en},
	author = {Hinton, Geoffrey and Sabour, Sara and Frosst, Nicholas},
	year = {2018},
	pages = {15},
	file = {Hinton et al. - 2018 - MATRIX CAPSULES WITH EM ROUTING.pdf:/home/dario/Zotero/storage/H253N4HM/Hinton et al. - 2018 - MATRIX CAPSULES WITH EM ROUTING.pdf:application/pdf}
}

@article{sabour_dynamic_2017,
	title = {Dynamic {Routing} {Between} {Capsules}},
	url = {http://arxiv.org/abs/1710.09829},
	abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
	urldate = {2018-06-20},
	journal = {arXiv:1710.09829 [cs]},
	author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.09829},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1710.09829 PDF:/home/dario/Zotero/storage/UNXCYPFT/Sabour et al. - 2017 - Dynamic Routing Between Capsules.pdf:application/pdf;arXiv.org Snapshot:/home/dario/Zotero/storage/WII36P4V/1710.html:text/html}
}

@article{silver_mastering_2016,
	title = {Mastering the game of {Go} with deep neural networks and tree search},
	volume = {529},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature16961},
	doi = {10.1038/nature16961},
	language = {en},
	number = {7587},
	urldate = {2018-06-20},
	journal = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	month = jan,
	year = {2016},
	pages = {484--489},
	file = {Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf:/home/dario/Zotero/storage/EYKMAWX6/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf:application/pdf}
}

@article{silver_mastering_2017,
	title = {Mastering the game of {Go} without human knowledge},
	volume = {550},
	copyright = {2017 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature24270},
	doi = {10.1038/nature24270},
	abstract = {Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games.},
	language = {en},
	number = {7676},
	urldate = {2018-06-20},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and Driessche, George van den and Graepel, Thore and Hassabis, Demis},
	month = oct,
	year = {2017},
	pages = {354},
	file = {Snapshot:/home/dario/Zotero/storage/L54E5F5L/nature24270.html:text/html}
}

@misc{noauthor_9_nodate,
	title = {(9) {Beyond} regression : new tools for prediction and analysis in the behavioral sciences /},
	shorttitle = {(9) {Beyond} regression},
	url = {https://www.researchgate.net/publication/35657389_Beyond_regression_new_tools_for_prediction_and_analysis_in_the_behavioral_sciences},
	abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
	language = {en},
	urldate = {2018-06-22},
	journal = {ResearchGate},
	file = {Snapshot:/home/dario/Zotero/storage/Y5WI3SEP/35657389_Beyond_regression_new_tools_for_prediction_and_analysis_in_the_behavioral_sciences.html:text/html}
}

@misc{noauthor_parker_nodate,
	title = {Parker {DB} 1985 {Learning} logic {Technical} {Report} {TR} 47 {Cambridge} {MA} {MIT} {Center}},
	url = {https://www.coursehero.com/file/p5227u3/Parker-DB-1985-Learning-logic-Technical-Report-TR-47-Cambridge-MA-MIT-Center/},
	abstract = {Parker DB 1985 Learning logic Technical Report TR 47 Cambridge MA MIT Center from MIS PLS at Hong Kong Shue Yan},
	language = {en},
	urldate = {2018-06-22},
	file = {Snapshot:/home/dario/Zotero/storage/JSWPCZ5T/Parker-DB-1985-Learning-logic-Technical-Report-TR-47-Cambridge-MA-MIT-Center.html:text/html}
}

@article{lecun_procedure_1985,
	title = {Une procedure d'apprentissage pour reseau a seuil asymmetrique ({A} learning scheme for asymmetric threshold networks)},
	url = {https://nyuscholars.nyu.edu/en/publications/une-procedure-dapprentissage-pour-reseau-a-seuil-asymmetrique-a-l},
	language = {English (US)},
	urldate = {2018-06-23},
	journal = {Proceedings of Cognitiva 85, Paris, France},
	author = {Lecun, Yann},
	year = {1985},
	file = {Snapshot:/home/dario/Zotero/storage/CDFYTBBC/une-procedure-dapprentissage-pour-reseau-a-seuil-asymmetrique-a-l.html:text/html}
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	copyright = {1986 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	language = {en},
	number = {6088},
	urldate = {2018-06-22},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	pages = {533--536},
	file = {Snapshot:/home/dario/Zotero/storage/BXMVG6XE/323533a0.html:text/html}
}

@inproceedings{hinton_what_2005,
	address = {San Francisco, CA, USA},
	series = {{IJCAI}'05},
	title = {What {Kind} of a {Graphical} {Model} is the {Brain}?},
	url = {http://dl.acm.org/citation.cfm?id=1642293.1642643},
	abstract = {If neurons are treated as latent variables, our visual systems are non-linear, densely-connected graphical models containing billions of variables and thousands of billions of parameters. Current algorithms would have difficulty learning a graphical model of this scale. Starting with an algorithm that has difficulty learning more than a few thousand parameters, I describe a series of progressively better learning algorithms all of which are designed to run on neuron-like hardware. The latest member of this series can learn deep, multi-layer belief nets quite rapidly. It turns a generic network with three hidden layers and 1:7 million connections into a very good generative model of handwritten digits. After learning, the model gives classification performance that is comparable to the best discriminative methods.},
	urldate = {2018-06-22},
	booktitle = {Proceedings of the 19th {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Hinton, Geoffrey E.},
	year = {2005},
	pages = {1765--1775}
}

@article{hinton_fast_2006,
	title = {A fast learning algorithm for deep belief nets},
	volume = {18},
	issn = {0899-7667},
	doi = {10.1162/neco.2006.18.7.1527},
	abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
	language = {eng},
	number = {7},
	journal = {Neural Computation},
	author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
	month = jul,
	year = {2006},
	pmid = {16764513},
	keywords = {Algorithms, Animals, Humans, Learning, Neural Networks (Computer), Neurons},
	pages = {1527--1554}
}

@inproceedings{bengio_greedy_2006,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'06},
	title = {Greedy {Layer}-wise {Training} of {Deep} {Networks}},
	url = {http://dl.acm.org/citation.cfm?id=2976456.2976476},
	abstract = {Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.},
	urldate = {2018-06-22},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
	year = {2006},
	pages = {153--160}
}

@inproceedings{ranzato_efficient_2006,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'06},
	title = {Efficient {Learning} of {Sparse} {Representations} with an {Energy}-based {Model}},
	url = {http://dl.acm.org/citation.cfm?id=2976456.2976599},
	abstract = {We describe a novel unsupervised method for learning sparse, overcomplete features. The model uses a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector. Given an input, the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible to the encoder output. Learning proceeds in a two-phase EM-like fashion: (1) compute the minimum-energy code vector, (2) adjust the parameters of the encoder and decoder so as to decrease the energy. The model produces "stroke detectors" when trained on handwritten numerals, and Gabor-like filters when trained on natural image patches. Inference and learning are very fast, requiring no preprocessing, and no expensive sampling. Using the proposed unsupervised method to initialize the first layer of a convolutional network, we achieved an error rate slightly lower than the best reported result on the MNIST dataset. Finally, an extension of the method is described to learn topographical filter maps.},
	urldate = {2018-06-22},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Ranzato, Marc'Aurelio and Poultney, Christopher and Chopra, Sumit and LeCun, Yann},
	year = {2006},
	pages = {1137--1144}
}

@article{hinton_reducing_2006,
	title = {Reducing the {Dimensionality} of {Data} with {Neural} {Networks}},
	volume = {313},
	copyright = {American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/313/5786/504},
	doi = {10.1126/science.1127647},
	abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.
Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks.
Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks.},
	language = {en},
	number = {5786},
	urldate = {2018-06-22},
	journal = {Science},
	author = {Hinton, G. E. and Salakhutdinov, R. R.},
	month = jul,
	year = {2006},
	pmid = {16873662},
	pages = {504--507},
	file = {Snapshot:/home/dario/Zotero/storage/ST9K4VA6/504.html:text/html}
}

@article{mohamed_acoustic_2012,
	title = {Acoustic {Modeling} {Using} {Deep} {Belief} {Networks}},
	volume = {20},
	issn = {1558-7916},
	url = {https://doi.org/10.1109/TASL.2011.2109382},
	doi = {10.1109/TASL.2011.2109382},
	abstract = {Gaussian mixture models are currently the dominant technique for modeling the emission distribution of hidden Markov models for speech recognition. We show that better phone recognition on the TIMIT dataset can be achieved by replacing Gaussian mixture models by deep neural networks that contain many layers of features and a very large number of parameters. These networks are first pre-trained as a multi-layer generative model of a window of spectral feature vectors without making use of any discriminative information. Once the generative pre-training has designed the features, we perform discriminative fine-tuning using backpropagation to adjust the features slightly to make them better at predicting a probability distribution over the states of monophone hidden Markov models.},
	number = {1},
	urldate = {2018-06-22},
	journal = {Trans. Audio, Speech and Lang. Proc.},
	author = {Mohamed, A. and Dahl, G. E. and Hinton, G.},
	month = jan,
	year = {2012},
	pages = {14--22}
}

@article{dahl_context-dependent_2012,
	title = {Context-{Dependent} {Pre}-{Trained} {Deep} {Neural} {Networks} for {Large}-{Vocabulary} {Speech} {Recognition}},
	volume = {20},
	issn = {1558-7916},
	doi = {10.1109/TASL.2011.2134090},
	abstract = {We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output. The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error. We illustrate the key components of our model, describe the procedure for applying CD-DNN-HMMs to LVSR, and analyze the effects of various modeling choices on performance. Experiments on a challenging business search dataset demonstrate that CD-DNN-HMMs can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs, with an absolute sentence accuracy improvement of 5.8\% and 9.2\% (or relative error reduction of 16.0\% and 23.2\%) over the CD-GMM-HMMs trained using the minimum phone error rate (MPE) and maximum-likelihood (ML) criteria, respectively.},
	number = {1},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Dahl, G. E. and Yu, D. and Deng, L. and Acero, A.},
	month = jan,
	year = {2012},
	keywords = {Acoustics, Artificial neural network–hidden Markov model (ANN-HMM), Artificial neural networks, Context modeling, context-dependent Gaussian mixture model, context-dependent phone, context-dependent pretrained deep neural network, deep belief network, deep belief network pretraining algorithm, deep neural network hidden Markov model (DNN-HMM), DNN-HMM, Gaussian processes, GMM, hidden Markov model, hidden Markov models, Hidden Markov models, large-vocabulary speech recognition, large-vocabulary speech recognition (LVSR), LVSR, Mathematical model, maximum likelihood estimation, maximum-likelihood criteria, minimum phone error rate, MPE, neural nets, relative error reduction, speech recognition, Speech recognition, Training},
	pages = {30--42},
	file = {IEEE Xplore Abstract Record:/home/dario/Zotero/storage/V7HIDKX7/5740583.html:text/html}
}

@article{ciresan_multi-column_2012,
	series = {Selected {Papers} from {IJCNN} 2011},
	title = {Multi-column deep neural network for traffic sign classification},
	volume = {32},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608012000524},
	doi = {10.1016/j.neunet.2012.02.023},
	abstract = {We describe the approach that won the final phase of the German traffic sign recognition benchmark. Our method is the only one that achieved a better-than-human recognition rate of 99.46\%. We use a fast, fully parameterizable GPU implementation of a Deep Neural Network (DNN) that does not require careful design of pre-wired feature extractors, which are rather learned in a supervised way. Combining various DNNs trained on differently preprocessed data into a Multi-Column DNN (MCDNN) further boosts recognition performance, making the system insensitive also to variations in contrast and illumination.},
	urldate = {2018-06-23},
	journal = {Neural Networks},
	author = {Cireşan, Dan and Meier, Ueli and Masci, Jonathan and Schmidhuber, Jürgen},
	month = aug,
	year = {2012},
	keywords = {Deep neural networks, Image classification, Image preprocessing, Traffic signs},
	pages = {333--338},
	file = {ScienceDirect Snapshot:/home/dario/Zotero/storage/LD3J43WM/S0893608012000524.html:text/html}
}

@article{ning_toward_2005,
	title = {Toward automatic phenotyping of developing embryos from videos},
	volume = {14},
	issn = {1057-7149},
	doi = {10.1109/TIP.2005.852470},
	abstract = {We describe a trainable system for analyzing videos of developing C. elegans embryos. The system automatically detects, segments, and locates cells and nuclei in microscopic images. The system was designed as the central component of a fully automated phenotyping system. The system contains three modules 1) a convolutional network trained to classify each pixel into five categories: cell wall, cytoplasm, nucleus membrane, nucleus, outside medium; 2) an energy-based model, which cleans up the output of the convolutional network by learning local consistency constraints that must be satisfied by label images; 3) a set of elastic models of the embryo at various stages of development that are matched to the label images.},
	number = {9},
	journal = {IEEE Transactions on Image Processing},
	author = {Ning, Feng and Delhomme, D. and LeCun, Y. and Piano, F. and Bottou, L. and Barbano, P. E.},
	month = sep,
	year = {2005},
	keywords = {Algorithms, Animals, Artificial Intelligence, automatic phenotyping, Bioinformatics, Biological system modeling, biological techniques, Caenorhabditis elegans, cellular biophysics, convolutional network, Convolutional network, cytoplasm, Embryo, Embryo, Nonmammalian, embryos, energy-based model, Fetal Development, genetics, Genomics, Image Enhancement, Image Interpretation, Computer-Assisted, image segmentation, Image segmentation, microscopic images, Microscopy, Microscopy, Phase-Contrast, Microscopy, Video, Motion pictures, nonlinear filter, nucleus membrane, optical microscopy, Pattern Recognition, Automated, Performance analysis, Phenotype, Reproducibility of Results, Sensitivity and Specificity, Videos},
	pages = {1360--1371},
	file = {IEEE Xplore Abstract Record:/home/dario/Zotero/storage/ZE8APMXJ/1495508.html:text/html}
}

@article{turaga_convolutional_2010,
	title = {Convolutional networks can learn to generate affinity graphs for image segmentation},
	volume = {22},
	issn = {1530-888X},
	doi = {10.1162/neco.2009.10-08-881},
	abstract = {Many image segmentation algorithms first generate an affinity graph and then partition it. We present a machine learning approach to computing an affinity graph using a convolutional network (CN) trained using ground truth provided by human experts. The CN affinity graph can be paired with any standard partitioning algorithm and improves segmentation accuracy significantly compared to standard hand-designed affinity functions. We apply our algorithm to the challenging 3D segmentation problem of reconstructing neuronal processes from volumetric electron microscopy (EM) and show that we are able to learn a good affinity graph directly from the raw EM images. Further, we show that our affinity graph improves the segmentation accuracy of both simple and sophisticated graph partitioning algorithms. In contrast to previous work, we do not rely on prior knowledge in the form of hand-designed image features or image preprocessing. Thus, we expect our algorithm to generalize effectively to arbitrary image types.},
	language = {eng},
	number = {2},
	journal = {Neural Computation},
	author = {Turaga, Srinivas C. and Murray, Joseph F. and Jain, Viren and Roth, Fabian and Helmstaedter, Moritz and Briggman, Kevin and Denk, Winfried and Seung, H. Sebastian},
	month = feb,
	year = {2010},
	pmid = {19922289},
	keywords = {Algorithms, Artificial Intelligence, Image Processing, Computer-Assisted, Mathematical Computing, Mathematical Concepts, Mathematics, Microscopy, Electron, Neural Networks (Computer), Pattern Recognition, Automated},
	pages = {511--538}
}

@article{sermanet_pedestrian_2012,
	title = {Pedestrian {Detection} with {Unsupervised} {Multi}-{Stage} {Feature} {Learning}},
	url = {https://arxiv.org/abs/1212.0142},
	language = {en},
	urldate = {2018-06-22},
	author = {Sermanet, Pierre and Kavukcuoglu, Koray and Chintala, Soumith and LeCun, Yann},
	month = dec,
	year = {2012},
	file = {Full Text PDF:/home/dario/Zotero/storage/AYEHZ8GP/Sermanet et al. - 2012 - Pedestrian Detection with Unsupervised Multi-Stage.pdf:application/pdf;Snapshot:/home/dario/Zotero/storage/WCGFKRGX/1212.html:text/html}
}

@article{vaillant_original_1994,
	title = {Original approach for the localisation of objects in images},
	volume = {141},
	issn = {1350-245X},
	doi = {10.1049/ip-vis:19941301},
	abstract = {An original approach is presented for the localisation of objects in an image which approach is neuronal and has two steps. In the first step, a rough localisation is performed by presenting each pixel with its neighbourhood to a neural net which is able to indicate whether this pixel and its neighbourhood are the image of the search object. This first filter does not discriminate for position. From its result, areas which might contain an image of the object can be selected. In the second step, these areas are presented to another neural net which can determine the exact position of the object in each area. This algorithm is applied to the problem of localising faces in images},
	number = {4},
	journal = {IEE Proceedings - Vision, Image and Signal Processing},
	author = {Vaillant, R. and Monrocq, C. and Cun, Y. Le},
	month = aug,
	year = {1994},
	keywords = {face detection, image analysis, image object localisation, image reconstruction, image segmentation, learning (artificial intelligence), neural net, neural nets},
	pages = {245--250},
	file = {IEEE Xplore Abstract Record:/home/dario/Zotero/storage/4HJ3L29P/318027.html:text/html}
}

@inproceedings{nowlan_convolutional_1995,
	title = {A {Convolutional} {Neural} {Network} {Hand} {Tracker}},
	abstract = {We describe a system that can track a hand in a sequence of video frames and recognize hand gestures in a user-independent manner. The system locates the hand in each video frame and determines if the hand is open or closed. The tracking system is able to track the hand to within {\textbackslash}Sigma10 pixels of its correct location in 99:7\% of the frames from a test set containing video sequences from 18 different individuals captured in 18 different room environments. The gesture recognition network correctly determines if the hand being tracked is open or closed in 99:1\% of the frames in this test set. The system has been designed to operate in real time with existing hardware. 1 Introduction  We describe an image processing system that uses convolutional neural networks to locate the position of a (moving) hand in a video frame, and to track the position of this hand across a sequence of video frames. In addition, for each frame, the system determines if the hand is currently open or closed. The...},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 7},
	publisher = {Morgan Kaufmann},
	author = {Nowlan, Steven J. and Platt, John C.},
	year = {1995},
	pages = {901--908},
	file = {Citeseer - Full Text PDF:/home/dario/Zotero/storage/RT3PGWN8/Nowlan and Platt - 1995 - A Convolutional Neural Network Hand Tracker.pdf:application/pdf;Citeseer - Snapshot:/home/dario/Zotero/storage/LE7BCW5G/summary.html:text/html}
}

@article{garcia_convolutional_2004,
	title = {Convolutional face finder: a neural architecture for fast and robust face detection},
	volume = {26},
	issn = {0162-8828},
	shorttitle = {Convolutional face finder},
	doi = {10.1109/TPAMI.2004.97},
	abstract = {In this paper, we present a novel face detection approach based on a convolutional neural architecture, designed to robustly detect highly variable face patterns, rotated up to ±20 degrees in image plane and turned up to ±60 degrees, in complex real world images. The proposed system automatically synthesizes simple problem-specific feature extractors from a training set of face and nonface patterns, without making any assumptions or using any hand-made design concerning the features to extract or the areas of the face pattern to analyze. The face detection procedure acts like a pipeline of simple convolution and subsampling modules that treat the raw input image as a whole. We therefore show that an efficient face detection system does not require any costly local preprocessing before classification of image areas. The proposed scheme provides very high detection rate with a particularly low level of false positives, demonstrated on difficult test sets, without requiring the use of multiple networks for handling difficult cases. We present extensive experimental results illustrating the efficiency of the proposed approach on difficult test sets and including an in-depth sensitivity analysis with respect to the degrees of variability of the face patterns.},
	number = {11},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Garcia, C. and Delakis, M.},
	month = nov,
	year = {2004},
	keywords = {Algorithms, Artificial Intelligence, Automatic Data Processing, Cluster Analysis, complex real world images, Computer Graphics, Computer Simulation, Convolution, convolution modules, Convolutional codes, convolutional face finder, convolutional networks., convolutional neural architecture, Face detection, face pattern detection, face patterns, face recognition, feature extraction, Feature extraction, hand made design, Handwriting, Humans, image classification, Image Enhancement, Image Interpretation, Computer-Assisted, image sampling, Index Terms- Face detection, Information Storage and Retrieval, learning (artificial intelligence), machine learning, neural net architecture, neural networks, Neural networks, nonface patterns, Numerical Analysis, Computer-Assisted, Pattern analysis, Pattern Recognition, Automated, Pipelines, Reading, Reproducibility of Results, Robustness, sensitivity analysis, Sensitivity and Specificity, Signal Processing, Computer-Assisted, specific feature extractors, subsampling modules, Subtraction Technique, Testing, training set, User-Computer Interface},
	pages = {1408--1423},
	file = {IEEE Xplore Abstract Record:/home/dario/Zotero/storage/2VB87UCC/1335446.html:text/html}
}

@article{osadchy_synergistic_2007,
	title = {Synergistic {Face} {Detection} and {Pose} {Estimation} with {Energy}-{Based} {Models}},
	volume = {8},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=1248659.1248700},
	abstract = {We describe a novel method for simultaneously detecting faces and estimating their pose in real time. The method employs a convolutional network to map images of faces to points on a low-dimensional manifold parametrized by pose, and images of non-faces to points far away from that manifold. Given an image, detecting a face and estimating its pose is viewed as minimizing an energy function with respect to the face/non-face binary variable and the continuous pose parameters. The system is trained to minimize a loss function that drives correct combinations of labels and pose to be associated with lower energy values than incorrect ones. The system is designed to handle very large range of poses without retraining. The performance of the system was tested on three standard data sets---for frontal views, rotated faces, and profiles---is comparable to previous systems that are designed to handle a single one of these data sets. We show that a system trained simuiltaneously for detection and pose estimation is more accurate on both tasks than similar systems trained for each task separately.},
	urldate = {2018-06-23},
	journal = {J. Mach. Learn. Res.},
	author = {Osadchy, Margarita and Cun, Yann Le and Miller, Matthew L.},
	month = may,
	year = {2007},
	pages = {1197--1215},
	file = {ACM Full Text PDF:/home/dario/Zotero/storage/6AIQKGV4/Osadchy et al. - 2007 - Synergistic Face Detection and Pose Estimation wit.pdf:application/pdf}
}

@article{tompson_efficient_2014,
	title = {Efficient {Object} {Localization} {Using} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1411.4280},
	abstract = {Recent state-of-the-art performance on human-body pose estimation has been achieved with Deep Convolutional Networks (ConvNets). Traditional ConvNet architectures include pooling and sub-sampling layers which reduce computational requirements, introduce invariance and prevent over-training. These benefits of pooling come at the cost of reduced localization accuracy. We introduce a novel architecture which includes an efficient `position refinement' model that is trained to estimate the joint offset location within a small region of the image. This refinement model is jointly trained in cascade with a state-of-the-art ConvNet model to achieve improved accuracy in human joint location estimation. We show that the variance of our detector approaches the variance of human annotations on the FLIC dataset and outperforms all existing approaches on the MPII-human-pose dataset.},
	urldate = {2018-06-23},
	journal = {arXiv:1411.4280 [cs]},
	author = {Tompson, Jonathan and Goroshin, Ross and Jain, Arjun and LeCun, Yann and Bregler, Christopher},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.4280},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 8 pages with 1 page of citations},
	file = {arXiv\:1411.4280 PDF:/home/dario/Zotero/storage/Z4TCEPII/Tompson et al. - 2014 - Efficient Object Localization Using Convolutional .pdf:application/pdf;arXiv.org Snapshot:/home/dario/Zotero/storage/9FPA9QFZ/1411.html:text/html}
}

@inproceedings{taigman_deepface:_2014,
	title = {{DeepFace}: {Closing} the {Gap} to {Human}-{Level} {Performance} in {Face} {Verification}},
	shorttitle = {{DeepFace}},
	doi = {10.1109/CVPR.2014.220},
	abstract = {In modern face recognition, the conventional pipeline consists of four stages: detect ={\textgreater} align ={\textgreater} represent ={\textgreater} classify. We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight sharing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an identity labeled dataset of four million facial images belonging to more than 4, 000 identities. The learned representations coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classifier. Our method reaches an accuracy of 97.35\% on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 27\%, closely approaching human-level performance.},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Taigman, Y. and Yang, M. and Ranzato, M. and Wolf, L.},
	month = jun,
	year = {2014},
	keywords = {3D face modeling, Agriculture, alignment step, deep neural network, DeepFace, Face, face recognition, Face recognition, face representation, face verification, human-level performance, image representation, labeled faces in the wild, LFW dataset, neural nets, piecewise affine transformation, representation step, Shape, Solid modeling, Three-dimensional displays, Training},
	pages = {1701--1708},
	file = {IEEE Xplore Abstract Record:/home/dario/Zotero/storage/IFMR5KUP/6909616.html:text/html}
}

@article{hadsell_learning_nodate,
	title = {Learning long-range vision for autonomous off-road driving},
	volume = {26},
	copyright = {Copyright © 2009 Wiley Periodicals, Inc.},
	issn = {1556-4967},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.20276},
	doi = {10.1002/rob.20276},
	abstract = {Most vision-based approaches to mobile robotics suffer from the limitations imposed by stereo obstacle detection, which is short range and prone to failure. We present a self-supervised learning process for long-range vision that is able to accurately classify complex terrain at distances up to the horizon, thus allowing superior strategic planning. The success of the learning process is due to the self-supervised training data that are generated on every frame: robust, visually consistent labels from a stereo module; normalized wide-context input windows; and a discriminative and concise feature representation. A deep hierarchical network is trained to extract informative and meaningful features from an input image, and the features are used to train a real-time classifier to predict traversability. The trained classifier sees obstacles and paths from 5 to more than 100 m, far beyond the maximum stereo range of 12 m, and adapts very quickly to new environments. The process was developed and tested on the LAGR (Learning Applied to Ground Robots) mobile robot. Results from a ground truth data set, as well as field test results, are given. © 2009 Wiley Periodicals, Inc.},
	language = {en},
	number = {2},
	urldate = {2018-06-23},
	journal = {Journal of Field Robotics},
	author = {Hadsell, Raia and Sermanet, Pierre and Ben, Jan and Erkan, Ayse and Scoffier, Marco and Kavukcuoglu, Koray and Muller, Urs and LeCun, Yann},
	pages = {120--144},
	file = {Snapshot:/home/dario/Zotero/storage/DLK5NEN2/rob.html:text/html}
}

@article{farabet_scene_2012,
	title = {Scene {Parsing} with {Multiscale} {Feature} {Learning}, {Purity} {Trees}, and {Optimal} {Covers}},
	url = {http://arxiv.org/abs/1202.2160},
	abstract = {Scene parsing, or semantic segmentation, consists in labeling each pixel in an image with the category of the object it belongs to. It is a challenging task that involves the simultaneous detection, segmentation and recognition of all the objects in the image. The scene parsing method proposed here starts by computing a tree of segments from a graph of pixel dissimilarities. Simultaneously, a set of dense feature vectors is computed which encodes regions of multiple sizes centered on each pixel. The feature extractor is a multiscale convolutional network trained from raw pixels. The feature vectors associated with the segments covered by each node in the tree are aggregated and fed to a classifier which produces an estimate of the distribution of object categories contained in the segment. A subset of tree nodes that cover the image are then selected so as to maximize the average "purity" of the class distributions, hence maximizing the overall likelihood that each segment will contain a single object. The convolutional network feature extractor is trained end-to-end from raw pixels, alleviating the need for engineered features. After training, the system is parameter free. The system yields record accuracies on the Stanford Background Dataset (8 classes), the Sift Flow Dataset (33 classes) and the Barcelona Dataset (170 classes) while being an order of magnitude faster than competing approaches, producing a 320 {\textbackslash}times 240 image labeling in less than 1 second.},
	urldate = {2018-06-23},
	journal = {arXiv:1202.2160 [cs]},
	author = {Farabet, Clément and Couprie, Camille and Najman, Laurent and LeCun, Yann},
	month = feb,
	year = {2012},
	note = {arXiv: 1202.2160},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	annote = {Comment: 9 pages, 4 figures - Published in 29th International Conference on Machine Learning (ICML 2012), Jun 2012, Edinburgh, United Kingdom},
	file = {arXiv\:1202.2160 PDF:/home/dario/Zotero/storage/3RRJN6UT/Farabet et al. - 2012 - Scene Parsing with Multiscale Feature Learning, Pu.pdf:application/pdf;arXiv.org Snapshot:/home/dario/Zotero/storage/362BJN28/1202.html:text/html}
}

@article{collobert_natural_2011,
	title = {Natural {Language} {Processing} (almost) from {Scratch}},
	url = {http://arxiv.org/abs/1103.0398},
	abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.},
	urldate = {2018-06-23},
	journal = {arXiv:1103.0398 [cs]},
	author = {Collobert, Ronan and Weston, Jason and Bottou, Leon and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
	month = mar,
	year = {2011},
	note = {arXiv: 1103.0398},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning},
	file = {arXiv\:1103.0398 PDF:/home/dario/Zotero/storage/XRGDMJ87/Collobert et al. - 2011 - Natural Language Processing (almost) from Scratch.pdf:application/pdf;arXiv.org Snapshot:/home/dario/Zotero/storage/72X2JT3G/1103.html:text/html}
}

@misc{noauthor_deep_nodate,
	title = {Deep convolutional neural networks for {LVCSR} - {Semantic} {Scholar}},
	url = {/paper/Deep-convolutional-neural-networks-for-LVCSR-Sainath-Mohamed/24e555913192d8722f4a0240445bf73db71bd884},
	abstract = {Convolutional Neural Networks (CNNs) are an alternative type of neural network that can be used to reduce spectral variations and model spectral correlations which exist in signals. Since speech signals exhibit both of these properties, CNNs are a more effective model for speech compared to Deep Neural Networks (DNNs). In this paper, we explore applying CNNs to large vocabulary speech tasks. First, we determine the appropriate architecture to make CNNs effective compared to DNNs for LVCSR tasks. Specifically, we focus on how many convolutional layers are needed, what is the optimal number of hidden units, what is the best pooling strategy, and the best input feature type for CNNs. We then explore the behavior of neural network features extracted from CNNs on a variety of LVCSR tasks, comparing CNNs to DNNs and GMMs. We find that CNNs offer between a 13-30\% relative improvement over GMMs, and a 4-12\% relative improvement over DNNs, on a 400-hr Broadcast News and 300-hr Switchboard task.},
	urldate = {2018-06-23},
	file = {Snapshot:/home/dario/Zotero/storage/4TGJZ6IZ/24e555913192d8722f4a0240445bf73db71bd884.html:text/html}
}

@inproceedings{krizhevsky_imagenet_2012,
	address = {USA},
	series = {{NIPS}'12},
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://dl.acm.org/citation.cfm?id=2999134.2999257},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	urldate = {2018-06-23},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 1},
	publisher = {Curran Associates Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	year = {2012},
	pages = {1097--1105}
}

@article{bengio_neural_2003,
	title = {A {Neural} {Probabilistic} {Language} {Model}},
	volume = {3},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v3/bengio03a.html},
	number = {Feb},
	urldate = {2018-06-23},
	journal = {Journal of Machine Learning Research},
	author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Jauvin, Christian},
	year = {2003},
	pages = {1137--1155},
	file = {Full Text PDF:/home/dario/Zotero/storage/PDS4Y4A3/Bengio et al. - 2003 - A Neural Probabilistic Language Model.pdf:application/pdf;Snapshot:/home/dario/Zotero/storage/XJZ3LTPP/bengio03a.html:text/html}
}

@book{noauthor_metaphors_nodate,
	title = {Metaphors {We} {Live} {By}},
	url = {http://www.press.uchicago.edu/ucp/books/book/chicago/M/bo3637992.html},
	abstract = {The now-classic Metaphors We Live By changed our understanding of metaphor and its role in language and the mind. Metaphor, the authors explain, is a fundamental mechanism of mind, one that allows us to use what we know about our physical and social experience to provide understanding of countless other subjects. Because such metaphors structure our most basic understandings of our experience, they are "metaphors we live by"—metaphors that can shape our perceptions and actions without our ever noticing them.In this updated edition of Lakoff and Johnson’s influential book, the authors supply an afterword surveying how their theory of metaphor has developed within the cognitive sciences to become central to the contemporary understanding of how we think and how we express our thoughts in language.},
	urldate = {2018-06-23},
	file = {Snapshot:/home/dario/Zotero/storage/LNF9YJGH/bo3637992.html:text/html}
}

@article{rogers_precis_2008,
	title = {Précis of {Semantic} {Cognition}: {A} {Parallel} {Distributed} {Processing} {Approach}},
	volume = {31},
	issn = {1469-1825, 0140-525X},
	shorttitle = {Précis of {Semantic} {Cognition}},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/precis-of-semantic-cognition-a-parallel-distributed-processing-approach/F3157F4E1DFF90DA60B0542E80586468},
	doi = {10.1017/S0140525X0800589X},
	abstract = {In this précis of our recent book, Semantic Cognition: A Parallel Distributed Processing Approach (Rogers \& McClelland 2004), we present a parallel distributed processing theory of the acquisition, representation, and use of human semantic knowledge. The theory proposes that semantic abilities arise from the flow of activation among simple, neuron-like processing units, as governed by the strengths of interconnecting weights; and that acquisition of new semantic information involves the gradual adjustment of weights in the system in response to experience. These simple ideas explain a wide range of empirical phenomena from studies of categorization, lexical acquisition, and disordered semantic cognition. In this précis we focus on phenomena central to the reaction against similarity-based theories that arose in the 1980s and that subsequently motivated the “theory-theory” approach to semantic knowledge. Specifically, we consider (1) how concepts differentiate in early development, (2) why some groupings of items seem to form “good” or coherent categories while others do not, (3) why different properties seem central or important to different concepts, (4) why children and adults sometimes attest to beliefs that seem to contradict their direct experience, (5) how concepts reorganize between the ages of 4 and 10, and (6) the relationship between causal knowledge and semantic knowledge. The explanations our theory offers for these phenomena are illustrated with reference to a simple feed-forward connectionist model. The relationships between this simple model, the broader theory, and more general issues in cognitive science are discussed.},
	language = {en},
	number = {6},
	urldate = {2018-06-23},
	journal = {Behavioral and Brain Sciences},
	author = {Rogers, Timothy T. and McClelland, James L.},
	month = dec,
	year = {2008},
	keywords = {categorization, causal knowledge, concepts, connectionism, development, innateness, learning, memory, semantics, theory-theory},
	pages = {689--714},
	file = {Snapshot:/home/dario/Zotero/storage/KX89ETRU/F3157F4E1DFF90DA60B0542E80586468.html:text/html}
}

@article{graves_speech_2013,
	title = {Speech {Recognition} with {Deep} {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1303.5778},
	abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates {\textbackslash}emph\{deep recurrent neural networks\}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7\% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
	urldate = {2018-06-23},
	journal = {arXiv:1303.5778 [cs]},
	author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
	month = mar,
	year = {2013},
	note = {arXiv: 1303.5778},
	keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: To appear in ICASSP 2013},
	file = {arXiv\:1303.5778 PDF:/home/dario/Zotero/storage/9T5T3F54/Graves et al. - 2013 - Speech Recognition with Deep Recurrent Neural Netw.pdf:application/pdf;arXiv.org Snapshot:/home/dario/Zotero/storage/VBWQI76B/1303.html:text/html}
}

@incollection{sutskever_sequence_2014,
	title = {Sequence to {Sequence} {Learning} with {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf},
	urldate = {2018-06-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {3104--3112},
	file = {NIPS Snapshort:/home/dario/Zotero/storage/DZ4HFLXS/5346-sequence-to-sequence-learning-with-neural-networks.html:text/html}
}
@article{cho_learning_2014,
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}-{Decoder} for {Statistical} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1406.1078},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	urldate = {2018-06-23},
	journal = {arXiv:1406.1078 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.1078},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: EMNLP 2014},
	file = {arXiv\:1406.1078 PDF:/home/dario/Zotero/storage/ZQPRIKNL/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-.pdf:application/pdf;arXiv.org Snapshot:/home/dario/Zotero/storage/C78D8W33/1406.html:text/html}
}

@article{bahdanau_neural_2014,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2018-06-23},
	journal = {arXiv:1409.0473 [cs, stat]},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.0473},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Accepted at ICLR 2015 as oral presentation},
	file = {arXiv\:1409.0473 PDF:/home/dario/Zotero/storage/KNGWCEWV/Bahdanau et al. - 2014 - Neural Machine Translation by Jointly Learning to .pdf:application/pdf;arXiv.org Snapshot:/home/dario/Zotero/storage/7QIUMHMC/1409.html:text/html}
}

@article{graves_neural_2014,
	title = {Neural {Turing} {Machines}},
	url = {http://arxiv.org/abs/1410.5401},
	abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
	urldate = {2018-06-23},
	journal = {arXiv:1410.5401 [cs]},
	author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
	month = oct,
	year = {2014},
	note = {arXiv: 1410.5401},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1410.5401 PDF:/home/dario/Zotero/storage/3BSWNQSH/Graves et al. - 2014 - Neural Turing Machines.pdf:application/pdf;arXiv.org Snapshot:/home/dario/Zotero/storage/HK897WZA/1410.html:text/html}
}

@article{weston_memory_2014,
	title = {Memory {Networks}},
	url = {http://arxiv.org/abs/1410.3916},
	abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
	urldate = {2018-06-23},
	journal = {arXiv:1410.3916 [cs, stat]},
	author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
	month = oct,
	year = {2014},
	note = {arXiv: 1410.3916},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv\:1410.3916 PDF:/home/dario/Zotero/storage/T6BH9U9W/Weston et al. - 2014 - Memory Networks.pdf:application/pdf;arXiv.org Snapshot:/home/dario/Zotero/storage/UPK5II45/1410.html:text/html}
}

@article{weston_towards_2015,
	title = {Towards {AI}-{Complete} {Question} {Answering}: {A} {Set} of {Prerequisite} {Toy} {Tasks}},
	shorttitle = {Towards {AI}-{Complete} {Question} {Answering}},
	url = {http://arxiv.org/abs/1502.05698},
	abstract = {One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.},
	urldate = {2018-06-23},
	journal = {arXiv:1502.05698 [cs, stat]},
	author = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Rush, Alexander M. and van Merriënboer, Bart and Joulin, Armand and Mikolov, Tomas},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.05698},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv\:1502.05698 PDF:/home/dario/Zotero/storage/TXGRV7IL/Weston et al. - 2015 - Towards AI-Complete Question Answering A Set of P.pdf:application/pdf;arXiv.org Snapshot:/home/dario/Zotero/storage/ZJF4LHUY/1502.html:text/html}
}

@article{felleman_distributed_1991,
	title = {Distributed hierarchical processing in the primate cerebral cortex},
	volume = {1},
	issn = {1047-3211},
	abstract = {In recent years, many new cortical areas have been identified in the macaque monkey. The number of identified connections between areas has increased even more dramatically. We report here on (1) a summary of the layout of cortical areas associated with vision and with other modalities, (2) a computerized database for storing and representing large amounts of information on connectivity patterns, and (3) the application of these data to the analysis of hierarchical organization of the cerebral cortex. Our analysis concentrates on the visual system, which includes 25 neocortical areas that are predominantly or exclusively visual in function, plus an additional 7 areas that we regard as visual-association areas on the basis of their extensive visual inputs. A total of 305 connections among these 32 visual and visual-association areas have been reported. This represents 31\% of the possible number of pathways if each area were connected with all others. The actual degree of connectivity is likely to be closer to 40\%. The great majority of pathways involve reciprocal connections between areas. There are also extensive connections with cortical areas outside the visual system proper, including the somatosensory cortex, as well as neocortical, transitional, and archicortical regions in the temporal and frontal lobes. In the somatosensory/motor system, there are 62 identified pathways linking 13 cortical areas, suggesting an overall connectivity of about 40\%. Based on the laminar patterns of connections between areas, we propose a hierarchy of visual areas and of somatosensory/motor areas that is more comprehensive than those suggested in other recent studies. The current version of the visual hierarchy includes 10 levels of cortical processing. Altogether, it contains 14 levels if one includes the retina and lateral geniculate nucleus at the bottom as well as the entorhinal cortex and hippocampus at the top. Within this hierarchy, there are multiple, intertwined processing streams, which, at a low level, are related to the compartmental organization of areas V1 and V2 and, at a high level, are related to the distinction between processing centers in the temporal and parietal lobes. However, there are some pathways and relationships (about 10\% of the total) whose descriptions do not fit cleanly into this hierarchical scheme for one reason or another. In most instances, though, it is unclear whether these represent genuine exceptions to a strict hierarchy rather than inaccuracies or uncertainities in the reported assignment.},
	language = {eng},
	number = {1},
	journal = {Cerebral Cortex (New York, N.Y.: 1991)},
	author = {Felleman, D. J. and Van Essen, D. C.},
	month = feb,
	year = {1991},
	pmid = {1822724},
	keywords = {Animals, Brain Mapping, Cerebral Cortex, Macaca, Mental Processes},
	pages = {1--47}
}

@misc{noauthor_why_nodate,
	title = {Why {Neurons} {Have} {Thousands} of {Synapses}, {A} {Theory} of {Sequence} {Memory} in {Neocortex}},
	url = {https://numenta.com/resources/papers/why-neurons-have-thousands-of-synapses-theory-of-sequence-memory-in-neocortex/},
	urldate = {2018-06-23},
	file = {Why Neurons Have Thousands of Synapses, A Theory of Sequence Memory in Neocortex:/home/dario/Zotero/storage/9FI832U6/why-neurons-have-thousands-of-synapses-theory-of-sequence-memory-in-neocortex.html:text/html}
}

@misc{noauthor_htm_nodate,
	title = {The {HTM} {Spatial} {Pooler}—{A} {Neocortical} {Algorithm} for {Online} {Sparse} {Distributed} {Coding}},
	url = {https://numenta.com/resources/papers/htm-spatial-pooler-neocortical-algorithm-for-online-sparse-distributed-coding/},
	urldate = {2018-06-23},
	file = {The HTM Spatial Pooler—A Neocortical Algorithm for Online Sparse Distributed Coding:/home/dario/Zotero/storage/IBSEAFLJ/htm-spatial-pooler-neocortical-algorithm-for-online-sparse-distributed-coding.html:text/html}
}

@misc{noauthor_theory_nodate,
	title = {A {Theory} of {How} {Columns} in the {Neocortex} {Enable} {Learning} the {Structure} of the {World}},
	url = {https://numenta.com/resources/papers/a-theory-of-how-columns-in-the-neocortex-enable-learning-the-structure-of-the-world/},
	urldate = {2018-06-23},
	file = {A Theory of How Columns in the Neocortex Enable Learning the Structure of the World:/home/dario/Zotero/storage/HYM4EZCK/a-theory-of-how-columns-in-the-neocortex-enable-learning-the-structure-of-the-world.html:text/html}
}

@article{saffran_statistical_1996,
	title = {Statistical learning by 8-month-old infants},
	volume = {274},
	issn = {0036-8075},
	abstract = {Learners rely on a combination of experience-independent and experience-dependent mechanisms to extract information from the environment. Language acquisition involves both types of mechanisms, but most theorists emphasize the relative importance of experience-independent mechanisms. The present study shows that a fundamental task of language acquisition, segmentation of words from fluent speech, can be accomplished by 8-month-old infants based solely on the statistical relationships between neighboring speech sounds. Moreover, this word segmentation was based on statistical learning from only 2 minutes of exposure, suggesting that infants have access to a powerful mechanism for the computation of statistical properties of the language input.},
	language = {eng},
	number = {5294},
	journal = {Science (New York, N.Y.)},
	author = {Saffran, J. R. and Aslin, R. N. and Newport, E. L.},
	month = dec,
	year = {1996},
	pmid = {8943209},
	keywords = {Discrimination Learning, Humans, Infant, Language Development, Learning, Speech Perception},
	pages = {1926--1928}
}


@article{horton_cortical_2005,
	title = {The cortical column: a structure without a function},
	volume = {360},
	issn = {0962-8436},
	shorttitle = {The cortical column},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1569491/},
	doi = {10.1098/rstb.2005.1623},
	abstract = {This year, the field of neuroscience celebrates the 50th anniversary of Mountcastle's discovery of the cortical column. In this review, we summarize half a century of research and come to the disappointing realization that the column may have no function. Originally, it was described as a discrete structure, spanning the layers of the somatosensory cortex, which contains cells responsive to only a single modality, such as deep joint receptors or cutaneous receptors. Subsequently, examples of columns have been uncovered in numerous cortical areas, expanding the original concept to embrace a variety of different structures and principles. A ‘column’ now refers to cells in any vertical cluster that share the same tuning for any given receptive field attribute. In striate cortex, for example, cells with the same eye preference are grouped into ocular dominance columns. Unaccountably, ocular dominance columns are present in some species, but not others. In principle, it should be possible to determine their function by searching for species differences in visual performance that correlate with their presence or absence. Unfortunately, this approach has been to no avail; no visual faculty has emerged that appears to require ocular dominance columns. Moreover, recent evidence has shown that the expression of ocular dominance columns can be highly variable among members of the same species, or even in different portions of the visual cortex in the same individual. These observations deal a fatal blow to the idea that ocular dominance columns serve a purpose. More broadly, the term ‘column’ also denotes the periodic termination of anatomical projections within or between cortical areas. In many instances, periodic projections have a consistent relationship with some architectural feature, such as the cytochrome oxidase patches in V1 or the stripes in V2. These tissue compartments appear to divide cells with different receptive field properties into distinct processing streams. However, it is unclear what advantage, if any, is conveyed by this form of columnar segregation. Although the column is an attractive concept, it has failed as a unifying principle for understanding cortical function. Unravelling the organization of the cerebral cortex will require a painstaking description of the circuits, projections and response properties peculiar to cells in each of its various areas.},
	number = {1456},
	urldate = {2018-07-10},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Horton, Jonathan C and Adams, Daniel L},
	month = apr,
	year = {2005},
	pmid = {15937015},
	pmcid = {PMC1569491},
	pages = {837--862},
	file = {PubMed Central Full Text PDF:/home/dario/Zotero/storage/DUQQAB99/Horton and Adams - 2005 - The cortical column a structure without a functio.pdf:application/pdf}
}

@article{dematties2018,
author = {Dario Dematties and Silvio Rizzi and George K. Thiruvathukal and Alejandro Wainselboim and Silvano Zanutto},
year = {2018},
title = {Phonetic Acquisition in Cortical Dynamics, a Computational Approach},
journal = {Plos One (Under peer review process)}
}



@article {PMID:17451657,
	Title = {Top-down knowledge supports the retrieval of lexical information from degraded speech},
	Author = {Hannemann, R and Obleser, J and Eulitz, C},
	DOI = {10.1016/j.brainres.2007.03.069},
	Volume = {1153},
	Month = {June},
	Year = {2007},
	Journal = {Brain research},
	ISSN = {0006-8993},
	Pages = {134—143},
	URL = {https://doi.org/10.1016/j.brainres.2007.03.069},
}


@article{OBLESER2011713,
title = "Multiple brain signatures of integration in the comprehension of degraded speech",
journal = "NeuroImage",
volume = "55",
number = "2",
pages = "713 - 723",
year = "2011",
issn = "1053-8119",
doi = "https://doi.org/10.1016/j.neuroimage.2010.12.020",
url = "http://www.sciencedirect.com/science/article/pii/S1053811910016034",
author = "Jonas Obleser and Sonja A. Kotz",
keywords = "EEG, Auditory, Speech comprehension, Semantic processing, N100, N400, Gamma",
abstract = "When listening to speech under adverse conditions, expectancies resulting from semantic context can have a strong impact on comprehension. Here we ask how minimal variations in semantic context (cloze probability) affect the unfolding comprehension of acoustically degraded speech. Three main results are observed in the brain electric response. First, auditory evoked responses to a degraded sentence's onset (N100) correlate with participants' comprehension scores, but are generally more vigorous for more degraded sentences. Second, a pronounced N400 in response to low-cloze sentence-final words, reflecting the integration effort of words into context, increases linearly with improving speech intelligibility. Conversely, transient enhancement in Gamma band power (γ, ~40–70Hz) during high-cloze sentence-final words (~600ms) reflects top-down-facilitated integration. This γ-band effect also varies parametrically with signal quality. Third, a negative correlation of N100 amplitude at sentence onset and the later γ-band response is found in moderately degraded speech. This reflects two partly distinct neural strategies when dealing with moderately degraded speech; a more “bottom-up,” resource-allocating, and effortful versus a more “top-down,” associative and facilitatory strategy. Results also emphasize the non-redundant contributions of phase-locked (evoked) and non-phase-locked (induced) oscillatory brain dynamics in auditory EEG."
}

@article{10.1093/cercor/bhp128,
    author = {Obleser, Jonas and Kotz, Sonja A.},
    title = "{Expectancy Constraints in Degraded Speech Modulate the Language Comprehension Network}",
    journal = {Cerebral Cortex},
    volume = {20},
    number = {3},
    pages = {633-640},
    year = {2009},
    month = {06},
    abstract = "{In speech comprehension, the processing of auditory information and linguistic context are mutually dependent. This functional magnetic resonance imaging study examines how semantic expectancy (“cloze probability”) in variably intelligible sentences (“noise vocoding”) modulates the brain bases of comprehension. First, intelligibility-modulated activation along the superior temporal sulci (STS) was extended anteriorly and posteriorly in low-cloze sentences (e.g., “she weighs the flour”) but restricted to a mid-superior temporal gyrus/STS area in more predictable high-cloze sentences (e.g., “she sifts the flour”). Second, the degree of left inferior frontal gyrus (IFG) (Brodmann's area 44) involvement in processing low-cloze constructions was proportional to increasing intelligibility. Left inferior parietal cortex (IPC; angular gyrus) activation accompanied successful speech comprehension that derived either from increased signal quality or from semantic facilitation. The results show that successful decoding of speech in auditory cortex areas regulates language-specific computation (left IFG and IPC). In return, semantic expectancy can constrain these speech-decoding processes, with fewer neural resources being allocated to highly predictable sentences. These findings offer an important contribution toward the understanding of the functional neuroanatomy in speech comprehension.}",
    issn = {1047-3211},
    doi = {10.1093/cercor/bhp128},
    url = {https://dx.doi.org/10.1093/cercor/bhp128},
    eprint = {http://oup.prod.sis.lan/cercor/article-pdf/20/3/633/1164452/bhp128.pdf},
}


@article{Saffran1996StatisticalLB,
  title={Statistical learning by 8-month-old infants.},
  author={Jenny R. Saffran and Richard N. Aslin and Elissa L. Newport},
  journal={Science},
  year={1996},
  volume={274 5294},
  pages={
          1926-8
        }
}


@misc{dematties_dario_2019_2576130,
  author       = {Dematties, Dario and
                  Thiruvathukal, George K. and
                  Rizzi, Silvio and
                  Wainselboim, Alejandro Javier and
                  Zanutto, Bonifacio Silvano},
  title        = {{Datasets used to train and test the Cortical 
                   Spectro-Temporal Model (CSTM).}},
  month        = feb,
  year         = 2019,
  doi          = {10.5281/zenodo.2576130},
  url          = {https://doi.org/10.5281/zenodo.2576130}
}


@article{10.1093/biomet/75.2.383,
    author = {HOMMEL, G.},
    title = "{A stagewise rejective multiple test procedure based on a modified Bonferroni test}",
    journal = {Biometrika},
    volume = {75},
    number = {2},
    pages = {383-386},
    year = {1988},
    month = {06},
    abstract = "{Simes (1986) has proposed a modified Bonferroni procedure for the test of an overall hypothesis which is the combination of n individual hypotheses. In contrast to the classical Bonferroni procedure, it is not obvious how statements about individual hypotheses are to be made for this procedure. In the present paper a multiple test procedure allowing statements on individual hypotheses is proposed. It is based on the principle of closed test procedures (Marcus, Peritz \\&amp; Gabriel, 1976) and controls the multiple level α.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/75.2.383},
    url = {https://doi.org/10.1093/biomet/75.2.383},
    eprint = {http://oup.prod.sis.lan/biomet/article-pdf/75/2/383/828082/75-2-383.pdf},
}

@ARTICLE{10.3389/fncom.2016.00094,
AUTHOR={Marblestone, Adam H. and Wayne, Greg and Kording, Konrad P.},
TITLE={Toward an Integration of Deep Learning and Neuroscience},
JOURNAL={Frontiers in Computational Neuroscience},
VOLUME={10},
PAGES={94},
YEAR={2016},
URL={https://www.frontiersin.org/article/10.3389/fncom.2016.00094},
DOI={10.3389/fncom.2016.00094}, 
ISSN={1662-5188},
ABSTRACT={Neuroscience has focused on the detailed implementation of computation, studying neural codes, dynamics and circuits. In machine learning, however, artificial neural networks tend to eschew precisely designed codes, dynamics or circuits in favor of brute force optimization of a cost function, often using simple and relatively uniform initial architectures. Two recent developments have emerged within machine learning that create an opportunity to connect these seemingly divergent perspectives. First, structured architectures are used, including dedicated systems for attention, recursion and various forms of short- and long-term memory storage. Second, cost functions and training procedures have become more complex and are varied across layers and over time. Here we think about the brain in terms of these ideas. We hypothesize that (1) the brain optimizes cost functions, (2) the cost functions are diverse and differ across brain locations and over development, and (3) optimization operates within a pre-structured architecture matched to the computational problems posed by behavior. In support of these hypotheses, we argue that a range of implementations of credit assignment through multiple layers of neurons are compatible with our current knowledge of neural circuitry, and that the brain's specialized systems can be interpreted as enabling efficient optimization for specific problem classes. Such a heterogeneously optimized system, enabled by a series of interacting cost functions, serves to make learning data-efficient and precisely targeted to the needs of the organism. We suggest directions by which neuroscience could seek to refine and test these hypotheses.}
}



@article{HOLLE2010875,
title = "Integration of iconic gestures and speech in left superior temporal areas boosts speech comprehension under adverse listening conditions",
journal = "NeuroImage",
volume = "49",
number = "1",
pages = "875 - 884",
year = "2010",
issn = "1053-8119",
doi = "https://doi.org/10.1016/j.neuroimage.2009.08.058",
url = "http://www.sciencedirect.com/science/article/pii/S1053811909009707",
author = "Henning Holle and Jonas Obleser and Shirley-Ann Rueschemeyer and Thomas C. Gunter",
keywords = "Language, fMRI, Multisensory, Audiovisual, Inverse effectiveness",
abstract = "Iconic gestures are spontaneous hand movements that illustrate certain contents of speech and, as such, are an important part of face-to-face communication. This experiment targets the brain bases of how iconic gestures and speech are integrated during comprehension. Areas of integration were identified on the basis of two classic properties of multimodal integration, bimodal enhancement and inverse effectiveness (i.e., greater enhancement for unimodally least effective stimuli). Participants underwent fMRI while being presented with videos of gesture-supported sentences as well as their unimodal components, which allowed us to identify areas showing bimodal enhancement. Additionally, we manipulated the signal-to-noise ratio of speech (either moderate or good) to probe for integration areas exhibiting the inverse effectiveness property. Bimodal enhancement was found at the posterior end of the superior temporal sulcus and adjacent superior temporal gyrus (pSTS/STG) in both hemispheres, indicating that the integration of iconic gestures and speech takes place in these areas. Furthermore, we found that the left pSTS/STG specifically showed a pattern of inverse effectiveness, i.e., the neural enhancement for bimodal stimulation was greater under adverse listening conditions. This indicates that activity in this area is boosted when an iconic gesture accompanies an utterance that is otherwise difficult to comprehend. The neural response paralleled the behavioral data observed. The present data extends results from previous gesture–speech integration studies in showing that pSTS/STG plays a key role in the facilitation of speech comprehension through simultaneous gestural input."
}


@article {Obleser2283,
	author = {Obleser, Jonas and Wise, Richard J. S. and Alex Dresner, M. and Scott, Sophie K.},
	title = {Functional Integration across Brain Regions Improves Speech Perception under Adverse Listening Conditions},
	volume = {27},
	number = {9},
	pages = {2283--2289},
	year = {2007},
	doi = {10.1523/JNEUROSCI.4663-06.2007},
	publisher = {Society for Neuroscience},
	abstract = {Speech perception is supported by both acoustic signal decomposition and semantic context. This study, using event-related functional magnetic resonance imaging, investigated the neural basis of this interaction with two speech manipulations, one acoustic (spectral degradation) and the other cognitive (semantic predictability). High compared with low predictability resulted in the greatest improvement in comprehension at an intermediate level of degradation, and this was associated with increased activity in the left angular gyrus, the medial and left lateral prefrontal cortices, and the posterior cingulate gyrus. Functional connectivity between these regions was also increased, particularly with respect to the left angular gyrus. In contrast, activity in both superior temporal sulci and the left inferior frontal gyrus correlated with the amount of spectral detail in the speech signal, regardless of predictability. These results demonstrate that increasing functional connectivity between high-order cortical areas, remote from the auditory cortex, facilitates speech comprehension when the clarity of speech is reduced.},
	issn = {0270-6474},
	URL = {http://www.jneurosci.org/content/27/9/2283},
	eprint = {http://www.jneurosci.org/content/27/9/2283.full.pdf},
	journal = {Journal of Neuroscience}
}


@inproceedings{Guerguiev2017TowardsDL,
  title={Towards deep learning with segregated dendrites},
  author={Jordan Guerguiev and Timothy P. Lillicrap and Blake A. Richards},
  booktitle={eLife},
  year={2017}
}


@article{Lillicrap_2016,
	doi = {10.1038/ncomms13276},
	url = {https://doi.org/10.1038%2Fncomms13276},
	year = 2016,
	month = {nov},
	publisher = {Springer Nature America, Inc},
	volume = {7},
	number = {1},
	author = {Timothy P. Lillicrap and Daniel Cownden and Douglas B. Tweed and Colin J. Akerman},
	title = {Random synaptic feedback weights support error backpropagation for deep learning},
	journal = {Nature Communications}
}



@article {Sur1437,
	author = {Sur, M and Garraghty, PE and Roe, AW},
	title = {Experimentally induced visual projections into auditory thalamus and cortex},
	volume = {242},
	number = {4884},
	pages = {1437--1441},
	year = {1988},
	doi = {10.1126/science.2462279},
	publisher = {American Association for the Advancement of Science},
	abstract = {Retinal cells have been induced to project into the medial geniculate nucleus, the principal auditory thalamic nucleus, in newborn ferrets by reduction of targets of retinal axons in one hemisphere and creation of alternative terminal space for these fibers in the auditory thalamus. Many cells in the medial geniculate nucleus are then visually driven, have large receptive fields, and receive input from retinal ganglion cells with small somata and slow conduction velocities. Visual cells with long conduction latencies and large contralateral receptive fields can also be recorded in primary auditory cortex. Some visual cells in auditory cortex are direction selective or have oriented receptive fields that resemble those of complex cells in primary visual cortex. Thus, functional visual projections can be routed into nonvisual structures in higher mammals, suggesting that the modality of a sensory thalamic nucleus or cortical area may be specified by its inputs during development.},
	issn = {0036-8075},
	URL = {http://science.sciencemag.org/content/242/4884/1437},
	eprint = {http://science.sciencemag.org/content/242/4884/1437.full.pdf},
	journal = {Science}
}


@article{doi:10.1002/(SICI)1096-9861(19981026)400:3<417::AID-CNE10>3.0.CO;2-O,
author = {Angelucci, Alessandra and Clascá, Francisco and Sur, Mriganka},
title = {Brainstem inputs to the ferret medial geniculate nucleus and the effect of early deafferentation on novel retinal projections to the auditory thalamus},
journal = {Journal of Comparative Neurology},
volume = {400},
number = {3},
pages = {417-439},
keywords = {retinal ganglion cells, plasticity, development, subcortical auditory pathways, inferior colliculus},
doi = {10.1002/(SICI)1096-9861(19981026)400:3<417::AID-CNE10>3.0.CO;2-O},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291096-9861%2819981026%29400%3A3%3C417%3A%3AAID-CNE10%3E3.0.CO%3B2-O},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/%28SICI%291096-9861%2819981026%29400%3A3%3C417%3A%3AAID-CNE10%3E3.0.CO%3B2-O},
abstract = {Abstract Following specific neonatal brain lesions in rodents and ferrets, retinal axons have been induced to innervate the medial geniculate nucleus (MGN). Previous studies have suggested that reduction of normal retinal targets along with deafferentation of the MGN are two concurrent factors required for the induction of novel retino-MGN projections. We have examined, in ferrets, the relative influence of these two factors on the extent of the novel retinal projection. We first characterized the inputs to the normal MGN, and the most effective combination of neonatal lesions to deafferent this nucleus, by injecting retrograde tracers into the MGN of normal and neonatally operated adult ferrets, respectively. In a second group of experiments, newborn ferrets received different combinations of lesions of normal retinal targets and MGN afferents. The resulting extent of retino-MGN projections was estimated for each case at adulthood, by using intraocular injections of anterograde tracers. We found that the extent of retino-MGN projections correlates well with the extent of MGN deafferentation, but not with extent of removal of normal retinal targets. Indeed, the presence of at least some normal retinal targets seems necessary for the formation of retino-MGN connections. The diameters of retino-MGN axons suggest that more than one type of retinal ganglion cells innervate the MGN under a lesion paradigm that spares the visual cortex and lateral geniculate nucleus. We also found that, after extensive deafferentation of MGN, other axonal systems in addition to retinal axons project ectopically to the MGN. These data are consistent with the idea that ectopic retino-MGN projections develop by sprouting of axon collaterals in response to signals arising from the deafferented nucleus, and that these axons compete with other sets of axons for terminal space in the MGN. J. Comp. Neurol. 400:417–439, 1998. © 1998 Wiley-Liss, Inc.},,
year = {1998}
}


@article{Roe1992VisualPR,
  title={Visual projections routed to the auditory pathway in ferrets: receptive fields of visual neurons in primary auditory cortex.},
  author={Anna W. Roe and Sarah L. Pallas and Young Ho Kwon and Mriganka Sur},
  journal={The Journal of neuroscience : the official journal of the Society for Neuroscience},
  year={1992},
  volume={12 9},
  pages={3651-64}
}


@article {Roe818,
	author = {Roe, AW and Pallas, SL and Hahm, JO and Sur, M},
	title = {A map of visual space induced in primary auditory cortex},
	volume = {250},
	number = {4982},
	pages = {818--820},
	year = {1990},
	doi = {10.1126/science.2237432},
	publisher = {American Association for the Advancement of Science},
	abstract = {Maps of sensory surfaces are a fundamental feature of sensory cortical areas of the brain. The relative roles of afferents and targets in forming neocortical maps in higher mammals can be examined in ferrets in which retinal inputs are directed into the auditory pathway. In these animals, the primary auditory cortex contains a systematic representation of the retina (and of visual space) rather than a representation of the cochlea (and of sound frequency). A representation of a two-dimensional sensory epithelium, the retina, in cortex that normally represents a one-dimensional epithelium, the cochlea, suggests that the same cortical area can support different types of maps. Topography in the visual map arises both from thalamocortical projections that are characteristic of the auditory pathway and from patterns of retinal activity that provide the input to the map.},
	issn = {0036-8075},
	URL = {http://science.sciencemag.org/content/250/4982/818},
	eprint = {http://science.sciencemag.org/content/250/4982/818.full.pdf},
	journal = {Science}
}


@article{Sharma2000InductionOV,
  title={Induction of visual orientation modules in auditory cortex},
  author={Jitendra N. Sharma and Alessandra Angelucci and Mriganka Sur},
  journal={Nature},
  year={2000},
  volume={404},
  pages={841-847}
}


@article{mountcastle_1957,
	author = {Mountcastle, V.},
	title = {Modality and topographic properties of cat’s somatic sensory cortex},
	journal = {J. Neurophysiol.},
        year = {1957},
        volume = {20},
        pages = {408–434}
}


@article{doi:10.1098/rstb.1992.0070,
author = {Stuart Rosen  and Robert P. Carlyon  and C. J. Darwin  and Ian John Russell },
title = {Temporal information in speech: acoustic, auditory and linguistic aspects},
journal = {Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences},
volume = {336},
number = {1278},
pages = {367-373},
year = {1992},
doi = {10.1098/rstb.1992.0070}

URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rstb.1992.0070},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rstb.1992.0070}
,
    abstract = { The temporal properties of speech appear to play a more important role in linguistic contrasts than has hitherto been appreciated. Therefore, a new framework for describing the acoustic structure of speech based purely on temporal aspects has been developed. From this point of view, speech can be said to be comprised of three main temporal features, based on dominant fluctuation rates: envelope, periodicity, and fine-structure. Each feature has distinct acoustic manifestations, auditory and perceptual correlates, and roles in linguistic contrasts. The applicability of this three-featured temporal system is discussed in relation to hearing-impaired and normal listeners. }
}


@misc{dematties_dario_2019_2654939,
  author       = {Dematties, Dario and
                  Thiruvathukal, George K. and
                  Rizzi, Silvio and
                  Wainselboim, Alejandro Javier and
                  Zanutto, Bonifacio Silvano},
  title        = {{Experimental Results and Appendices: Cortical 
                   Spectro-Temporal Model (CSTM).}},
  month        = mar,
  year         = 2019,
  doi          = {10.5281/zenodo.2654939},
  url          = {https://doi.org/10.5281/zenodo.2654939}
}


@misc{dematties_dario_2019_2576130,
  author       = {Dematties, Dario and
                  Thiruvathukal, George K. and
                  Rizzi, Silvio and
                  Wainselboim, Alejandro Javier and
                  Zanutto, Bonifacio Silvano},
  title        = {{Datasets used to train and test the Cortical 
                   Spectro-Temporal Model (CSTM).}},
  month        = feb,
  year         = 2019,
  doi          = {10.5281/zenodo.2576130},
  url          = {https://doi.org/10.5281/zenodo.2576130}
}


@misc{dematties_dario_2019_2580396,
  author       = {Dematties, Dario and
                  Thiruvathukal, George K. and
                  Rizzi, Silvio and
                  Wainselboim, Alejandro Javier and
                  Zanutto, Bonifacio Silvano},
  title        = {neurophon/neurophon: Release for PLOS submission},
  month        = feb,
  year         = 2019,
  doi          = {10.5281/zenodo.2580396},
  url          = {https://doi.org/10.5281/zenodo.2580396}
}

@article{HODGKIN199025,
title = "A quantitative description of membrane current and its application to conduction and excitation in nerve",
journal = "Bulletin of Mathematical Biology",
volume = "52",
number = "1",
pages = "25 - 71",
year = "1990",
issn = "0092-8240",
doi = "https://doi.org/10.1016/S0092-8240(05)80004-7",
url = "http://www.sciencedirect.com/science/article/pii/S0092824005800047",
author = "A.L. Hodgkin and A.F. Huxley",
abstract = "This article concludes a series of papers concerned with the flow of electric current through the surface membrane of a giant nerve fibre (Hodgkinet al., 1952,J. Physiol.116, 424–448; Hodgkin and Huxley, 1952,J. Physiol.116, 449–566). Its general object is to discuss the results of the preceding papers (Section 1), to put them into mathematical form (Section 2) and to whow that they will account for conduction and excitation in quantitative terms (Sections 3–6)."
}


@article{Izhikevich2004SpiketimingDO,
  title={Spike-timing dynamics of neuronal groups.},
  author={Eugene M. Izhikevich and Joseph A. Gally and Gerald M. Edelman},
  journal={Cerebral cortex},
  year={2004},
  volume={14 8},
  pages={933-44}
}


@ARTICLE{1333071,
author={E. M. Izhikevich},
journal={IEEE Transactions on Neural Networks},
title={Which model to use for cortical spiking neurons?},
year={2004},
volume={15},
number={5},
pages={1063-1070},
keywords={neural nets;neurophysiology;cortical spiking neurons;biological plausibility;computational efficiency;bursting neurons;cortical neural networks;Neurons;Biological system modeling;Biological neural networks;Artificial neural networks;Information processing;Fires;Large-scale systems;Frequency;Computational efficiency;Computational modeling;Action Potentials;Animals;Cerebral Cortex;Humans;Models, Neurological;Nerve Net;Neural Pathways;Neurons;Nonlinear Dynamics;Reaction Time;Synapses;Synaptic Transmission},
doi={10.1109/TNN.2004.832719},
ISSN={1045-9227},
month={Sept},
}


@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	language = {en},
	number = {7553},
	urldate = {2018-06-19},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444},
	annote = {Trends in Deep Learning Research
Unsupervised learning had a catalytic effect in reviving interest indeep learning, but has since been overshadowed by the successes ofpurely supervised learning. Although we have not focused on it in thisReview, we expect unsupervised learning to become far more importantin the longer term. Human and animal learning is largely unsupervised:we discover the structure of the world by observing it, not by being toldthe name of every object.Human vision is an active process that sequentially samples the opticarray in an intelligent, task-specific way using a small, high-resolutionfovea with a large, low-resolution surround. We expect much of thefuture progress in vision to come from systems that are trained end-to-end and combine ConvNets with RNNs that use reinforcement learningto decide where to look. Systems combining deep learning and rein-forcement learning are in their infancy, but they already outperformpassive vision systems99 at classification tasks and produce impressiveresults in learning to play many different video games100.Natural language understanding is another area in which deep learn-ing is poised to make a large impact over the next few years. We expectsystems that use RNNs to understand sentences or whole documentswill become much better when they learn strategies for selectivelyattending to one part at a time.Ultimately, major progress in artificial intelligence will come aboutthrough systems that combine representation learning with complexreasoning. Although deep learning and simple reasoning have beenused for speech and handwriting recognition for a long time, newparadigms are needed to replace rule-based manipulation of symbolicexpressions by operations on large vectors.},
	file = {LeCun et al. - 2015 - Deep learning.pdf:/home/dario/Zotero/storage/3JHKY65E/LeCun et al. - 2015 - Deep learning.pdf:application/pdf}
}


@article{10.1371/journal.pone.0217966,
    author = {Dematties, Dario AND Rizzi, Silvio AND Thiruvathukal, George K. AND Wainselboim, Alejandro AND Zanutto, B. Silvano},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Phonetic acquisition in cortical dynamics, a computational approach},
    year = {2019},
    month = {06},
    volume = {14},
    url = {https://doi.org/10.1371/journal.pone.0217966},
    pages = {1-28},
    abstract = {Many computational theories have been developed to improve artificial phonetic classification performance from linguistic auditory streams. However, less attention has been given to psycholinguistic data and neurophysiological features recently found in cortical tissue. We focus on a context in which basic linguistic units–such as phonemes–are extracted and robustly classified by humans and other animals from complex acoustic streams in speech data. We are especially motivated by the fact that 8-month-old human infants can accomplish segmentation of words from fluent audio streams based exclusively on the statistical relationships between neighboring speech sounds without any kind of supervision. In this paper, we introduce a biologically inspired and fully unsupervised neurocomputational approach that incorporates key neurophysiological and anatomical cortical properties, including columnar organization, spontaneous micro-columnar formation, adaptation to contextual activations and Sparse Distributed Representations (SDRs) produced by means of partial N-Methyl-D-aspartic acid (NMDA) depolarization. Its feature abstraction capabilities show promising phonetic invariance and generalization attributes. Our model improves the performance of a Support Vector Machine (SVM) classifier for monosyllabic, disyllabic and trisyllabic word classification tasks in the presence of environmental disturbances such as white noise, reverberation, and pitch and voice variations. Furthermore, our approach emphasizes potential self-organizing cortical principles achieving improvement without any kind of optimization guidance which could minimize hypothetical loss functions by means of–for example–backpropagation. Thus, our computational model outperforms multiresolution spectro-temporal auditory feature representations using only the statistical sequential structure immerse in the phonotactic rules of the input stream.},
    number = {6},
    doi = {10.1371/journal.pone.0217966}
}


@ARTICLE{10.3389/fncir.2016.00023,
AUTHOR={Hawkins, Jeff and Ahmad, Subutai},
TITLE={Why Neurons Have Thousands of Synapses, a Theory of Sequence Memory in Neocortex},
JOURNAL={Frontiers in Neural Circuits},
VOLUME={10},
PAGES={23},
YEAR={2016},
URL={https://www.frontiersin.org/article/10.3389/fncir.2016.00023},
DOI={10.3389/fncir.2016.00023},
ISSN={1662-5110},
ABSTRACT={Pyramidal neurons represent the majority of excitatory neurons in the neocortex. Each pyramidal neuron receives input from thousands of excitatory synapses that are segregated onto dendritic branches. The dendrites themselves are segregated into apical, basal, and proximal integration zones, which have different properties. It is a mystery how pyramidal neurons integrate the input from thousands of synapses, what role the different dendrites play in this integration, and what kind of network behavior this enables in cortical tissue. It has been previously proposed that non-linear properties of dendrites enable cortical neurons to recognize multiple independent patterns. In this paper we extend this idea in multiple ways. First we show that a neuron with several thousand synapses segregated on active dendrites can recognize hundreds of independent patterns of cellular activity even in the presence of large amounts of noise and pattern variation. We then propose a neuron model where patterns detected on proximal dendrites lead to action potentials, defining the classic receptive field of the neuron, and patterns detected on basal and apical dendrites act as predictions by slightly depolarizing the neuron without generating an action potential. By this mechanism, a neuron can predict its activation in hundreds of independent contexts. We then present a network model based on neurons with these properties that learns time-based sequences. The network relies on fast local inhibition to preferentially activate neurons that are slightly depolarized. Through simulation we show that the network scales well and operates robustly over a wide range of parameters as long as the network uses a sparse distributed code of cellular activations. We contrast the properties of the new network model with several other neural network models to illustrate the relative capabilities of each. We conclude that pyramidal neurons with thousands of synapses, active dendrites, and multiple integration zones create a robust and powerful sequence memory. Given the prevalence and similarity of excitatory neurons throughout the neocortex and the importance of sequence memory in inference and behavior, we propose that this form of sequence memory may be a universal property of neocortical tissue.}
}


@inproceedings{Bahrampour2015ComparativeSO,
  title={Comparative Study of Deep Learning Software Frameworks},
  author={Soheil Bahrampour and Naveen Ramakrishnan and Lukas Schott and Mohak Shah},
  year={2015}
}

@INPROCEEDINGS{7979887,
author={S. Shi and Q. Wang and P. Xu and X. Chu},
booktitle={2016 7th International Conference on Cloud Computing and Big Data (CCBD)},
title={Benchmarking State-of-the-Art Deep Learning Software Tools},
year={2016},
volume={},
number={},
pages={99-104},
abstract={Deep learning has been shown as a successful machine learning method for a variety of tasks, and its popularity results in numerous open-source deep learning software tools coming to public. Training a deep network is usually a very time-consuming process. To address the huge computational challenge in deep learning, many tools exploit hardware features such as multi-core CPUs and many-core GPUs to shorten the training and inference time. However, different tools exhibit different features and running performance when they train different types of deep networks on different hardware platforms, making it difficult for end users to select an appropriate pair of software and hardware. In this paper, we present our attempt to benchmark several state-of-the-art GPU-accelerated deep learning software tools, including Caffe, CNTK, TensorFlow, and Torch. We focus on evaluating the running time performance (i.e., speed) of these tools with three popular types of neural networks on two representative CPU platforms and three representative GPU platforms. Our contribution is two-fold. First, for end users of deep learning software tools, our benchmarking results can serve as a reference to selecting appropriate hardware platforms and software tools. Second, for developers of deep learning software tools, our in-depth analysis points out possible future directions to further optimize the running performance.},
keywords={benchmark testing;feedforward neural nets;graphics processing units;learning (artificial intelligence);microprocessor chips;performance evaluation;recurrent neural nets;software tools;open-source deep learning software tool benchmarking;machine learning method;deep network training;hardware platforms;GPU-accelerated deep learning software tools;Caffe;CNTK;TensorFlow;Torch;running time performance evaluation;CPU platform;Tools;Machine learning;Neural networks;Benchmark testing;Graphics processing units;Instruction sets;Training;Deep Learning;GPU;Feed-forward Neural Networks;Convolutional Neural Networks;Recurrent Neural Networks},
doi={10.1109/CCBD.2016.029},
ISSN={},
month={Nov},
}


@article{doi:10.3109/0954898X.2012.739292,
author = {Helge Ülo Dinkelbach and Julien Vitay and Frederik Beuth and Fred H Hamker},
title = {Comparison of GPU- and CPU-implementations of mean-firing rate neural networks on parallel hardware},
journal = {Network: Computation in Neural Systems},
volume = {23},
number = {4},
pages = {212-236},
year  = {2012},
publisher = {Taylor & Francis},
doi = {10.3109/0954898X.2012.739292},
    note ={PMID: 23140422},

URL = { 
	https://doi.org/10.3109/0954898X.2012.739292
    
},
eprint = { 
	https://doi.org/10.3109/0954898X.2012.739292
    
}
,
    abstract = { Modern parallel hardware such as multi-core processors (CPUs) and graphics processing units (GPUs) have a high computational power which can be greatly beneficial to the simulation of large-scale neural networks. Over the past years, a number of efforts have focused on developing parallel algorithms and simulators best suited for the simulation of spiking neural models. In this article, we aim at investigating the advantages and drawbacks of the CPU and GPU parallelization of mean-firing rate neurons, widely used in systems-level computational neuroscience. By comparing OpenMP, CUDA and OpenCL implementations towards a serial CPU implementation, we show that GPUs are better suited than CPUs for the simulation of very large networks, but that smaller networks would benefit more from an OpenMP implementation. As this performance strongly depends on data organization, we analyze the impact of various factors such as data structure, memory alignment and floating precision. We then discuss the suitability of the different hardware depending on the networks' size and connectivity, as random or sparse connectivities in mean-firing rate networks tend to break parallel performance on GPUs due to the violation of coalescence. }
}


@misc{noauthor_cooley_nodate,
	title = {Cooley {\textbar} {Argonne} {Leadership} {Computing} {Facility}},
	howpublished = {\url{https://www.alcf.anl.gov/user-guides/cooley}},
	note = {Accessed: 2018-11-24},
	file = {Cooley | Argonne Leadership Computing Facility:/home/dario/Zotero/storage/ZIXLEY4Q/cooley.html:text/html}
}


@book{edited_by_stephen_c._cunnane_human_2010,
	title = {Human brain evolution : the influence of freshwater and marine food resources},
	url = {https://search.library.wisc.edu/catalog/9910250984602121},
	abstract = {xix, 213 pages, 4 unnumbered pages of plates : illustrations (some color) ; 26 cm},
	publisher = {Hoboken, N.J. : Wiley-Blackwell, [2010] ©2010},
	author = {edited by Stephen C. Cunnane, Kathlyn M. Stewart},
	year = {2010},
	annote = {Includes bibliographical references and index.}
}


@book{10.2307/j.ctt5vkr1h,
 ISBN = {9780300181111},
 URL = {http://www.jstor.org/stable/j.ctt5vkr1h},
 abstract = {<p>In this classic work, one of the greatest mathematicians of the twentieth century explores the analogies between computing machines and the living human brain. John von Neumann, whose many contributions to science, mathematics, and engineering include the basic organizational framework at the heart of today's computers, concludes that the brain operates both digitally and analogically, but also has its own peculiar statistical language.</p><p>In his foreword to this new edition, Ray Kurzweil, a futurist famous in part for his own reflections on the relationship between technology and intelligence, places von Neumann's work in a historical context and shows how it remains relevant today.</p>},
 author = {JOHN VON NEUMANN},
 publisher = {Yale University Press},
 title = {The Computer and the Brain},
 year = {1986}
}


@article{Azevedo2009EqualNO,
  title={Equal numbers of neuronal and nonneuronal cells make the human brain an isometrically scaled-up primate brain.},
  author={Frederico A. C. Azevedo and Ludmila R B Carvalho and Lea Tenenholz Grinberg and Jos{\'e} Marcelo Farfel and Renata Eloah de Lucena Ferretti and Renata E. P. Leite and Wilson Jacob Filho and Roberto Lent and Suzana Herculano-Houzel},
  journal={The Journal of comparative neurology},
  year={2009},
  volume={513 5},
  pages={532-41}
}




@article{MAASS19971659,
title = "Networks of spiking neurons: The third generation of neural network models",
journal = "Neural Networks",
volume = "10",
number = "9",
pages = "1659 - 1671",
year = "1997",
issn = "0893-6080",
doi = "https://doi.org/10.1016/S0893-6080(97)00011-7",
url = "http://www.sciencedirect.com/science/article/pii/S0893608097000117",
author = "Wolfgang Maass",
keywords = "Spiking neuron, Integrate-and-fire neutron, Computational complexity, Sigmoidal neural nets, Lower bounds",
abstract = "The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology."
}

@InProceedings{10.1007/978-3-642-03156-4_17,
author="Ghosh-Dastidar, Samanwoy
and Adeli, Hojjat",
editor="Yu, Wen
and Sanchez, Edgar N.",
title="Third Generation Neural Networks: Spiking Neural Networks",
booktitle="Advances in Computational Intelligence",
year="2009",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="167--178",
abstract="Artificial Neural Networks (ANNs) are based on highly simplified brain dynamics and have been used as powerful computational tools to solve complex pattern recognition, function estimation, and classification problems. Throughout their development, ANNs have been evolving towards more powerful and more biologically realistic models. In the last decade, the third generation Spiking Neural Networks (SNNs) have been developed which comprise of spiking neurons. Information transfer in these neurons models the information transfer in biological neurons, i.e., via the precise timing of spikes or a sequence of spikes. Addition of the temporal dimension for information encoding in SNNs yields new insight into the dynamics of the human brain and has the potential to result in compact representations of large neural networks. As such, SNNs have great potential for solving complicated time-dependent pattern recognition problems defined by time series because of their inherent dynamic representation. This article presents an overview of the development of spiking neurons and SNNs within the context of feedforward networks, and provides insight into their potential for becoming the next generation neural networks.",
isbn="978-3-642-03156-4"
}



@article{McCulloch1990ALC,
  title={A logical calculus of the ideas immanent in nervous activity. 1943.},
  author={Warren S. McCulloch and Walter Pitts},
  journal={Bulletin of mathematical biology},
  year={1990},
  volume={52 1-2},
  pages={99-115; discussion 73-97}
}




@book{Valiant:1994:CM:199266,
 author = {Valiant, Leslie G.},
 title = {Circuits of the Mind},
 year = {1994},
 isbn = {0-19-508926-X},
 publisher = {Oxford University Press, Inc.},
 address = {New York, NY, USA},
} 

@article{Abeles1993SpatiotemporalFP,
  title={Spatiotemporal firing patterns in the frontal cortex of behaving monkeys.},
  author={M. Abeles and Hagai Bergman and E Margalit and Eilon Vaadia},
  journal={Journal of neurophysiology},
  year={1993},
  volume={70 4},
  pages={1629-38}
}

@article{bair1994,
  title={Reliable temporal modulation in cortical spike trains in the awake monkey.},
  author={W. Bair and C. Koch and W. T. Newsoine and K. H. Britten},
  journal={Proceeding of Dynamics of Neural Processing},
  year={1994},
  pages={84-88}
}





@ARTICLE{10.3389/fninf.2015.00019,
AUTHOR={Vitay, Julien and Dinkelbach, Helge and Hamker, Fred},
TITLE={ANNarchy: a code generation approach to neural simulations on parallel hardware},
JOURNAL={Frontiers in Neuroinformatics},
VOLUME={9},
PAGES={19},
YEAR={2015},
URL={https://www.frontiersin.org/article/10.3389/fninf.2015.00019},
DOI={10.3389/fninf.2015.00019},
ISSN={1662-5196},
ABSTRACT={Many modern neural simulators focus on the simulation of networks of spiking neurons on parallel hardware. Another important framework in computational neuroscience, rate-coded neural networks, is mostly difficult or impossible to implement using these simulators. We present here the ANNarchy (Artificial Neural Networks architect) neural simulator, which allows to easily define and simulate rate-coded and spiking networks, as well as combinations of both. The interface in Python has been designed to be close to the PyNN interface, while the definition of neuron and synapse models can be specified using an equation-oriented mathematical description similar to the Brian neural simulator. This information is used to generate C++ code that will efficiently perform the simulation on the chosen parallel hardware (multi-core system or graphical processing unit). Several numerical methods are available to transform ordinary differential equations into an efficient C++ code. We compare the parallel performance of the simulator to existing solutions.}
}


@article{HUQQANI2013349,
title = "Multicore and GPU Parallelization of Neural Networks for Face Recognition",
journal = "Procedia Computer Science",
volume = "18",
pages = "349 - 358",
year = "2013",
note = "2013 International Conference on Computational Science",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2013.05.198",
url = "http://www.sciencedirect.com/science/article/pii/S1877050913003414",
author = "Altaf Ahmad Huqqani and Erich Schikuta and Sicen Ye and Peng Chen",
keywords = "Parallel Simulation, Multicores, GPUs, OpenMP, CUDA, Artificial Neural Network",
abstract = "Training of Artificial Neural Networks for large data sets is a time consuming task. Various approaches have been proposed to reduce the efforts, many of them by applying parallelization techniques. In this paper we develop and analyze two novel parallel training approaches for Backpropagation neural networks for face recognition. We focus on two specific paralleliza- tion environments, using on the one hand OpenMP on a conventional multithreaded CPU and CUDA on a GPU. Based on our findings we give guidelines for the efficient parallelization of Backpropagation neural networks on multicore and GPU architectures. Additionally, we present a traversal method finding the best combination of learning rate and momentum term by varying the number of hidden neurons supporting the parallelization efforts."
}


@inproceedings{Schuessler:2011:PTA:1997052.1997062,
 author = {Schuessler, Olena and Loyola, Diego},
 title = {Parallel Training of Artificial Neural Networks Using Multithreaded and Multicore CPUs},
 booktitle = {Proceedings of the 10th International Conference on Adaptive and Natural Computing Algorithms - Volume Part I},
 series = {ICANNGA'11},
 year = {2011},
 isbn = {978-3-642-20281-0},
 location = {Ljubljana, Slovenia},
 pages = {70--79},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=1997052.1997062},
 acmid = {1997062},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
 keywords = {multithreading and multicore, neural network training, pthreads and openMP parallelization},
} 

@inproceedings{Strey2003ACO,
  title={A comparison of OpenMP and MPI for neural network simulations on a SunFire 6800},
  author={Alfred Strey},
  booktitle={PARCO},
  year={2003}
}


@INPROCEEDINGS{6232827,
author={B. P. Gonzalez and G. G. Sánchez and J. P. Donate and P. Cortez and A. S. de Miguel},
booktitle={2012 IEEE Conference on Evolving and Adaptive Intelligent Systems},
title={Parallelization of an evolving Artificial Neural Networks system to Forecast Time Series using OPENMP and MPI},
year={2012},
volume={},
number={},
pages={186-191},
abstract={Time Series Forecasting (TSF) is a key tool to support decision making, for instance by producing better estimates to be used when planning production resources. Artificial Neural Networks (ANN) are innate candidates for TSF due to advantages such as nonlinear learning and noise tolerance. The search for the best ANN is a complex task that strongly affects the forecasting performance while often requiring a high computational time. However, obtaining fast predictions is a relevant issue in several real-world scenarios, such as real-time and control systems. In this work, we present an Evolutionary (EANN) approach for TSF based on Estimation Distribution Algorithm (EDA) that evolves fully connected Artificial Neural Network (EANN). To speed up such approach, we propose the use of two parallel programming standards: Message Passing Interface (MPI) and Open Multi-Processing (OpenMP). Several experiments were held, using five real-world time series with different characteristics and from distinct domains, in order to compare with sequential EANN approach with the MPI and OpenMP parallel variants, under a number of cores that ranged from 1 to 6. Overall, the EANN results are competitive when compared with the popular ForecastPro tool. Moreover, the setup that included the MPI parallelization method and the use of 5 cores lead to the lowest execution times, while making a reasonable use of the available computational resources.},
keywords={decision making;message passing;neural nets;parallel programming;production planning;time series;parallelization;artificial neural networks system;time series forecasting;TSF;decision making;production resources planning;EANN;estimation distribution algorithm;EDA;parallel programming;message passing interface;MPI;open multi-processing;OpenMP;Standards;Yttrium;Biological cells;Artificial neural networks;Computational modeling},
doi={10.1109/EAIS.2012.6232827},
ISSN={},
month={May},
}

@INPROCEEDINGS{6511739,
author={M. Joldos and I. L. Muntean},
booktitle={2013 11th RoEduNet International Conference},
title={Multicore asynchronous simulation of spiking neural networks on the grid},
year={2013},
volume={},
number={},
pages={1-4},
abstract={Dynamics analysis studies of spiking neural network behavior entails the computation of a large number of simulation scenarios. Moreover, when the simulated neural micro-circuits are fairly large, the use of multiple cores in a simulation tends to be beneficial. In this paper we focus on the parallelization of an asynchronous simulation strategy with OpenMP threads and on the management of the neural simulations on compute grids. The first results show a parallel efficiency in the ranges of 0.31-0.98, with hyper-threading disabled or enabled, resp. By using the grid as underlying middleware for performing the simulations, we could generate and manage hundreds of simulation scenarios in an easy way.},
keywords={biology computing;digital simulation;grid computing;message passing;middleware;multiprocessing systems;multi-threading;neural nets;neurophysiology;parallel processing;multicore asynchronous simulation;dynamics analysis;spiking neural network behavior;simulation scenario;neural microcircuit simulation;asynchronous simulation strategy parallelization;OpenMP thread;neural simulation management;compute grid;parallel efficiency;hyperthreading;middleware;Computational modeling;Neurons;Integrated circuit modeling;Instruction sets;Biological system modeling;Multicore processing;Neural microtechnology;Grid computing;Spiking Neural Networks;Asynchronous Simulation;Globus Toolkit;Globus Online Client},
doi={10.1109/RoEduNet.2013.6511739},
ISSN={2068-1038},
month={Jan},
}


@inproceedings{Chatzikonstantis:2016:FID:2903150.2903477,
 author = {Chatzikonstantis, George and Rodopoulos, Dimitrios and Nomikou, Sofia and Strydis, Christos and De Zeeuw, Chris I. and Soudris, Dimitrios},
 title = {First Impressions from Detailed Brain Model Simulations on a Xeon/Xeon-Phi Node},
 booktitle = {Proceedings of the ACM International Conference on Computing Frontiers},
 series = {CF '16},
 year = {2016},
 isbn = {978-1-4503-4128-8},
 location = {Como, Italy},
 pages = {361--364},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/2903150.2903477},
 doi = {10.1145/2903150.2903477},
 acmid = {2903477},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MPI, OpenMP, neuron modeling, performance},
} 


@INPROCEEDINGS{6821186,
author={M. Joldos and O. Vinteler and I. R. Peter and I. L. Muntean},
booktitle={2013 15th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing},
title={MPI-Based Asynchronous Simulation of Spiking Neural Networks on the Grid},
year={2013},
volume={},
number={},
pages={481-487},
abstract={Brain microcircuits exhibit an almost chaotic behavior. This is of high interest in developing new computational models or designing high capacity storage systems. Therefore, the simulation of such microcircuits must preserve the brain dynamic behavior. But investigating the dynamics analysis of such systems is a complex computational task due to the large number of neurons and synapses in the network, the large number of simulation scenarios that need to be computed, and to their model representation. To address the first challenge, we propose in this paper an MPI-based parallelization scheme of the asynchronous spiking neural network simulation algorithm. Due to the partitioning method, we can compute scenarios with more than 50.000 neurons and 300 millions synapses. The proposed solution has been evaluated on production HPC systems, employing up to 512 parallel processes. We contribute to the second challenge by extending the fACIBiNET framework with client-side capabilities for the Globus Online service. As such, scenarios with both high-throughput and high-performance computing requirements are managed in an efficient manner from within the framework, using grid technologies.},
keywords={grid computing;integrated circuit noise;integrated circuits;message passing;neural nets;MPI-based asynchronous simulation;spiking neural networks;brain microcircuits;chaotic behavior;computational models;high capacity storage systems;brain dynamic behavior;dynamics analysis;complex computational task;neurons;synapses;MPI-based parallelization scheme;asynchronous spiking neural network simulation algorithm;partitioning method;production HPC systems;fACIBiNET framework;client-side capabilities;Globus Online service;high performance computing requirements;grid technologies;Neurons;Computational modeling;Biological system modeling;Brain modeling;Delays;Mathematical model;Neural microtechnology;Grid computing;Spiking Neural Networks;Asynchronous Simulation},
doi={10.1109/SYNASC.2013.69},
ISSN={},
month={Sept},
}


@article{Pastorelli2015ScalingT1,
  title={Scaling to 1024 software processes and hardware cores of the distributed simulation of a spiking neural network including up to 20G synapses},
  author={Elena Pastorelli and Pier Stanislao Paolucci and Roberto Ammendola and Andrea Biagioni and Ottorino Frezza and Francesca Lo Cicero and Alessandro Lonardo and Michele Martinelli and Francesco Simula and Piero Vicini},
  journal={CoRR},
  year={2015},
  volume={abs/1511.09325}
}


@book{hu2012biophysically,
  title={Biophysically Accurate Brain Modeling and Simulation Using Hybrid MPI/OpenMP Parallel Processing},
  author={Hu, J.},
  url={https://books.google.com.ar/books?id=F3pVAQAACAAJ},
  year={2012},
  publisher={Texas A \& M University}
}


@INPROCEEDINGS{7733347,
author={S. Ristov and R. Prodan and M. Gusev and K. Skala},
booktitle={2016 Federated Conference on Computer Science and Information Systems (FedCSIS)},
title={Superlinear speedup in HPC systems: Why and when?},
year={2016},
volume={},
number={},
pages={889-898},
abstract={The speedup is usually limited by two main laws in high-performance computing, that is, the Amdahl's and Gustafson's laws. However, the speedup sometimes can reach far beyond the limited linear speedup, known as superlinear speedup, which means that the speedup is greater than the number of processors that are used. Although the superlinear speedup is not a new concept and many authors have already reported its existence, most of them reported it as a side effect, without explaining why and how it is happening. In this paper, we analyze several different superlinear speedup types and define a taxonomy for them. Additionally, we present several explanations and cases of superlinearity existence for different types of granular algorithms (tasks), which means that they can be divided into many sub-tasks and scattered to the processors for execution. Apart from frequent explanation that having more cache memory in parallel execution is the main reason, we summarize other different effects that cause the superlinearity, including the superlinear speedup in cloud virtual environment for both vertical and horizontal scaling.},
keywords={cache storage;cloud computing;multiprocessing systems;parallel processing;superlinear speedup;HPC systems;granular algorithms;cache memory;parallel execution;cloud virtual environment;high-performance computing;Program processors;Cloud computing;Cache memory;Clocks;Complexity theory;Throughput;Synchronization;Cache memory;load;parallel and distributed processing;performance},
doi={},
ISSN={},
month={Sept},
}
