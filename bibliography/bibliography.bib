@misc{audacity,
  title = {Audacity 2.0.5 free, open source, cross-platform software for recording and editing sounds.
Audacity® software is copyright © Audacity Team},
  howpublished = {\url{https://audacityteam.org/}},
  note = {It is free software distributed under the terms of the GNU General Public License.
The name Audacity® is a registered trademark of Dominic Mazzoni.},
  year = {1999-2018}
}



@misc{libsvm,
  title = {libsvm Version 3.22 released on December 22},
  year = {2016}
}



@misc{sable,
  title = {SABLE cross synthesizer standard mark up language},
  howpublished = {\url{https://www.cs.cmu.edu/~awb/festival_demos/sable.html}},
  note = {Accessed: 2018-04-27}
}



@misc{fftw,
  title = {FFTW package},
  howpublished = {\url{http://www.fftw.org/}},
  note = {Accessed: 2018-04-27}
}



@Misc{festival2014,
title = {Festival Speech Synthesis System 2.4: release},
howpublished = {Copyright (C) University of Edinburgh, 1996-2010. All rights reserved.
clunits: Copyright (C) University of Edinburgh and CMU 1997-2010
clustergen\_engine: Copyright (C) Carnegie Mellon University 2005-2014
hts\_engine: 
The HMM-Based Speech Synthesis Engine "hts\_engine API"
hts\_engine API version 1.07 (\url{http://hts-engine.sourceforge.net/})
Copyright (C) The HMM-Based Speech Synthesis Engine "hts\_engine API"
Version 1.07 (\url{http://hts-engine.sourceforge.net/})
Copyright (C) 2001-2012 Nagoya Institute of Technology
              2001-2008 Tokyo Institute of Technology
All rights reserved.
\url{http://www.cstr.ed.ac.uk/projects/festival/}},
year = {December 2014}
}





@article{Marques2018,
author = {Marques, Tiago and Nguyen, Julia and Fioreze, Gabriela and Petreanu, Leopoldo},
year = {2018},
month = {04},
pages = {1546-1726},
title = {The functional organization of cortical feedback inputs to primary visual cortex},
journal = {Nature Neuroscience}
}



@article{reiter_1998,
author = {Reiter, H.O. and Stryker, Michael},
year = {1988},
month = {06},
pages = {3623-7},
title = {Neural Plasticity without Postsynaptic Action Potentials: Less-Active Inputs become Dominant When Kitten Visual Cortical Cells are Pharmacologically Inhibited},
volume = {85},
booktitle = {Proceedings of the National Academy of Sciences of the United States of America}
}



@InProceedings{silos_2016,
author="de-la-Calle-Silos, F.
and Gallardo-Antol{\'i}n, A.
and Pel{\'a}ez-Moreno, C.",
editor="Abad, Alberto
and Ortega, Alfonso
and Teixeira, Ant{\'o}nio 
and Garc{\'i}a Mateo, Carmen
and Mart{\'i}nez Hinarejos, Carlos D.
and Perdig{\~a}o, Fernando
and Batista, Fernando
and Mamede, Nuno",
title="An Analysis of Deep Neural Networks in Broad Phonetic Classes for Noisy Speech Recognition",
booktitle="Advances in Speech and Language Technologies for Iberian Languages",
year="2016",
publisher="Springer International Publishing",
address="Cham",
pages="87--96",
abstract="The introduction of Deep Neural Network (DNN) based acoustic models has produced dramatic improvements in performance. In particular, we have recently found that Deep Maxout Networks, a modification of DNNs' feed-forward architecture that uses a max-out activation function, provides enhanced robustness to environmental noise. In this paper we further investigate how these improvements are translated into the different broad phonetic classes and how does it compare to classical Hidden Markov Models (HMM) based back-ends. Our experiments demonstrate that performance is still tightly related to the particular phonetic class being stops and affricates the least resilient but also that relative improvements of both DNN variants are distributed unevenly across those classes having the type of noise a significant influence on the distribution. A combination of the different systems DNN and classical HMM is also proposed to validate our hypothesis that the traditional GMM/HMM systems have a different type of error than the Deep Neural Networks hybrid models.",
isbn="978-3-319-49169-1"
}


@article {Javitt11962,
	author = {Javitt, D C and Steinschneider, M and Schroeder, C E and Arezzo, J C},
	title = {Role of cortical N-methyl-D-aspartate receptors in auditory sensory memory and mismatch negativity generation: implications for schizophrenia},
	volume = {93},
	number = {21},
	pages = {11962--11967},
	year = {1996},
	publisher = {National Academy of Sciences},
	abstract = {Working memory refers to the ability of the brain to store and manipulate information over brief time periods, ranging from seconds to minutes. As opposed to long-term memory, which is critically dependent upon hippocampal processing, critical substrates for working memory are distributed in a modality-specific fashion throughout cortex. N-methyl-D-aspartate (NMDA) receptors play a crucial role in the initiation of long-term memory. Neurochemical mechanisms underlying the transient memory storage required for working memory, however, remain obscure. Auditory sensory memory, which refers to the ability of the brain to retain transient representations of the physical features (e.g., pitch) of simple auditory stimuli for periods of up to approximately 30 sec, represents one of the simplest components of the brain working memory system. Functioning of the auditory sensory memory system is indexed by the generation of a well-defined event-related potential, termed mismatch negativity (MMN). MMN can thus be used as an objective index of auditory sensory memory functioning and a probe for investigating underlying neurochemical mechanisms. Monkeys generate cortical activity in response to deviant stimuli that closely resembles human MMN. This study uses a combination of intracortical recording and pharmacological micromanipulations in awake monkeys to demonstrate that both competitive and noncompetitive NMDA antagonists block the generation of MMN without affecting prior obligatory activity in primary auditory cortex. These findings suggest that, on a neurophysiological level, MMN represents selective current flow through open, unblocked NMDA channels. Furthermore, they suggest a crucial role of cortical NMDA receptors in the assessment of stimulus familiarity/unfamiliarity, which is a key process underlying working memory performance.},
	issn = {0027-8424},
	URL = {http://www.pnas.org/content/93/21/11962},
	eprint = {http://www.pnas.org/content/93/21/11962.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}


@article{nachum_2003,
author = {Ulanovsky, Nachum and Las, Liora and Nelken, Israel},
year = {2003},
month = {05},
pages = {391-8},
title = {Processing of low-probability sounds by cortical neurons},
volume = {6},
booktitle = {Nature neuroscience}
}


@inproceedings{Natan2015ComplementaryCO,
  title={Complementary control of sensory adaptation by two types of cortical interneurons},
  author={Ryan G Natan and John J Briguglio and Laetitia Mwilambwe-Tshilobo and Sara I Jones and Mark Aizenberg and Ethan M Goldberg and Maria Neimark Geffen},
  booktitle={eLife},
  year={2015}
}


@article{kuhl_1975,
author = {K Kuhl, P and Miller, James},
year = {1975},
month = {11},
pages = {69-72},
title = {Speech Perception by the Chinchilla: Voiced-Voiceless Distinction in Alveolar Plosive Consonants},
volume = {190},
booktitle = {Science (New York, N.Y.)}
}


@article {Meyer19113,
	author = {Meyer, Hanno S. and Egger, Robert and Guest, Jason M. and Foerster, Rita and Reissl, Stefan and Oberlaender, Marcel},
	title = {Cellular organization of cortical barrel columns is whisker-specific},
	volume = {110},
	number = {47},
	pages = {19113--19118},
	year = {2013},
	doi = {10.1073/pnas.1312691110},
	publisher = {National Academy of Sciences},
	abstract = {Cortical columns are thought to be the elementary functional building blocks of sensory cortices. Here we show that the cellular architecture of cortical {\textquotedblleft}barrel{\textquotedblright} columns in rodent somatosensory cortex is not stereotypic, but specific for each whisker on the animals{\textquoteright} snout. Our findings challenge the concepts underlying contemporary simulation efforts that build up large-scale network models of repeatedly occurring identical cortical circuits.The cellular organization of the cortex is of fundamental importance for elucidating the structural principles that underlie its functions. It has been suggested that reconstructing the structure and synaptic wiring of the elementary functional building block of mammalian cortices, the cortical column, might suffice to reverse engineer and simulate the functions of entire cortices. In the vibrissal area of rodent somatosensory cortex, whisker-related {\textquotedblleft}barrel{\textquotedblright} columns have been referred to as potential cytoarchitectonic equivalents of functional cortical columns. Here, we investigated the structural stereotypy of cortical barrel columns by measuring the 3D neuronal composition of the entire vibrissal area in rat somatosensory cortex and thalamus. We found that the number of neurons per cortical barrel column and thalamic {\textquotedblleft}barreloid{\textquotedblright} varied substantially within individual animals, increasing by \~{}2.5-fold from dorsal to ventral whiskers. As a result, the ratio between whisker-specific thalamic and cortical neurons was remarkably constant. Thus, we hypothesize that the cellular architecture of sensory cortices reflects the degree of similarity in sensory input and not columnar and/or cortical uniformity principles.},
	issn = {0027-8424},
	URL = {http://www.pnas.org/content/110/47/19113},
	eprint = {http://www.pnas.org/content/110/47/19113.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}


@article{BRENT199693,
title = "Distributional regularity and phonotactic constraints are useful for segmentation",
journal = "Cognition",
volume = "61",
number = "1",
pages = "93 - 125",
year = "1996",
note = "Compositional Language Acquisition",
issn = "0010-0277",
doi = "https://doi.org/10.1016/S0010-0277(96)00719-6",
url = "http://www.sciencedirect.com/science/article/pii/S0010027796007196",
author = "Michael R. Brent and Timothy A. Cartwright"
}


@article{saffran_1997,
author = {Saffran, Jenny and Newport, Elissa and N. Aslin, Richard and A. Tunick, Rachel and Barrueco, Sandra},
year = {1997},
month = {03},
pages = {101-105},
title = {Incidental Language Learning: Listening (and Learning) Out of the Corner of Your Ear},
volume = {8},
booktitle = {Psychological Science - PSYCHOL SCI}
}


@article{CC01a,
 author = {Chang, Chih-Chung and Lin, Chih-Jen},
 title = {{LIBSVM}: A library for support vector machines},
 journal = {ACM Transactions on Intelligent Systems and Technology},
 volume = {2},
 issue = {3},
 year = {2011},
 pages = {27:1--27:27},
 note =	 {Software available at \url{http://www.csie.ntu.edu.tw/~cjlin/libsvm}}
}


@Article{FFTW05,
  author = 	 {Frigo, Matteo and Johnson, Steven~G.},
  title = 	 {The Design and Implementation of {FFTW3}},
  journal = 	 {Proceedings of the IEEE},
  year = 	 2005,
  volume =	 93,
  number =	 2,
  pages =	 {216--231},
  note =	 {Special issue on ``Program Generation, Optimization, and Platform Adaptation''}
}


@article{cui_2016,
author = {Yuwei Cui and Subutai Ahmad and Jeff Hawkins},
title = {Continuous Online Sequence Learning with an Unsupervised Neural Network Model},
journal = {Neural Computation},
volume = {28},
number = {11},
pages = {2474-2504},
year = {2016},
doi = {10.1162/NECO\_a\_00893},
    note ={PMID: 27626963},

URL = { 
        https://doi.org/10.1162/NECO_a_00893
    
},
eprint = { 
        https://doi.org/10.1162/NECO_a_00893
    
}
,
    abstract = { The ability to recognize and predict temporal sequences of sensory inputs is vital for survival in natural environments. Based on many known properties of cortical neurons, hierarchical temporal memory (HTM) sequence memory recently has been proposed as a theoretical framework for sequence learning in the cortex. In this letter, we analyze properties of HTM sequence memory and apply it to sequence learning and prediction problems with streaming data. We show the model is able to continuously learn a large number of variable order temporal sequences using an unsupervised Hebbian-like learning rule. The sparse temporal codes formed by the model can robustly handle branching temporal sequences by maintaining multiple predictions until there is sufficient disambiguating evidence. We compare the HTM sequence memory with other sequence learning algorithms, including statistical methods—autoregressive integrated moving average; feedforward neural networks—time delay neural network and online sequential extreme learning machine; and recurrent neural networks—long short-term memory and echo-state networks on sequence prediction problems with both artificial and real-world data. The HTM model achieves comparable accuracy to other state-of-the-art algorithms. The model also exhibits properties that are critical for sequence learning, including continuous online learning, the ability to handle multiple predictions and branching sequences with high-order statistics, robustness to sensor noise and fault tolerance, and good performance without task-specific hyperparameter tuning. Therefore, the HTM sequence memory not only advances our understanding of how the brain may solve the sequence learning problem but is also applicable to real-world sequence learning problems from continuous data streams. }
}


@article{eatock_2000,
	author = {Eatock, R., A.},
	title = {Adaptation in hair cells.},
	journal = {Annu. Rev. Neurosci.},
        year = {2000},
        volume = {23},
	pages = {285–314}
}


@article{holt_2000,
	author = {Holt, J., R, and Corey, D., P.},
	title = {Two mechanisms for transducer adaptation in vertebrate hair cells.},
	journal = {Proc. Natl. Acad. Sci. USA.},
        year = {2000},
        volume = {97},
	pages = {11730–11735}
}


@article{le_goff_2005,
	author = {Le Goff, L and Bozovic, D and Hudspeth, A. J.},
	title = {Adaptive shift in the domain of negative stiffness during spontaneous oscillation by hair bundles from the internal ear.},
	journal = {Proc. Natl. Acad. Sci. USA.},
        year = {2005},
        volume = {102},
	pages = {16996–17001}
}


@article{shamma_1993,
	author = {Shama, S. A. and Fleshman, J. W. and Wiser, P. W. and Versnel, H.},
	title = {Organization of response areas in ferret primary auditory cortex.},
	journal = {Neurophysiol.},
        year = {1993},
        volume = {69},
        month = {Feb.},
	pages = {367-383}
}


@article{schreiner_1990,
	author = {Schreiner, C. E. and Sutler, M. L.},
	title = {Functional topography of cat primary auditory cortex: distribution of integrated excitation.},
	journal = {J.Neurophys.},
        year = {1990},
        volume = {64},
        month = {Nov.},
	pages = {1442-1459}
}


@article{heil_1992,
	author = {Heil, P. and Rajan, R. and Irvine, D. R.},
	title = {Sensitivity of neurons in cat primary auditory cortex to tones and frequency-modulated stimuli 11: Organization of response properties along the ‘isofrequency‘ dimension.},
	journal = {Hearing Res.},
        year = {1992},
        volume = {63},
        month = {Nov.},
	pages = {135-156}
}


@article{mendelson_1985,
	author = {Mendelson, J. R. and Cynader, M. S.},
	title = {Sensitivity of cat primary auditory cortex (AI) neurons to the direction and rate of frequency modulation.},
	journal = {Brain Res.},
        year = {1985},
        volume = {327},
	pages = {331-335}
}


@article{antic_2010,
	author = {Antic, S. D. and Zhou, W. L. and Moore, A. R. and Short, S. M. and Ikonomu, K. D.},
	title = {The decade of the dendritic NMDA spike.},
	journal = {J. Neurosci. Res.},
        year = {2010},
        volume = {88},
	pages = {2991–3001}
}


@article{major_2013,
	author = {Major, G. and Larkum, M. E. and Schiller, J.},
	title = {Active properties of neocortical pyramidal neuron dendrites.},
	journal = {Annu. Rev. Neurosci.},
        year = {2013},
        volume = {36},
	pages = {1–24}
}


@article{wang_1995,
	author = {Kuansan, Wang and Shihab, A., Shamma},
	title = {Spectral Shape Analysis in the Central Auditory System},
	journal = {IEEE TRANSACTIONS ON SPEECH AND AUDIO PROCESSING.},
        year = {1995},
        volume = {3},
	number = {5},
	month = {sept},
	pages = {382-395}
}


@article{ahmad_2015,
	author = {Ahmad, S. and Hawkins, J.},
	title = {Properties of Sparse Distributed Representations and their Application to Hierarchical Temporal Memory},
	journal = {https://arxiv.org/pdf/1503.07469.pdf.},
        year = {2015},
	month = {March}
}


@article{kuhl_1983,
	author = {Kuhl, P. K. and Padden, D. M.},
	title = {Enhanced discriminability at the phonetic boundaries for the place feature in macaques},
	journal = {J. Acoust. Soc. Am.},
        year = {1983},
        volume = {73},
        pages = {1003-1010}
}


@article{kluender_1998,
	author = {Kluender, K. R. and  Lotto, A. J. and  Holt, L. L. and  Bloedel, S. L.},
	title = {Role of experience for language-specific functional mappings of vowel sounds},
	journal = {J. Acoust. Soc. Am.},
        year = {1998},
	volume = {104},
	pages = {3568–3582}
}


@article{pons_2006,
	author = {Pons, F.},
	title = {The effects of distributional learning on rats sensitivity to phonetic information},
	journal = {J. Exp. Psychol. Anim. Behav.},
        year = {2006},
        volume = {32},
        pages = {97–101}
}


@article{hienz_1996,
	author = {Hienz, R. D.and Aleszczyk, C. M.and May, B. J.},
	title = {Vowel discrimination in cats: Acquisition, effects of stimulus level, and performance in noise},
	journal = {J. Acoust. Soc. Am.},
        year = {1996},
        volume = {99},
        pages = {3656–3668}
}


@article{dent_1997,
	author = {Dent, M. L. and Brittan-Powell, E. F. and Dooling, R. J. and Pierce, A.},
	title = {Perception of synthetic /ba/-/wa/ speech continuum by budgerigars (Melopsittacus undulatus)},
	journal = {J. Acoust. Soc. Am.},
        year = {1997},
        volume = {102},
        pages = {1891–1897}
}


@article{lotto_1997,
	author = {Lotto, A. J. and Kluender, K. R. and Holt, L. L.},
	title = {Perceptual compensation for coarticulation by Japanese quail (Coturnix coturnix japonica)},
	journal = {J. Acoust. Soc. Am.},
        year = {1997},
        volume = {102},
        pages = {1134–1140}
}


@article{rasanen_2012,
	author = {R\"{a}s\"{a}nen, O.},
	title = {Computational modeling of phonetic and lexical learning in early language acquisition: Existing models and future directions},
	journal = {Speech Communication.},
        year = {2012},
        volume = {54},
        pages = {975–997}
}


@article{appelbaum_1996,
	author = {Appelbaum, I.},
	title = {The lack of invariance problem and the goal of speech perception},
	journal = {Spoken Language, 1996. ICSLP 96. Proceedings., Fourth International Conference on},
        year = {1996},
        volume = {3}
}


@article{lecun_2015,
	author = {LeCun, Y. and Bengio, Y. and Hinton, G.},
	title = {Deep learning},
	journal = {Nature},
        year = {2015},
        volume = {521},
	pages = {436-444}
}


@unpublished{Hawkins-et-al-2016-Book,
   title={Biological and Machine Intelligence (BAMI)},
   author={Hawkins, J. and Ahmad, S. and Purdy, S. and Lavin, A.},
   note={Initial online release 0.4},
   url={http://numenta.com/biological-and-machine-intelligence/},
   year={2016}
}


@article{scharenborg_2010,
	author = {Scharenborg, O. and Boves, L.},
	title = {Computational modelling of spoken-word recognition processes},
	journal = {John Benjamins Publishing Company.},
        year = {2010}
}


@article{dominey_2000,
	author = {Dominey, P. F. and Ramus, F.},
	title = {Neural network processing of natural language: I. Sensitivity to serial, temporal and abstract structure of language in the infant},
	journal = {Language and Cognitive Processes.},
        year = {2000},
        volume = {15},
        pages = {87-127}
}


@article{boer_2003,
	author = {de Boer, B. and Kuhl, P.},
	title = {Investigating the role of infant-directed speech with a computer model},
	journal = {Acoustics Research Letters Online.},
        year = {2003},
        volume = {4},
        pages = {129–134}
}


@article{vallabha_2007,
	author = {Vallabha, G. K. and McLelland, J. L. and Pons, F. and Werker, J. F. and Amano, S.},
	title = {Unsupervised learning of vowel categories from infant-directed speech},
	journal = {Proceedings of National Academy of Sciences.},
        year = {2007},
        volume = {104},
        pages = {13273–13278}
}


@article{toscano_2010,
	author = {Toscano, J. C. and McMurray, B.},
	title = {Cue Integration With Categories: Weighting Acoustic Cues in Speech Using Unsupervised Learning and Distributional Statistics},
	journal = {Cognitive Scienc.},
        year = {2010},
        volume = {34},
        pages = {434–464}
}


@article{kouki_2010,
	author = {Kouki, M. and Hideaki, K. and Reiko, M.},
	title = {Unsupervised Learning of Vowels from Continuous Speech based on Self-organized Phoneme Acquisition Model},
	journal = {Interspeech.},
        year = {2010}
}


@article{kouki_2011,
	author = {Kouki, M. and Hideaki, M. and Hideaki, K. and Reiko, M.},
	title = {The Multi Timescale Phoneme Acquisition Model of the Self-Organizing Based on the Dynamic Features},
	journal = {Interspeech.},
        year = {2011}
}


@article{hawkins_2004,
	author = {Hawkins, J. and Blakeslee, S},
	title = {On Intelligence},
	journal = {Times Books.},
        year = {2004}
}


@inproceedings{Lee:2009:UFL:2984093.2984217,
 author = {Lee, Honglak and Largman, Yan and Pham, Peter and Ng, Andrew Y.},
 title = {Unsupervised Feature Learning for Audio Classification Using Convolutional Deep Belief Networks},
 booktitle = {Proceedings of the 22Nd International Conference on Neural Information Processing Systems},
 series = {NIPS'09},
 year = {2009},
 isbn = {978-1-61567-911-9},
 location = {Vancouver, British Columbia, Canada},
 pages = {1096--1104},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2984093.2984217},
 acmid = {2984217},
 publisher = {Curran Associates Inc.},
 address = {USA},
} 


@article{hawkins_2016,
	author = {Hawkins, J. and Ahmad, S.},
	title = {Why Neurons Have Thousands of Synapses, a Theory of Sequence Memory in Neocortex},
	journal = {Frontiers in Neural Circuits.},
        year = {2016},
        volume = {10},
}


@article{mountcastle_1955,
	author = {Mountcastle, V. B. and Berman, A. L. and  Davies, P. W.},
	title = {Topographic organization and modality representation in first somatic area of cat’s cerebral cortex by method of single unit analysis},
	journal = {Am. J. Physiol.},
        year = {1955},
        volume = {183}
}


@article{mountcastle_modality_1957,
	title = {{MODALITY} {AND} {TOPOGRAPHIC} {PROPERTIES} {OF} {SINGLE} {NEURONS} {OF} {CAT}'{S} {SOMATIC} {SENSORY} {CORTEX}},
	volume = {20},
	issn = {0022-3077, 1522-1598},
	url = {http://www.physiology.org/doi/10.1152/jn.1957.20.4.408},
	doi = {10.1152/jn.1957.20.4.408},
	language = {en},
	number = {4},
	urldate = {2018-07-10},
	journal = {Journal of Neurophysiology},
	author = {Mountcastle, Vernon B.},
	month = jul,
	year = {1957},
	pages = {408--434},
	file = {Mountcastle - 1957 - MODALITY AND TOPOGRAPHIC PROPERTIES OF SINGLE NEUR.pdf:/home/dario/Zotero/storage/EXXRNZXE/Mountcastle - 1957 - MODALITY AND TOPOGRAPHIC PROPERTIES OF SINGLE NEUR.pdf:application/pdf}
}


@article{hubel_1962,
	author = {Hubel, D. and Wiesel, T.},
	title = {Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex},
	journal = {J Physiol.},
        year = {1962},
        volume = {160},
        pages = {106–154}
}


@article{hubel_1968,
	author = {Hubel, D. and Wiesel, T.},
	title = {Receptive fields and functional architecture of monkey striate cortex},
	journal = {J Physiol.},
        year = {1968},
        volume = {195},
        pages = {215–243}
}


@article{rakic_1995,
	author = {Rakic, P.},
	title = {Radial versus tangential migration of neuronal clones in the developing cerebral cortex},
	journal = {Proc Natl Acad Sci USA.},
        year = {1995},
        volume = {92}
}


@article{mountcastle_1978,
	author = {Mountcastle, V. B.},
	title = {An Organizing Principle for Cerebral Function: The Unit Model and the Distributed System},
	journal = {Cambridge, MA.},
        year = {1978}
}


@article{linden_2003,
author = {Linden, Jennifer F. and Schreiner, Christoph E.},
title = {Columnar Transformations in Auditory Cortex? A Comparison to Visual and Somatosensory Cortices},
journal = {Cerebral Cortex},
volume = {13},
number = {1},
pages = {83-89},
year = {2003},
doi = {10.1093/cercor/13.1.83},
URL = {http://dx.doi.org/10.1093/cercor/13.1.83},
eprint = {/oup/backfile/content_public/journal/cercor/13/1/10.1093_cercor_13.1.83/1/1300083.pdf}
}


@article{mountcastle_1997,
author = {B Mountcastle, V},
year = {1997},
month = {04},
pages = {701-722},
title = {The columnar organization of the neocortex},
volume = {120 ( Pt 4)},
booktitle = {Brain}
}


@article{huang_2000,
	author = {Huang, C. L. and Winer, J. A.},
	title = {Auditory thalamocortical projections in the cat: laminar and areal patterns of input},
	journal = {J. Comp. Neurol.},
        year = {2000},
        volume = {427},
        pages = {302-331}
}


@article{winer_1992,
	author = {Winer, J. A.},
	title = {The functional architecture of the medial geniculate body and the primary auditory cortex,  In: The mammalian auditory pathway: neuroanatomy},
	journal = {New York: Springer-Verlag.},
        year = {1992},
        pages = {222-409}
}


@article{rockel_1980,
	author = {Rockel, A. J. and Hiorns, R. W. and Powell, T. P.},
	title = {The basic uniformity in the structure of the neocortex},
	journal = {Brain},
        year = {1980},
        volume = {103},
        pages = {221-244}
}


@article{mitani_1985,
	author = {Mitani, A. and Shimokouchi, M.},
	title = {Neuronal connections in the primary auditor y cortex: an electrophysiological study in the cat},
	journal = {J. Comp. Neurol.},
        year = {1985},
        volume = {235},
        pages = {417-429}
}


@article{mitani_1985A,
	author = {Mitani, A. and Shimokouchi, M. and Itoh, K. and Nomura, S. and Kudo, M. and Mizuno, N.},
	title = {Morphology and laminar organization of electrophysiologically identified neurons in the primary auditory cortex in the cat},
	journal = {J. Comp. Neurol.},
        year = {1985},
        volume = {235},
        pages = {430-447}
}


@article{sur_1988,
	author = {Sur, M. and Garraghty, P. E. and Roe, A. W.},
	title = {Experimentally induced visual projections into auditory thalamus and cortex},
	journal = {Science.},
        year = {1988},
        volume = {242},
        pages = {1437–1441}
}


@article{angelucci_1998,
	author = {Angelucci, A. and Clasca, F. and Sur, M.},
	title = {Brainstem inputs to the ferret medial geniculate nucleus and the effect of early deafferentation on novel retinal projections to the auditory thalamus},
	journal = {J. Comp. Neurol.},
        year = {1998},
        volume = {400},
        pages = {417–439}
}


@article{roe_1992,
	author = {Roe, A. W. and Pallas, S. L. and Kwon, Y. H. and Sur, M.},
	title = {Visual projections routed to the auditory pathway in ferrets: receptive fields of visual neurons in primary auditory cortex},
	journal = {J. Neurosci.},
        year = {1992},
        volume = {12},
        pages = {3651–3664}
}


@article{roe_1990,
	author = {Roe, A. W. and Pallas, S. L. and Hahm, J. O. and Sur, M.},
	title = {A map of visual space induced in primary auditory cortex},
	journal = {Science.},
        year = {1990},
        volume = {250},
        pages = {818–820}
}


@article{sur_2000,
	author = {Sharma, J. and Angelucci, A. and Sur, M.},
	title = {Induction of visual orientation modules in auditory cortex},
	journal = {Nature.},
        year = {2000},
        volume = {404},
        pages = {841–847}
}


@article{mesgarani_2008,
	author = {Mesgarani, N. and David, S. V. and Fritz, J. B. and Shamma, S. A.},
	title = {Phoneme representation and classification in primary auditory cortex},
	journal = {J. Acoust. Soc. Am.},
        year = {2008},
        volume = {123},
        pages = {899–909}
}


@article{mesgarani_2014A,
	author = {Mesgarani, N. and David, S. V. and Fritz, J. B. and Shamma, S. A.},
	title = {Mechanisms of noise robust representation of speech in primary auditory cortex},
	journal = {PNAS.},
        year = {2014},
        volume = {123},
        pages = {899–909}
}


@article{chi_2005,
	author = {Chi, T. and Ru, P. and Shamma, S.A.},
	title = {Multiresolution spectrotemporal analysis of complex sounds.},
	journal = {J. Acoust. Soc. Am.},
        year = {2005},
        volume = {118},
        pages = {887-906}
}


@article{kohonen_2082,
	author = {Kohonen, T.},
	title = {Self-Organized Formation of Topologically Correct Feature Maps},
	journal = {Biological Cybernetics.},
        year = {1982},
        volume = {43},
        pages = {59-69}
}


@book{Kohonen:1989:SAM:69371,
 author = {Kohonen, T.},
 title = {Self-organization and Associative Memory: 3rd Edition},
 year = {1989},
 isbn = {0-387-51387-6},
 publisher = {Springer-Verlag New York, Inc.},
 address = {New York, NY, USA},
} 


@article{barth_2012,
	author = {Barth, A.L. and Poulet, J.F.},
	title = {Experimental evidence for sparse firing in the neocortex.},
	journal = {Trends Neurosci.},
        year = {2012},
        volume = {35},
        pages = {345-55}
}


@article{turrigiano_2012,
	author = {Turrigiano, G.},
	title = {Homeostatic Synaptic Plasticity: Local and Global Mechanisms for Stabilizing Neuronal Function},
	journal = {Cold Spring Harb Perspect Biol.},
        year = {2012},
        volume = {4}
}


@article{ahmad_2016,
	author = {Ahmad, S. and Hawkins, J.},
	title = {How do neurons operate on sparse distributed representations? A mathematical theory of sparsity, neurons and active dendrites.},
	journal = {arXiv:1601.00720 [q–bio.NC]},
        year = {2016}
}


@article{JMLR:v15:srivastava14a,
  author  = {Nitish Srivastava and Geoffrey Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  title   = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  journal = {Journal of Machine Learning Research},
  year    = {2014},
  volume  = {15},
  pages   = {1929-1958},
  url     = {http://jmlr.org/papers/v15/srivastava14a.html}
}


@article{taishih_2005,
	author = {Taishih, Chi and Powen, Ru and Shihab, A., Shamma },
	title = {Multiresulution spectrotemporal analysis of complex sounds},
	journal = {J. Acoust. Soc. Am.},
        year = {2005},
        volume = {118},
        pages = {887-906}
}


@book{Miller_1993,
 author = {Kenneth S. Miller and Bertram Ross},
 title = {An Introduction to the Fractional Calculus and Fractional Differential Equations},
 year = {1993},
 isbn = {0-471-58884-9},
 publisher = {A Wiley-Interscience Publication},
 address = {Printed in the United States of America},
 edition = {First},
 pages = {23}
} 


@article{KRAUSE201436,
title = "Contextual modulation and stimulus selectivity in extrastriate cortex",
journal = "Vision Research",
volume = "104",
pages = "36 - 46",
year = "2014",
note = "The Function of Contextual Modulation",
issn = "0042-6989",
doi = "https://doi.org/10.1016/j.visres.2014.10.006",
url = "http://www.sciencedirect.com/science/article/pii/S0042698914002399",
author = "Matthew R. Krause and Christopher C. Pack",
keywords = "Contextual modulation, Normalization, Surround, Extrastriate cortex, Neurophysiology, Macaque"
}


@article{doi:10.1167/16.13.1,
author = {Snow, Michoel and Coen-Cagli, Ruben and Schwartz, Odelia},
title = {Specificity and timescales of cortical adaptation as inferences about natural movie statistics},
journal = {Journal of Vision},
volume = {16},
number = {13},
pages = {},
year = {2016},
doi = {10.1167/16.13.1},
URL = { + http://dx.doi.org/10.1167/16.13.1},
eprint = {/data/journals/jov/935767/i1534-7362-16-13-1.pdf}
}


@article{guerguiev_towards_2017,
	title = {Towards deep learning with segregated dendrites},
	volume = {6},
	copyright = {© 2017 Guerguiev et al.. This article is distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use and redistribution provided that the original author and source are credited.},
	issn = {2050-084X},
	url = {https://elifesciences.org/articles/22901},
	doi = {10.7554/eLife.22901},
	abstract = {A multi-compartment spiking neural network model demonstrates that biologically feasible deep learning can be achieved if sensory inputs and higher-order feedback are received by different dendritic compartments.},
	language = {en},
	urldate = {2018-06-20},
	journal = {eLife},
	author = {Guerguiev, Jordan and Lillicrap, Timothy P. and Richards, Blake A.},
	month = dec,
	year = {2017},
	pages = {e22901},
	file = {Full Text PDF:/home/dario/Zotero/storage/HM44QJKB/Guerguiev et al. - 2017 - Towards deep learning with segregated dendrites.pdf:application/pdf;Snapshot:/home/dario/Zotero/storage/GLAXT948/22901.html:text/html}
}

@article{hinton_matrix_2018,
	title = {{MATRIX} {CAPSULES} {WITH} {EM} {ROUTING}},
	abstract = {A capsule is a group of neurons whose outputs represent different properties of the same entity. Each layer in a capsule network contains many capsules. We describe a version of capsules in which each capsule has a logistic unit to represent the presence of an entity and a 4x4 matrix which could learn to represent the relationship between that entity and the viewer (the pose). A capsule in one layer votes for the pose matrix of many different capsules in the layer above by multiplying its own pose matrix by trainable viewpoint-invariant transformation matrices that could learn to represent part-whole relationships. Each of these votes is weighted by an assignment coefﬁcient. These coefﬁcients are iteratively updated for each image using the Expectation-Maximization algorithm such that the output of each capsule is routed to a capsule in the layer above that receives a cluster of similar votes. The transformation matrices are trained discriminatively by backpropagating through the unrolled iterations of EM between each pair of adjacent capsule layers. On the smallNORB benchmark, capsules reduce the number of test errors by 45\% compared to the state-of-the-art. Capsules also show far more resistance to white box adversarial attacks than our baseline convolutional neural network.},
	language = {en},
	author = {Hinton, Geoffrey and Sabour, Sara and Frosst, Nicholas},
	year = {2018},
	pages = {15},
	file = {Hinton et al. - 2018 - MATRIX CAPSULES WITH EM ROUTING.pdf:/home/dario/Zotero/storage/H253N4HM/Hinton et al. - 2018 - MATRIX CAPSULES WITH EM ROUTING.pdf:application/pdf}
}

@article{sabour_dynamic_2017,
	title = {Dynamic {Routing} {Between} {Capsules}},
	url = {http://arxiv.org/abs/1710.09829},
	abstract = {A capsule is a group of neurons whose activity vector represents the instantiation parameters of a specific type of entity such as an object or an object part. We use the length of the activity vector to represent the probability that the entity exists and its orientation to represent the instantiation parameters. Active capsules at one level make predictions, via transformation matrices, for the instantiation parameters of higher-level capsules. When multiple predictions agree, a higher level capsule becomes active. We show that a discrimininatively trained, multi-layer capsule system achieves state-of-the-art performance on MNIST and is considerably better than a convolutional net at recognizing highly overlapping digits. To achieve these results we use an iterative routing-by-agreement mechanism: A lower-level capsule prefers to send its output to higher level capsules whose activity vectors have a big scalar product with the prediction coming from the lower-level capsule.},
	urldate = {2018-06-20},
	journal = {arXiv:1710.09829 [cs]},
	author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
	month = oct,
	year = {2017},
	note = {arXiv: 1710.09829},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv\:1710.09829 PDF:/home/dario/Zotero/storage/UNXCYPFT/Sabour et al. - 2017 - Dynamic Routing Between Capsules.pdf:application/pdf;arXiv.org Snapshot:/home/dario/Zotero/storage/WII36P4V/1710.html:text/html}
}

@article{silver_mastering_2016,
	title = {Mastering the game of {Go} with deep neural networks and tree search},
	volume = {529},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature16961},
	doi = {10.1038/nature16961},
	language = {en},
	number = {7587},
	urldate = {2018-06-20},
	journal = {Nature},
	author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
	month = jan,
	year = {2016},
	pages = {484--489},
	file = {Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf:/home/dario/Zotero/storage/EYKMAWX6/Silver et al. - 2016 - Mastering the game of Go with deep neural networks.pdf:application/pdf}
}

@article{silver_mastering_2017,
	title = {Mastering the game of {Go} without human knowledge},
	volume = {550},
	copyright = {2017 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature24270},
	doi = {10.1038/nature24270},
	abstract = {Starting from zero knowledge and without human data, AlphaGo Zero was able to teach itself to play Go and to develop novel strategies that provide new insights into the oldest of games.},
	language = {en},
	number = {7676},
	urldate = {2018-06-20},
	journal = {Nature},
	author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and Driessche, George van den and Graepel, Thore and Hassabis, Demis},
	month = oct,
	year = {2017},
	pages = {354},
	file = {Snapshot:/home/dario/Zotero/storage/L54E5F5L/nature24270.html:text/html}
}

@misc{noauthor_9_nodate,
	title = {(9) {Beyond} regression : new tools for prediction and analysis in the behavioral sciences /},
	shorttitle = {(9) {Beyond} regression},
	url = {https://www.researchgate.net/publication/35657389_Beyond_regression_new_tools_for_prediction_and_analysis_in_the_behavioral_sciences},
	abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
	language = {en},
	urldate = {2018-06-22},
	journal = {ResearchGate},
	file = {Snapshot:/home/dario/Zotero/storage/Y5WI3SEP/35657389_Beyond_regression_new_tools_for_prediction_and_analysis_in_the_behavioral_sciences.html:text/html}
}

@misc{noauthor_parker_nodate,
	title = {Parker {DB} 1985 {Learning} logic {Technical} {Report} {TR} 47 {Cambridge} {MA} {MIT} {Center}},
	url = {https://www.coursehero.com/file/p5227u3/Parker-DB-1985-Learning-logic-Technical-Report-TR-47-Cambridge-MA-MIT-Center/},
	abstract = {Parker DB 1985 Learning logic Technical Report TR 47 Cambridge MA MIT Center from MIS PLS at Hong Kong Shue Yan},
	language = {en},
	urldate = {2018-06-22},
	file = {Snapshot:/home/dario/Zotero/storage/JSWPCZ5T/Parker-DB-1985-Learning-logic-Technical-Report-TR-47-Cambridge-MA-MIT-Center.html:text/html}
}

@article{lecun_procedure_1985,
	title = {Une procedure d'apprentissage pour reseau a seuil asymmetrique ({A} learning scheme for asymmetric threshold networks)},
	url = {https://nyuscholars.nyu.edu/en/publications/une-procedure-dapprentissage-pour-reseau-a-seuil-asymmetrique-a-l},
	language = {English (US)},
	urldate = {2018-06-23},
	journal = {Proceedings of Cognitiva 85, Paris, France},
	author = {Lecun, Yann},
	year = {1985},
	file = {Snapshot:/home/dario/Zotero/storage/CDFYTBBC/une-procedure-dapprentissage-pour-reseau-a-seuil-asymmetrique-a-l.html:text/html}
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	copyright = {1986 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/323533a0},
	doi = {10.1038/323533a0},
	abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal ‘hidden’ units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
	language = {en},
	number = {6088},
	urldate = {2018-06-22},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	pages = {533--536},
	file = {Snapshot:/home/dario/Zotero/storage/BXMVG6XE/323533a0.html:text/html}
}

@inproceedings{hinton_what_2005,
	address = {San Francisco, CA, USA},
	series = {{IJCAI}'05},
	title = {What {Kind} of a {Graphical} {Model} is the {Brain}?},
	url = {http://dl.acm.org/citation.cfm?id=1642293.1642643},
	abstract = {If neurons are treated as latent variables, our visual systems are non-linear, densely-connected graphical models containing billions of variables and thousands of billions of parameters. Current algorithms would have difficulty learning a graphical model of this scale. Starting with an algorithm that has difficulty learning more than a few thousand parameters, I describe a series of progressively better learning algorithms all of which are designed to run on neuron-like hardware. The latest member of this series can learn deep, multi-layer belief nets quite rapidly. It turns a generic network with three hidden layers and 1:7 million connections into a very good generative model of handwritten digits. After learning, the model gives classification performance that is comparable to the best discriminative methods.},
	urldate = {2018-06-22},
	booktitle = {Proceedings of the 19th {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Hinton, Geoffrey E.},
	year = {2005},
	pages = {1765--1775}
}

@article{hinton_fast_2006,
	title = {A fast learning algorithm for deep belief nets},
	volume = {18},
	issn = {0899-7667},
	doi = {10.1162/neco.2006.18.7.1527},
	abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
	language = {eng},
	number = {7},
	journal = {Neural Computation},
	author = {Hinton, Geoffrey E. and Osindero, Simon and Teh, Yee-Whye},
	month = jul,
	year = {2006},
	pmid = {16764513},
	keywords = {Algorithms, Animals, Humans, Learning, Neural Networks (Computer), Neurons},
	pages = {1527--1554}
}

@inproceedings{bengio_greedy_2006,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'06},
	title = {Greedy {Layer}-wise {Training} of {Deep} {Networks}},
	url = {http://dl.acm.org/citation.cfm?id=2976456.2976476},
	abstract = {Complexity theory of circuits strongly suggests that deep architectures can be much more efficient (sometimes exponentially) than shallow architectures, in terms of computational elements required to represent some functions. Deep multi-layer neural networks have many levels of non-linearities allowing them to compactly represent highly non-linear and highly-varying functions. However, until recently it was not clear how to train such deep networks, since gradient-based optimization starting from random initialization appears to often get stuck in poor solutions. Hinton et al. recently introduced a greedy layer-wise unsupervised learning algorithm for Deep Belief Networks (DBN), a generative model with many layers of hidden causal variables. In the context of the above optimization problem, we study this algorithm empirically and explore variants to better understand its success and extend it to cases where the inputs are continuous or where the structure of the input distribution is not revealing enough about the variable to be predicted in a supervised task. Our experiments also confirm the hypothesis that the greedy layer-wise unsupervised training strategy mostly helps the optimization, by initializing weights in a region near a good local minimum, giving rise to internal distributed representations that are high-level abstractions of the input, bringing better generalization.},
	urldate = {2018-06-22},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Bengio, Yoshua and Lamblin, Pascal and Popovici, Dan and Larochelle, Hugo},
	year = {2006},
	pages = {153--160}
}

@inproceedings{ranzato_efficient_2006,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'06},
	title = {Efficient {Learning} of {Sparse} {Representations} with an {Energy}-based {Model}},
	url = {http://dl.acm.org/citation.cfm?id=2976456.2976599},
	abstract = {We describe a novel unsupervised method for learning sparse, overcomplete features. The model uses a linear encoder, and a linear decoder preceded by a sparsifying non-linearity that turns a code vector into a quasi-binary sparse code vector. Given an input, the optimal code minimizes the distance between the output of the decoder and the input patch while being as similar as possible to the encoder output. Learning proceeds in a two-phase EM-like fashion: (1) compute the minimum-energy code vector, (2) adjust the parameters of the encoder and decoder so as to decrease the energy. The model produces "stroke detectors" when trained on handwritten numerals, and Gabor-like filters when trained on natural image patches. Inference and learning are very fast, requiring no preprocessing, and no expensive sampling. Using the proposed unsupervised method to initialize the first layer of a convolutional network, we achieved an error rate slightly lower than the best reported result on the MNIST dataset. Finally, an extension of the method is described to learn topographical filter maps.},
	urldate = {2018-06-22},
	booktitle = {Proceedings of the 19th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {MIT Press},
	author = {Ranzato, Marc'Aurelio and Poultney, Christopher and Chopra, Sumit and LeCun, Yann},
	year = {2006},
	pages = {1137--1144}
}

@article{hinton_reducing_2006,
	title = {Reducing the {Dimensionality} of {Data} with {Neural} {Networks}},
	volume = {313},
	copyright = {American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/313/5786/504},
	doi = {10.1126/science.1127647},
	abstract = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “autoencoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.
Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks.
Neural networks can be used to reduce accurately high-dimensional data to lower dimensional representations for pattern recognition tasks.},
	language = {en},
	number = {5786},
	urldate = {2018-06-22},
	journal = {Science},
	author = {Hinton, G. E. and Salakhutdinov, R. R.},
	month = jul,
	year = {2006},
	pmid = {16873662},
	pages = {504--507},
	file = {Snapshot:/home/dario/Zotero/storage/ST9K4VA6/504.html:text/html}
}

@article{mohamed_acoustic_2012,
	title = {Acoustic {Modeling} {Using} {Deep} {Belief} {Networks}},
	volume = {20},
	issn = {1558-7916},
	url = {https://doi.org/10.1109/TASL.2011.2109382},
	doi = {10.1109/TASL.2011.2109382},
	abstract = {Gaussian mixture models are currently the dominant technique for modeling the emission distribution of hidden Markov models for speech recognition. We show that better phone recognition on the TIMIT dataset can be achieved by replacing Gaussian mixture models by deep neural networks that contain many layers of features and a very large number of parameters. These networks are first pre-trained as a multi-layer generative model of a window of spectral feature vectors without making use of any discriminative information. Once the generative pre-training has designed the features, we perform discriminative fine-tuning using backpropagation to adjust the features slightly to make them better at predicting a probability distribution over the states of monophone hidden Markov models.},
	number = {1},
	urldate = {2018-06-22},
	journal = {Trans. Audio, Speech and Lang. Proc.},
	author = {Mohamed, A. and Dahl, G. E. and Hinton, G.},
	month = jan,
	year = {2012},
	pages = {14--22}
}

@article{dahl_context-dependent_2012,
	title = {Context-{Dependent} {Pre}-{Trained} {Deep} {Neural} {Networks} for {Large}-{Vocabulary} {Speech} {Recognition}},
	volume = {20},
	issn = {1558-7916},
	doi = {10.1109/TASL.2011.2134090},
	abstract = {We propose a novel context-dependent (CD) model for large-vocabulary speech recognition (LVSR) that leverages recent advances in using deep belief networks for phone recognition. We describe a pre-trained deep neural network hidden Markov model (DNN-HMM) hybrid architecture that trains the DNN to produce a distribution over senones (tied triphone states) as its output. The deep belief network pre-training algorithm is a robust and often helpful way to initialize deep neural networks generatively that can aid in optimization and reduce generalization error. We illustrate the key components of our model, describe the procedure for applying CD-DNN-HMMs to LVSR, and analyze the effects of various modeling choices on performance. Experiments on a challenging business search dataset demonstrate that CD-DNN-HMMs can significantly outperform the conventional context-dependent Gaussian mixture model (GMM)-HMMs, with an absolute sentence accuracy improvement of 5.8\% and 9.2\% (or relative error reduction of 16.0\% and 23.2\%) over the CD-GMM-HMMs trained using the minimum phone error rate (MPE) and maximum-likelihood (ML) criteria, respectively.},
	number = {1},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Dahl, G. E. and Yu, D. and Deng, L. and Acero, A.},
	month = jan,
	year = {2012},
	keywords = {Acoustics, Artificial neural network–hidden Markov model (ANN-HMM), Artificial neural networks, Context modeling, context-dependent Gaussian mixture model, context-dependent phone, context-dependent pretrained deep neural network, deep belief network, deep belief network pretraining algorithm, deep neural network hidden Markov model (DNN-HMM), DNN-HMM, Gaussian processes, GMM, hidden Markov model, hidden Markov models, Hidden Markov models, large-vocabulary speech recognition, large-vocabulary speech recognition (LVSR), LVSR, Mathematical model, maximum likelihood estimation, maximum-likelihood criteria, minimum phone error rate, MPE, neural nets, relative error reduction, speech recognition, Speech recognition, Training},
	pages = {30--42},
	file = {IEEE Xplore Abstract Record:/home/dario/Zotero/storage/V7HIDKX7/5740583.html:text/html}
}

@article{ciresan_multi-column_2012,
	series = {Selected {Papers} from {IJCNN} 2011},
	title = {Multi-column deep neural network for traffic sign classification},
	volume = {32},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608012000524},
	doi = {10.1016/j.neunet.2012.02.023},
	abstract = {We describe the approach that won the final phase of the German traffic sign recognition benchmark. Our method is the only one that achieved a better-than-human recognition rate of 99.46\%. We use a fast, fully parameterizable GPU implementation of a Deep Neural Network (DNN) that does not require careful design of pre-wired feature extractors, which are rather learned in a supervised way. Combining various DNNs trained on differently preprocessed data into a Multi-Column DNN (MCDNN) further boosts recognition performance, making the system insensitive also to variations in contrast and illumination.},
	urldate = {2018-06-23},
	journal = {Neural Networks},
	author = {Cireşan, Dan and Meier, Ueli and Masci, Jonathan and Schmidhuber, Jürgen},
	month = aug,
	year = {2012},
	keywords = {Deep neural networks, Image classification, Image preprocessing, Traffic signs},
	pages = {333--338},
	file = {ScienceDirect Snapshot:/home/dario/Zotero/storage/LD3J43WM/S0893608012000524.html:text/html}
}

@article{ning_toward_2005,
	title = {Toward automatic phenotyping of developing embryos from videos},
	volume = {14},
	issn = {1057-7149},
	doi = {10.1109/TIP.2005.852470},
	abstract = {We describe a trainable system for analyzing videos of developing C. elegans embryos. The system automatically detects, segments, and locates cells and nuclei in microscopic images. The system was designed as the central component of a fully automated phenotyping system. The system contains three modules 1) a convolutional network trained to classify each pixel into five categories: cell wall, cytoplasm, nucleus membrane, nucleus, outside medium; 2) an energy-based model, which cleans up the output of the convolutional network by learning local consistency constraints that must be satisfied by label images; 3) a set of elastic models of the embryo at various stages of development that are matched to the label images.},
	number = {9},
	journal = {IEEE Transactions on Image Processing},
	author = {Ning, Feng and Delhomme, D. and LeCun, Y. and Piano, F. and Bottou, L. and Barbano, P. E.},
	month = sep,
	year = {2005},
	keywords = {Algorithms, Animals, Artificial Intelligence, automatic phenotyping, Bioinformatics, Biological system modeling, biological techniques, Caenorhabditis elegans, cellular biophysics, convolutional network, Convolutional network, cytoplasm, Embryo, Embryo, Nonmammalian, embryos, energy-based model, Fetal Development, genetics, Genomics, Image Enhancement, Image Interpretation, Computer-Assisted, image segmentation, Image segmentation, microscopic images, Microscopy, Microscopy, Phase-Contrast, Microscopy, Video, Motion pictures, nonlinear filter, nucleus membrane, optical microscopy, Pattern Recognition, Automated, Performance analysis, Phenotype, Reproducibility of Results, Sensitivity and Specificity, Videos},
	pages = {1360--1371},
	file = {IEEE Xplore Abstract Record:/home/dario/Zotero/storage/ZE8APMXJ/1495508.html:text/html}
}

@article{turaga_convolutional_2010,
	title = {Convolutional networks can learn to generate affinity graphs for image segmentation},
	volume = {22},
	issn = {1530-888X},
	doi = {10.1162/neco.2009.10-08-881},
	abstract = {Many image segmentation algorithms first generate an affinity graph and then partition it. We present a machine learning approach to computing an affinity graph using a convolutional network (CN) trained using ground truth provided by human experts. The CN affinity graph can be paired with any standard partitioning algorithm and improves segmentation accuracy significantly compared to standard hand-designed affinity functions. We apply our algorithm to the challenging 3D segmentation problem of reconstructing neuronal processes from volumetric electron microscopy (EM) and show that we are able to learn a good affinity graph directly from the raw EM images. Further, we show that our affinity graph improves the segmentation accuracy of both simple and sophisticated graph partitioning algorithms. In contrast to previous work, we do not rely on prior knowledge in the form of hand-designed image features or image preprocessing. Thus, we expect our algorithm to generalize effectively to arbitrary image types.},
	language = {eng},
	number = {2},
	journal = {Neural Computation},
	author = {Turaga, Srinivas C. and Murray, Joseph F. and Jain, Viren and Roth, Fabian and Helmstaedter, Moritz and Briggman, Kevin and Denk, Winfried and Seung, H. Sebastian},
	month = feb,
	year = {2010},
	pmid = {19922289},
	keywords = {Algorithms, Artificial Intelligence, Image Processing, Computer-Assisted, Mathematical Computing, Mathematical Concepts, Mathematics, Microscopy, Electron, Neural Networks (Computer), Pattern Recognition, Automated},
	pages = {511--538}
}

@article{sermanet_pedestrian_2012,
	title = {Pedestrian {Detection} with {Unsupervised} {Multi}-{Stage} {Feature} {Learning}},
	url = {https://arxiv.org/abs/1212.0142},
	language = {en},
	urldate = {2018-06-22},
	author = {Sermanet, Pierre and Kavukcuoglu, Koray and Chintala, Soumith and LeCun, Yann},
	month = dec,
	year = {2012},
	file = {Full Text PDF:/home/dario/Zotero/storage/AYEHZ8GP/Sermanet et al. - 2012 - Pedestrian Detection with Unsupervised Multi-Stage.pdf:application/pdf;Snapshot:/home/dario/Zotero/storage/WCGFKRGX/1212.html:text/html}
}

@article{vaillant_original_1994,
	title = {Original approach for the localisation of objects in images},
	volume = {141},
	issn = {1350-245X},
	doi = {10.1049/ip-vis:19941301},
	abstract = {An original approach is presented for the localisation of objects in an image which approach is neuronal and has two steps. In the first step, a rough localisation is performed by presenting each pixel with its neighbourhood to a neural net which is able to indicate whether this pixel and its neighbourhood are the image of the search object. This first filter does not discriminate for position. From its result, areas which might contain an image of the object can be selected. In the second step, these areas are presented to another neural net which can determine the exact position of the object in each area. This algorithm is applied to the problem of localising faces in images},
	number = {4},
	journal = {IEE Proceedings - Vision, Image and Signal Processing},
	author = {Vaillant, R. and Monrocq, C. and Cun, Y. Le},
	month = aug,
	year = {1994},
	keywords = {face detection, image analysis, image object localisation, image reconstruction, image segmentation, learning (artificial intelligence), neural net, neural nets},
	pages = {245--250},
	file = {IEEE Xplore Abstract Record:/home/dario/Zotero/storage/4HJ3L29P/318027.html:text/html}
}

@inproceedings{nowlan_convolutional_1995,
	title = {A {Convolutional} {Neural} {Network} {Hand} {Tracker}},
	abstract = {We describe a system that can track a hand in a sequence of video frames and recognize hand gestures in a user-independent manner. The system locates the hand in each video frame and determines if the hand is open or closed. The tracking system is able to track the hand to within {\textbackslash}Sigma10 pixels of its correct location in 99:7\% of the frames from a test set containing video sequences from 18 different individuals captured in 18 different room environments. The gesture recognition network correctly determines if the hand being tracked is open or closed in 99:1\% of the frames in this test set. The system has been designed to operate in real time with existing hardware. 1 Introduction  We describe an image processing system that uses convolutional neural networks to locate the position of a (moving) hand in a video frame, and to track the position of this hand across a sequence of video frames. In addition, for each frame, the system determines if the hand is currently open or closed. The...},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 7},
	publisher = {Morgan Kaufmann},
	author = {Nowlan, Steven J. and Platt, John C.},
	year = {1995},
	pages = {901--908},
	file = {Citeseer - Full Text PDF:/home/dario/Zotero/storage/RT3PGWN8/Nowlan and Platt - 1995 - A Convolutional Neural Network Hand Tracker.pdf:application/pdf;Citeseer - Snapshot:/home/dario/Zotero/storage/LE7BCW5G/summary.html:text/html}
}

@article{garcia_convolutional_2004,
	title = {Convolutional face finder: a neural architecture for fast and robust face detection},
	volume = {26},
	issn = {0162-8828},
	shorttitle = {Convolutional face finder},
	doi = {10.1109/TPAMI.2004.97},
	abstract = {In this paper, we present a novel face detection approach based on a convolutional neural architecture, designed to robustly detect highly variable face patterns, rotated up to ±20 degrees in image plane and turned up to ±60 degrees, in complex real world images. The proposed system automatically synthesizes simple problem-specific feature extractors from a training set of face and nonface patterns, without making any assumptions or using any hand-made design concerning the features to extract or the areas of the face pattern to analyze. The face detection procedure acts like a pipeline of simple convolution and subsampling modules that treat the raw input image as a whole. We therefore show that an efficient face detection system does not require any costly local preprocessing before classification of image areas. The proposed scheme provides very high detection rate with a particularly low level of false positives, demonstrated on difficult test sets, without requiring the use of multiple networks for handling difficult cases. We present extensive experimental results illustrating the efficiency of the proposed approach on difficult test sets and including an in-depth sensitivity analysis with respect to the degrees of variability of the face patterns.},
	number = {11},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Garcia, C. and Delakis, M.},
	month = nov,
	year = {2004},
	keywords = {Algorithms, Artificial Intelligence, Automatic Data Processing, Cluster Analysis, complex real world images, Computer Graphics, Computer Simulation, Convolution, convolution modules, Convolutional codes, convolutional face finder, convolutional networks., convolutional neural architecture, Face detection, face pattern detection, face patterns, face recognition, feature extraction, Feature extraction, hand made design, Handwriting, Humans, image classification, Image Enhancement, Image Interpretation, Computer-Assisted, image sampling, Index Terms- Face detection, Information Storage and Retrieval, learning (artificial intelligence), machine learning, neural net architecture, neural networks, Neural networks, nonface patterns, Numerical Analysis, Computer-Assisted, Pattern analysis, Pattern Recognition, Automated, Pipelines, Reading, Reproducibility of Results, Robustness, sensitivity analysis, Sensitivity and Specificity, Signal Processing, Computer-Assisted, specific feature extractors, subsampling modules, Subtraction Technique, Testing, training set, User-Computer Interface},
	pages = {1408--1423},
	file = {IEEE Xplore Abstract Record:/home/dario/Zotero/storage/2VB87UCC/1335446.html:text/html}
}

@article{osadchy_synergistic_2007,
	title = {Synergistic {Face} {Detection} and {Pose} {Estimation} with {Energy}-{Based} {Models}},
	volume = {8},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=1248659.1248700},
	abstract = {We describe a novel method for simultaneously detecting faces and estimating their pose in real time. The method employs a convolutional network to map images of faces to points on a low-dimensional manifold parametrized by pose, and images of non-faces to points far away from that manifold. Given an image, detecting a face and estimating its pose is viewed as minimizing an energy function with respect to the face/non-face binary variable and the continuous pose parameters. The system is trained to minimize a loss function that drives correct combinations of labels and pose to be associated with lower energy values than incorrect ones. The system is designed to handle very large range of poses without retraining. The performance of the system was tested on three standard data sets---for frontal views, rotated faces, and profiles---is comparable to previous systems that are designed to handle a single one of these data sets. We show that a system trained simuiltaneously for detection and pose estimation is more accurate on both tasks than similar systems trained for each task separately.},
	urldate = {2018-06-23},
	journal = {J. Mach. Learn. Res.},
	author = {Osadchy, Margarita and Cun, Yann Le and Miller, Matthew L.},
	month = may,
	year = {2007},
	pages = {1197--1215},
	file = {ACM Full Text PDF:/home/dario/Zotero/storage/6AIQKGV4/Osadchy et al. - 2007 - Synergistic Face Detection and Pose Estimation wit.pdf:application/pdf}
}

@article{tompson_efficient_2014,
	title = {Efficient {Object} {Localization} {Using} {Convolutional} {Networks}},
	url = {http://arxiv.org/abs/1411.4280},
	abstract = {Recent state-of-the-art performance on human-body pose estimation has been achieved with Deep Convolutional Networks (ConvNets). Traditional ConvNet architectures include pooling and sub-sampling layers which reduce computational requirements, introduce invariance and prevent over-training. These benefits of pooling come at the cost of reduced localization accuracy. We introduce a novel architecture which includes an efficient `position refinement' model that is trained to estimate the joint offset location within a small region of the image. This refinement model is jointly trained in cascade with a state-of-the-art ConvNet model to achieve improved accuracy in human joint location estimation. We show that the variance of our detector approaches the variance of human annotations on the FLIC dataset and outperforms all existing approaches on the MPII-human-pose dataset.},
	urldate = {2018-06-23},
	journal = {arXiv:1411.4280 [cs]},
	author = {Tompson, Jonathan and Goroshin, Ross and Jain, Arjun and LeCun, Yann and Bregler, Christopher},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.4280},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 8 pages with 1 page of citations},
	file = {arXiv\:1411.4280 PDF:/home/dario/Zotero/storage/Z4TCEPII/Tompson et al. - 2014 - Efficient Object Localization Using Convolutional .pdf:application/pdf;arXiv.org Snapshot:/home/dario/Zotero/storage/9FPA9QFZ/1411.html:text/html}
}

@inproceedings{taigman_deepface:_2014,
	title = {{DeepFace}: {Closing} the {Gap} to {Human}-{Level} {Performance} in {Face} {Verification}},
	shorttitle = {{DeepFace}},
	doi = {10.1109/CVPR.2014.220},
	abstract = {In modern face recognition, the conventional pipeline consists of four stages: detect ={\textgreater} align ={\textgreater} represent ={\textgreater} classify. We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight sharing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an identity labeled dataset of four million facial images belonging to more than 4, 000 identities. The learned representations coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classifier. Our method reaches an accuracy of 97.35\% on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 27\%, closely approaching human-level performance.},
	booktitle = {2014 {IEEE} {Conference} on {Computer} {Vision} and {Pattern} {Recognition}},
	author = {Taigman, Y. and Yang, M. and Ranzato, M. and Wolf, L.},
	month = jun,
	year = {2014},
	keywords = {3D face modeling, Agriculture, alignment step, deep neural network, DeepFace, Face, face recognition, Face recognition, face representation, face verification, human-level performance, image representation, labeled faces in the wild, LFW dataset, neural nets, piecewise affine transformation, representation step, Shape, Solid modeling, Three-dimensional displays, Training},
	pages = {1701--1708},
	file = {IEEE Xplore Abstract Record:/home/dario/Zotero/storage/IFMR5KUP/6909616.html:text/html}
}

@article{hadsell_learning_nodate,
	title = {Learning long-range vision for autonomous off-road driving},
	volume = {26},
	copyright = {Copyright © 2009 Wiley Periodicals, Inc.},
	issn = {1556-4967},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/rob.20276},
	doi = {10.1002/rob.20276},
	abstract = {Most vision-based approaches to mobile robotics suffer from the limitations imposed by stereo obstacle detection, which is short range and prone to failure. We present a self-supervised learning process for long-range vision that is able to accurately classify complex terrain at distances up to the horizon, thus allowing superior strategic planning. The success of the learning process is due to the self-supervised training data that are generated on every frame: robust, visually consistent labels from a stereo module; normalized wide-context input windows; and a discriminative and concise feature representation. A deep hierarchical network is trained to extract informative and meaningful features from an input image, and the features are used to train a real-time classifier to predict traversability. The trained classifier sees obstacles and paths from 5 to more than 100 m, far beyond the maximum stereo range of 12 m, and adapts very quickly to new environments. The process was developed and tested on the LAGR (Learning Applied to Ground Robots) mobile robot. Results from a ground truth data set, as well as field test results, are given. © 2009 Wiley Periodicals, Inc.},
	language = {en},
	number = {2},
	urldate = {2018-06-23},
	journal = {Journal of Field Robotics},
	author = {Hadsell, Raia and Sermanet, Pierre and Ben, Jan and Erkan, Ayse and Scoffier, Marco and Kavukcuoglu, Koray and Muller, Urs and LeCun, Yann},
	pages = {120--144},
	file = {Snapshot:/home/dario/Zotero/storage/DLK5NEN2/rob.html:text/html}
}

@article{farabet_scene_2012,
	title = {Scene {Parsing} with {Multiscale} {Feature} {Learning}, {Purity} {Trees}, and {Optimal} {Covers}},
	url = {http://arxiv.org/abs/1202.2160},
	abstract = {Scene parsing, or semantic segmentation, consists in labeling each pixel in an image with the category of the object it belongs to. It is a challenging task that involves the simultaneous detection, segmentation and recognition of all the objects in the image. The scene parsing method proposed here starts by computing a tree of segments from a graph of pixel dissimilarities. Simultaneously, a set of dense feature vectors is computed which encodes regions of multiple sizes centered on each pixel. The feature extractor is a multiscale convolutional network trained from raw pixels. The feature vectors associated with the segments covered by each node in the tree are aggregated and fed to a classifier which produces an estimate of the distribution of object categories contained in the segment. A subset of tree nodes that cover the image are then selected so as to maximize the average "purity" of the class distributions, hence maximizing the overall likelihood that each segment will contain a single object. The convolutional network feature extractor is trained end-to-end from raw pixels, alleviating the need for engineered features. After training, the system is parameter free. The system yields record accuracies on the Stanford Background Dataset (8 classes), the Sift Flow Dataset (33 classes) and the Barcelona Dataset (170 classes) while being an order of magnitude faster than competing approaches, producing a 320 {\textbackslash}times 240 image labeling in less than 1 second.},
	urldate = {2018-06-23},
	journal = {arXiv:1202.2160 [cs]},
	author = {Farabet, Clément and Couprie, Camille and Najman, Laurent and LeCun, Yann},
	month = feb,
	year = {2012},
	note = {arXiv: 1202.2160},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	annote = {Comment: 9 pages, 4 figures - Published in 29th International Conference on Machine Learning (ICML 2012), Jun 2012, Edinburgh, United Kingdom},
	file = {arXiv\:1202.2160 PDF:/home/dario/Zotero/storage/3RRJN6UT/Farabet et al. - 2012 - Scene Parsing with Multiscale Feature Learning, Pu.pdf:application/pdf;arXiv.org Snapshot:/home/dario/Zotero/storage/362BJN28/1202.html:text/html}
}

@article{collobert_natural_2011,
	title = {Natural {Language} {Processing} (almost) from {Scratch}},
	url = {http://arxiv.org/abs/1103.0398},
	abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements.},
	urldate = {2018-06-23},
	journal = {arXiv:1103.0398 [cs]},
	author = {Collobert, Ronan and Weston, Jason and Bottou, Leon and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
	month = mar,
	year = {2011},
	note = {arXiv: 1103.0398},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning},
	file = {arXiv\:1103.0398 PDF:/home/dario/Zotero/storage/XRGDMJ87/Collobert et al. - 2011 - Natural Language Processing (almost) from Scratch.pdf:application/pdf;arXiv.org Snapshot:/home/dario/Zotero/storage/72X2JT3G/1103.html:text/html}
}

@misc{noauthor_deep_nodate,
	title = {Deep convolutional neural networks for {LVCSR} - {Semantic} {Scholar}},
	url = {/paper/Deep-convolutional-neural-networks-for-LVCSR-Sainath-Mohamed/24e555913192d8722f4a0240445bf73db71bd884},
	abstract = {Convolutional Neural Networks (CNNs) are an alternative type of neural network that can be used to reduce spectral variations and model spectral correlations which exist in signals. Since speech signals exhibit both of these properties, CNNs are a more effective model for speech compared to Deep Neural Networks (DNNs). In this paper, we explore applying CNNs to large vocabulary speech tasks. First, we determine the appropriate architecture to make CNNs effective compared to DNNs for LVCSR tasks. Specifically, we focus on how many convolutional layers are needed, what is the optimal number of hidden units, what is the best pooling strategy, and the best input feature type for CNNs. We then explore the behavior of neural network features extracted from CNNs on a variety of LVCSR tasks, comparing CNNs to DNNs and GMMs. We find that CNNs offer between a 13-30\% relative improvement over GMMs, and a 4-12\% relative improvement over DNNs, on a 400-hr Broadcast News and 300-hr Switchboard task.},
	urldate = {2018-06-23},
	file = {Snapshot:/home/dario/Zotero/storage/4TGJZ6IZ/24e555913192d8722f4a0240445bf73db71bd884.html:text/html}
}

@inproceedings{krizhevsky_imagenet_2012,
	address = {USA},
	series = {{NIPS}'12},
	title = {{ImageNet} {Classification} with {Deep} {Convolutional} {Neural} {Networks}},
	url = {http://dl.acm.org/citation.cfm?id=2999134.2999257},
	abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overriding in the fully-connected layers we employed a recently-developed regularization method called "dropout" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
	urldate = {2018-06-23},
	booktitle = {Proceedings of the 25th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 1},
	publisher = {Curran Associates Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
	year = {2012},
	pages = {1097--1105}
}

@article{bengio_neural_2003,
	title = {A {Neural} {Probabilistic} {Language} {Model}},
	volume = {3},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v3/bengio03a.html},
	number = {Feb},
	urldate = {2018-06-23},
	journal = {Journal of Machine Learning Research},
	author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Jauvin, Christian},
	year = {2003},
	pages = {1137--1155},
	file = {Full Text PDF:/home/dario/Zotero/storage/PDS4Y4A3/Bengio et al. - 2003 - A Neural Probabilistic Language Model.pdf:application/pdf;Snapshot:/home/dario/Zotero/storage/XJZ3LTPP/bengio03a.html:text/html}
}

@book{noauthor_metaphors_nodate,
	title = {Metaphors {We} {Live} {By}},
	url = {http://www.press.uchicago.edu/ucp/books/book/chicago/M/bo3637992.html},
	abstract = {The now-classic Metaphors We Live By changed our understanding of metaphor and its role in language and the mind. Metaphor, the authors explain, is a fundamental mechanism of mind, one that allows us to use what we know about our physical and social experience to provide understanding of countless other subjects. Because such metaphors structure our most basic understandings of our experience, they are "metaphors we live by"—metaphors that can shape our perceptions and actions without our ever noticing them.In this updated edition of Lakoff and Johnson’s influential book, the authors supply an afterword surveying how their theory of metaphor has developed within the cognitive sciences to become central to the contemporary understanding of how we think and how we express our thoughts in language.},
	urldate = {2018-06-23},
	file = {Snapshot:/home/dario/Zotero/storage/LNF9YJGH/bo3637992.html:text/html}
}

@article{rogers_precis_2008,
	title = {Précis of {Semantic} {Cognition}: {A} {Parallel} {Distributed} {Processing} {Approach}},
	volume = {31},
	issn = {1469-1825, 0140-525X},
	shorttitle = {Précis of {Semantic} {Cognition}},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/precis-of-semantic-cognition-a-parallel-distributed-processing-approach/F3157F4E1DFF90DA60B0542E80586468},
	doi = {10.1017/S0140525X0800589X},
	abstract = {In this précis of our recent book, Semantic Cognition: A Parallel Distributed Processing Approach (Rogers \& McClelland 2004), we present a parallel distributed processing theory of the acquisition, representation, and use of human semantic knowledge. The theory proposes that semantic abilities arise from the flow of activation among simple, neuron-like processing units, as governed by the strengths of interconnecting weights; and that acquisition of new semantic information involves the gradual adjustment of weights in the system in response to experience. These simple ideas explain a wide range of empirical phenomena from studies of categorization, lexical acquisition, and disordered semantic cognition. In this précis we focus on phenomena central to the reaction against similarity-based theories that arose in the 1980s and that subsequently motivated the “theory-theory” approach to semantic knowledge. Specifically, we consider (1) how concepts differentiate in early development, (2) why some groupings of items seem to form “good” or coherent categories while others do not, (3) why different properties seem central or important to different concepts, (4) why children and adults sometimes attest to beliefs that seem to contradict their direct experience, (5) how concepts reorganize between the ages of 4 and 10, and (6) the relationship between causal knowledge and semantic knowledge. The explanations our theory offers for these phenomena are illustrated with reference to a simple feed-forward connectionist model. The relationships between this simple model, the broader theory, and more general issues in cognitive science are discussed.},
	language = {en},
	number = {6},
	urldate = {2018-06-23},
	journal = {Behavioral and Brain Sciences},
	author = {Rogers, Timothy T. and McClelland, James L.},
	month = dec,
	year = {2008},
	keywords = {categorization, causal knowledge, concepts, connectionism, development, innateness, learning, memory, semantics, theory-theory},
	pages = {689--714},
	file = {Snapshot:/home/dario/Zotero/storage/KX89ETRU/F3157F4E1DFF90DA60B0542E80586468.html:text/html}
}

@article{graves_speech_2013,
	title = {Speech {Recognition} with {Deep} {Recurrent} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1303.5778},
	abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates {\textbackslash}emph\{deep recurrent neural networks\}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7\% on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
	urldate = {2018-06-23},
	journal = {arXiv:1303.5778 [cs]},
	author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
	month = mar,
	year = {2013},
	note = {arXiv: 1303.5778},
	keywords = {Computer Science - Computation and Language, Computer Science - Neural and Evolutionary Computing},
	annote = {Comment: To appear in ICASSP 2013},
	file = {arXiv\:1303.5778 PDF:/home/dario/Zotero/storage/9T5T3F54/Graves et al. - 2013 - Speech Recognition with Deep Recurrent Neural Netw.pdf:application/pdf;arXiv.org Snapshot:/home/dario/Zotero/storage/VBWQI76B/1303.html:text/html}
}

@incollection{sutskever_sequence_2014,
	title = {Sequence to {Sequence} {Learning} with {Neural} {Networks}},
	url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf},
	urldate = {2018-06-23},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 27},
	publisher = {Curran Associates, Inc.},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
	editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	pages = {3104--3112},
	file = {NIPS Snapshort:/home/dario/Zotero/storage/DZ4HFLXS/5346-sequence-to-sequence-learning-with-neural-networks.html:text/html}
}
@article{cho_learning_2014,
	title = {Learning {Phrase} {Representations} using {RNN} {Encoder}-{Decoder} for {Statistical} {Machine} {Translation}},
	url = {http://arxiv.org/abs/1406.1078},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder-Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixed-length vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder-Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	urldate = {2018-06-23},
	journal = {arXiv:1406.1078 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.1078},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: EMNLP 2014},
	file = {arXiv\:1406.1078 PDF:/home/dario/Zotero/storage/ZQPRIKNL/Cho et al. - 2014 - Learning Phrase Representations using RNN Encoder-.pdf:application/pdf;arXiv.org Snapshot:/home/dario/Zotero/storage/C78D8W33/1406.html:text/html}
}

@article{bahdanau_neural_2014,
	title = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
	url = {http://arxiv.org/abs/1409.0473},
	abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
	urldate = {2018-06-23},
	journal = {arXiv:1409.0473 [cs, stat]},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.0473},
	keywords = {Computer Science - Computation and Language, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Accepted at ICLR 2015 as oral presentation},
	file = {arXiv\:1409.0473 PDF:/home/dario/Zotero/storage/KNGWCEWV/Bahdanau et al. - 2014 - Neural Machine Translation by Jointly Learning to .pdf:application/pdf;arXiv.org Snapshot:/home/dario/Zotero/storage/7QIUMHMC/1409.html:text/html}
}

@article{graves_neural_2014,
	title = {Neural {Turing} {Machines}},
	url = {http://arxiv.org/abs/1410.5401},
	abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
	urldate = {2018-06-23},
	journal = {arXiv:1410.5401 [cs]},
	author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
	month = oct,
	year = {2014},
	note = {arXiv: 1410.5401},
	keywords = {Computer Science - Neural and Evolutionary Computing},
	file = {arXiv\:1410.5401 PDF:/home/dario/Zotero/storage/3BSWNQSH/Graves et al. - 2014 - Neural Turing Machines.pdf:application/pdf;arXiv.org Snapshot:/home/dario/Zotero/storage/HK897WZA/1410.html:text/html}
}

@article{weston_memory_2014,
	title = {Memory {Networks}},
	url = {http://arxiv.org/abs/1410.3916},
	abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
	urldate = {2018-06-23},
	journal = {arXiv:1410.3916 [cs, stat]},
	author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
	month = oct,
	year = {2014},
	note = {arXiv: 1410.3916},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv\:1410.3916 PDF:/home/dario/Zotero/storage/T6BH9U9W/Weston et al. - 2014 - Memory Networks.pdf:application/pdf;arXiv.org Snapshot:/home/dario/Zotero/storage/UPK5II45/1410.html:text/html}
}

@article{weston_towards_2015,
	title = {Towards {AI}-{Complete} {Question} {Answering}: {A} {Set} of {Prerequisite} {Toy} {Tasks}},
	shorttitle = {Towards {AI}-{Complete} {Question} {Answering}},
	url = {http://arxiv.org/abs/1502.05698},
	abstract = {One long-term goal of machine learning research is to produce methods that are applicable to reasoning and natural language, in particular building an intelligent dialogue agent. To measure progress towards that goal, we argue for the usefulness of a set of proxy tasks that evaluate reading comprehension via question answering. Our tasks measure understanding in several ways: whether a system is able to answer questions via chaining facts, simple induction, deduction and many more. The tasks are designed to be prerequisites for any system that aims to be capable of conversing with a human. We believe many existing learning systems can currently not solve them, and hence our aim is to classify these tasks into skill sets, so that researchers can identify (and then rectify) the failings of their systems. We also extend and improve the recently introduced Memory Networks model, and show it is able to solve some, but not all, of the tasks.},
	urldate = {2018-06-23},
	journal = {arXiv:1502.05698 [cs, stat]},
	author = {Weston, Jason and Bordes, Antoine and Chopra, Sumit and Rush, Alexander M. and van Merriënboer, Bart and Joulin, Armand and Mikolov, Tomas},
	month = feb,
	year = {2015},
	note = {arXiv: 1502.05698},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Statistics - Machine Learning},
	file = {arXiv\:1502.05698 PDF:/home/dario/Zotero/storage/TXGRV7IL/Weston et al. - 2015 - Towards AI-Complete Question Answering A Set of P.pdf:application/pdf;arXiv.org Snapshot:/home/dario/Zotero/storage/ZJF4LHUY/1502.html:text/html}
}

@article{felleman_distributed_1991,
	title = {Distributed hierarchical processing in the primate cerebral cortex},
	volume = {1},
	issn = {1047-3211},
	abstract = {In recent years, many new cortical areas have been identified in the macaque monkey. The number of identified connections between areas has increased even more dramatically. We report here on (1) a summary of the layout of cortical areas associated with vision and with other modalities, (2) a computerized database for storing and representing large amounts of information on connectivity patterns, and (3) the application of these data to the analysis of hierarchical organization of the cerebral cortex. Our analysis concentrates on the visual system, which includes 25 neocortical areas that are predominantly or exclusively visual in function, plus an additional 7 areas that we regard as visual-association areas on the basis of their extensive visual inputs. A total of 305 connections among these 32 visual and visual-association areas have been reported. This represents 31\% of the possible number of pathways if each area were connected with all others. The actual degree of connectivity is likely to be closer to 40\%. The great majority of pathways involve reciprocal connections between areas. There are also extensive connections with cortical areas outside the visual system proper, including the somatosensory cortex, as well as neocortical, transitional, and archicortical regions in the temporal and frontal lobes. In the somatosensory/motor system, there are 62 identified pathways linking 13 cortical areas, suggesting an overall connectivity of about 40\%. Based on the laminar patterns of connections between areas, we propose a hierarchy of visual areas and of somatosensory/motor areas that is more comprehensive than those suggested in other recent studies. The current version of the visual hierarchy includes 10 levels of cortical processing. Altogether, it contains 14 levels if one includes the retina and lateral geniculate nucleus at the bottom as well as the entorhinal cortex and hippocampus at the top. Within this hierarchy, there are multiple, intertwined processing streams, which, at a low level, are related to the compartmental organization of areas V1 and V2 and, at a high level, are related to the distinction between processing centers in the temporal and parietal lobes. However, there are some pathways and relationships (about 10\% of the total) whose descriptions do not fit cleanly into this hierarchical scheme for one reason or another. In most instances, though, it is unclear whether these represent genuine exceptions to a strict hierarchy rather than inaccuracies or uncertainities in the reported assignment.},
	language = {eng},
	number = {1},
	journal = {Cerebral Cortex (New York, N.Y.: 1991)},
	author = {Felleman, D. J. and Van Essen, D. C.},
	month = feb,
	year = {1991},
	pmid = {1822724},
	keywords = {Animals, Brain Mapping, Cerebral Cortex, Macaca, Mental Processes},
	pages = {1--47}
}

@misc{noauthor_why_nodate,
	title = {Why {Neurons} {Have} {Thousands} of {Synapses}, {A} {Theory} of {Sequence} {Memory} in {Neocortex}},
	url = {https://numenta.com/resources/papers/why-neurons-have-thousands-of-synapses-theory-of-sequence-memory-in-neocortex/},
	urldate = {2018-06-23},
	file = {Why Neurons Have Thousands of Synapses, A Theory of Sequence Memory in Neocortex:/home/dario/Zotero/storage/9FI832U6/why-neurons-have-thousands-of-synapses-theory-of-sequence-memory-in-neocortex.html:text/html}
}

@misc{noauthor_htm_nodate,
	title = {The {HTM} {Spatial} {Pooler}—{A} {Neocortical} {Algorithm} for {Online} {Sparse} {Distributed} {Coding}},
	url = {https://numenta.com/resources/papers/htm-spatial-pooler-neocortical-algorithm-for-online-sparse-distributed-coding/},
	urldate = {2018-06-23},
	file = {The HTM Spatial Pooler—A Neocortical Algorithm for Online Sparse Distributed Coding:/home/dario/Zotero/storage/IBSEAFLJ/htm-spatial-pooler-neocortical-algorithm-for-online-sparse-distributed-coding.html:text/html}
}

@misc{noauthor_theory_nodate,
	title = {A {Theory} of {How} {Columns} in the {Neocortex} {Enable} {Learning} the {Structure} of the {World}},
	url = {https://numenta.com/resources/papers/a-theory-of-how-columns-in-the-neocortex-enable-learning-the-structure-of-the-world/},
	urldate = {2018-06-23},
	file = {A Theory of How Columns in the Neocortex Enable Learning the Structure of the World:/home/dario/Zotero/storage/HYM4EZCK/a-theory-of-how-columns-in-the-neocortex-enable-learning-the-structure-of-the-world.html:text/html}
}

@article{saffran_statistical_1996,
	title = {Statistical learning by 8-month-old infants},
	volume = {274},
	issn = {0036-8075},
	abstract = {Learners rely on a combination of experience-independent and experience-dependent mechanisms to extract information from the environment. Language acquisition involves both types of mechanisms, but most theorists emphasize the relative importance of experience-independent mechanisms. The present study shows that a fundamental task of language acquisition, segmentation of words from fluent speech, can be accomplished by 8-month-old infants based solely on the statistical relationships between neighboring speech sounds. Moreover, this word segmentation was based on statistical learning from only 2 minutes of exposure, suggesting that infants have access to a powerful mechanism for the computation of statistical properties of the language input.},
	language = {eng},
	number = {5294},
	journal = {Science (New York, N.Y.)},
	author = {Saffran, J. R. and Aslin, R. N. and Newport, E. L.},
	month = dec,
	year = {1996},
	pmid = {8943209},
	keywords = {Discrimination Learning, Humans, Infant, Language Development, Learning, Speech Perception},
	pages = {1926--1928}
}


@article{horton_cortical_2005,
	title = {The cortical column: a structure without a function},
	volume = {360},
	issn = {0962-8436},
	shorttitle = {The cortical column},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1569491/},
	doi = {10.1098/rstb.2005.1623},
	abstract = {This year, the field of neuroscience celebrates the 50th anniversary of Mountcastle's discovery of the cortical column. In this review, we summarize half a century of research and come to the disappointing realization that the column may have no function. Originally, it was described as a discrete structure, spanning the layers of the somatosensory cortex, which contains cells responsive to only a single modality, such as deep joint receptors or cutaneous receptors. Subsequently, examples of columns have been uncovered in numerous cortical areas, expanding the original concept to embrace a variety of different structures and principles. A ‘column’ now refers to cells in any vertical cluster that share the same tuning for any given receptive field attribute. In striate cortex, for example, cells with the same eye preference are grouped into ocular dominance columns. Unaccountably, ocular dominance columns are present in some species, but not others. In principle, it should be possible to determine their function by searching for species differences in visual performance that correlate with their presence or absence. Unfortunately, this approach has been to no avail; no visual faculty has emerged that appears to require ocular dominance columns. Moreover, recent evidence has shown that the expression of ocular dominance columns can be highly variable among members of the same species, or even in different portions of the visual cortex in the same individual. These observations deal a fatal blow to the idea that ocular dominance columns serve a purpose. More broadly, the term ‘column’ also denotes the periodic termination of anatomical projections within or between cortical areas. In many instances, periodic projections have a consistent relationship with some architectural feature, such as the cytochrome oxidase patches in V1 or the stripes in V2. These tissue compartments appear to divide cells with different receptive field properties into distinct processing streams. However, it is unclear what advantage, if any, is conveyed by this form of columnar segregation. Although the column is an attractive concept, it has failed as a unifying principle for understanding cortical function. Unravelling the organization of the cerebral cortex will require a painstaking description of the circuits, projections and response properties peculiar to cells in each of its various areas.},
	number = {1456},
	urldate = {2018-07-10},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Horton, Jonathan C and Adams, Daniel L},
	month = apr,
	year = {2005},
	pmid = {15937015},
	pmcid = {PMC1569491},
	pages = {837--862},
	file = {PubMed Central Full Text PDF:/home/dario/Zotero/storage/DUQQAB99/Horton and Adams - 2005 - The cortical column a structure without a functio.pdf:application/pdf}
}

@article{dematties2018,
author = {Dario Dematties and Silvio Rizzi and George K. Thiruvathukal and Alejandro Wainselboim and Silvano Zanutto},
year = {2018},
title = {Phonetic Acquisition in Cortical Dynamics, a Computational Approach},
journal = {Plos One (Under peer review process)}
}



@article {PMID:17451657,
	Title = {Top-down knowledge supports the retrieval of lexical information from degraded speech},
	Author = {Hannemann, R and Obleser, J and Eulitz, C},
	DOI = {10.1016/j.brainres.2007.03.069},
	Volume = {1153},
	Month = {June},
	Year = {2007},
	Journal = {Brain research},
	ISSN = {0006-8993},
	Pages = {134—143},
	URL = {https://doi.org/10.1016/j.brainres.2007.03.069},
}


@article{OBLESER2011713,
title = "Multiple brain signatures of integration in the comprehension of degraded speech",
journal = "NeuroImage",
volume = "55",
number = "2",
pages = "713 - 723",
year = "2011",
issn = "1053-8119",
doi = "https://doi.org/10.1016/j.neuroimage.2010.12.020",
url = "http://www.sciencedirect.com/science/article/pii/S1053811910016034",
author = "Jonas Obleser and Sonja A. Kotz",
keywords = "EEG, Auditory, Speech comprehension, Semantic processing, N100, N400, Gamma",
abstract = "When listening to speech under adverse conditions, expectancies resulting from semantic context can have a strong impact on comprehension. Here we ask how minimal variations in semantic context (cloze probability) affect the unfolding comprehension of acoustically degraded speech. Three main results are observed in the brain electric response. First, auditory evoked responses to a degraded sentence's onset (N100) correlate with participants' comprehension scores, but are generally more vigorous for more degraded sentences. Second, a pronounced N400 in response to low-cloze sentence-final words, reflecting the integration effort of words into context, increases linearly with improving speech intelligibility. Conversely, transient enhancement in Gamma band power (γ, ~40–70Hz) during high-cloze sentence-final words (~600ms) reflects top-down-facilitated integration. This γ-band effect also varies parametrically with signal quality. Third, a negative correlation of N100 amplitude at sentence onset and the later γ-band response is found in moderately degraded speech. This reflects two partly distinct neural strategies when dealing with moderately degraded speech; a more “bottom-up,” resource-allocating, and effortful versus a more “top-down,” associative and facilitatory strategy. Results also emphasize the non-redundant contributions of phase-locked (evoked) and non-phase-locked (induced) oscillatory brain dynamics in auditory EEG."
}

@article{10.1093/cercor/bhp128,
    author = {Obleser, Jonas and Kotz, Sonja A.},
    title = "{Expectancy Constraints in Degraded Speech Modulate the Language Comprehension Network}",
    journal = {Cerebral Cortex},
    volume = {20},
    number = {3},
    pages = {633-640},
    year = {2009},
    month = {06},
    abstract = "{In speech comprehension, the processing of auditory information and linguistic context are mutually dependent. This functional magnetic resonance imaging study examines how semantic expectancy (“cloze probability”) in variably intelligible sentences (“noise vocoding”) modulates the brain bases of comprehension. First, intelligibility-modulated activation along the superior temporal sulci (STS) was extended anteriorly and posteriorly in low-cloze sentences (e.g., “she weighs the flour”) but restricted to a mid-superior temporal gyrus/STS area in more predictable high-cloze sentences (e.g., “she sifts the flour”). Second, the degree of left inferior frontal gyrus (IFG) (Brodmann's area 44) involvement in processing low-cloze constructions was proportional to increasing intelligibility. Left inferior parietal cortex (IPC; angular gyrus) activation accompanied successful speech comprehension that derived either from increased signal quality or from semantic facilitation. The results show that successful decoding of speech in auditory cortex areas regulates language-specific computation (left IFG and IPC). In return, semantic expectancy can constrain these speech-decoding processes, with fewer neural resources being allocated to highly predictable sentences. These findings offer an important contribution toward the understanding of the functional neuroanatomy in speech comprehension.}",
    issn = {1047-3211},
    doi = {10.1093/cercor/bhp128},
    url = {https://dx.doi.org/10.1093/cercor/bhp128},
    eprint = {http://oup.prod.sis.lan/cercor/article-pdf/20/3/633/1164452/bhp128.pdf},
}


@article{Saffran1996StatisticalLB,
  title={Statistical learning by 8-month-old infants.},
  author={Jenny R. Saffran and Richard N. Aslin and Elissa L. Newport},
  journal={Science},
  year={1996},
  volume={274 5294},
  pages={
          1926-8
        }
}


@misc{dematties_dario_2019_2576130,
  author       = {Dematties, Dario and
                  Thiruvathukal, George K. and
                  Rizzi, Silvio and
                  Wainselboim, Alejandro Javier and
                  Zanutto, Bonifacio Silvano},
  title        = {{Datasets used to train and test the Cortical 
                   Spectro-Temporal Model (CSTM).}},
  month        = feb,
  year         = 2019,
  doi          = {10.5281/zenodo.2576130},
  url          = {https://doi.org/10.5281/zenodo.2576130}
}


@article{10.1093/biomet/75.2.383,
    author = {HOMMEL, G.},
    title = "{A stagewise rejective multiple test procedure based on a modified Bonferroni test}",
    journal = {Biometrika},
    volume = {75},
    number = {2},
    pages = {383-386},
    year = {1988},
    month = {06},
    abstract = "{Simes (1986) has proposed a modified Bonferroni procedure for the test of an overall hypothesis which is the combination of n individual hypotheses. In contrast to the classical Bonferroni procedure, it is not obvious how statements about individual hypotheses are to be made for this procedure. In the present paper a multiple test procedure allowing statements on individual hypotheses is proposed. It is based on the principle of closed test procedures (Marcus, Peritz \\&amp; Gabriel, 1976) and controls the multiple level α.}",
    issn = {0006-3444},
    doi = {10.1093/biomet/75.2.383},
    url = {https://doi.org/10.1093/biomet/75.2.383},
    eprint = {http://oup.prod.sis.lan/biomet/article-pdf/75/2/383/828082/75-2-383.pdf},
}

@ARTICLE{10.3389/fncom.2016.00094,
AUTHOR={Marblestone, Adam H. and Wayne, Greg and Kording, Konrad P.},
TITLE={Toward an Integration of Deep Learning and Neuroscience},
JOURNAL={Frontiers in Computational Neuroscience},
VOLUME={10},
PAGES={94},
YEAR={2016},
URL={https://www.frontiersin.org/article/10.3389/fncom.2016.00094},
DOI={10.3389/fncom.2016.00094}, 
ISSN={1662-5188},
ABSTRACT={Neuroscience has focused on the detailed implementation of computation, studying neural codes, dynamics and circuits. In machine learning, however, artificial neural networks tend to eschew precisely designed codes, dynamics or circuits in favor of brute force optimization of a cost function, often using simple and relatively uniform initial architectures. Two recent developments have emerged within machine learning that create an opportunity to connect these seemingly divergent perspectives. First, structured architectures are used, including dedicated systems for attention, recursion and various forms of short- and long-term memory storage. Second, cost functions and training procedures have become more complex and are varied across layers and over time. Here we think about the brain in terms of these ideas. We hypothesize that (1) the brain optimizes cost functions, (2) the cost functions are diverse and differ across brain locations and over development, and (3) optimization operates within a pre-structured architecture matched to the computational problems posed by behavior. In support of these hypotheses, we argue that a range of implementations of credit assignment through multiple layers of neurons are compatible with our current knowledge of neural circuitry, and that the brain's specialized systems can be interpreted as enabling efficient optimization for specific problem classes. Such a heterogeneously optimized system, enabled by a series of interacting cost functions, serves to make learning data-efficient and precisely targeted to the needs of the organism. We suggest directions by which neuroscience could seek to refine and test these hypotheses.}
}



@article{HOLLE2010875,
title = "Integration of iconic gestures and speech in left superior temporal areas boosts speech comprehension under adverse listening conditions",
journal = "NeuroImage",
volume = "49",
number = "1",
pages = "875 - 884",
year = "2010",
issn = "1053-8119",
doi = "https://doi.org/10.1016/j.neuroimage.2009.08.058",
url = "http://www.sciencedirect.com/science/article/pii/S1053811909009707",
author = "Henning Holle and Jonas Obleser and Shirley-Ann Rueschemeyer and Thomas C. Gunter",
keywords = "Language, fMRI, Multisensory, Audiovisual, Inverse effectiveness",
abstract = "Iconic gestures are spontaneous hand movements that illustrate certain contents of speech and, as such, are an important part of face-to-face communication. This experiment targets the brain bases of how iconic gestures and speech are integrated during comprehension. Areas of integration were identified on the basis of two classic properties of multimodal integration, bimodal enhancement and inverse effectiveness (i.e., greater enhancement for unimodally least effective stimuli). Participants underwent fMRI while being presented with videos of gesture-supported sentences as well as their unimodal components, which allowed us to identify areas showing bimodal enhancement. Additionally, we manipulated the signal-to-noise ratio of speech (either moderate or good) to probe for integration areas exhibiting the inverse effectiveness property. Bimodal enhancement was found at the posterior end of the superior temporal sulcus and adjacent superior temporal gyrus (pSTS/STG) in both hemispheres, indicating that the integration of iconic gestures and speech takes place in these areas. Furthermore, we found that the left pSTS/STG specifically showed a pattern of inverse effectiveness, i.e., the neural enhancement for bimodal stimulation was greater under adverse listening conditions. This indicates that activity in this area is boosted when an iconic gesture accompanies an utterance that is otherwise difficult to comprehend. The neural response paralleled the behavioral data observed. The present data extends results from previous gesture–speech integration studies in showing that pSTS/STG plays a key role in the facilitation of speech comprehension through simultaneous gestural input."
}


@article {Obleser2283,
	author = {Obleser, Jonas and Wise, Richard J. S. and Alex Dresner, M. and Scott, Sophie K.},
	title = {Functional Integration across Brain Regions Improves Speech Perception under Adverse Listening Conditions},
	volume = {27},
	number = {9},
	pages = {2283--2289},
	year = {2007},
	doi = {10.1523/JNEUROSCI.4663-06.2007},
	publisher = {Society for Neuroscience},
	abstract = {Speech perception is supported by both acoustic signal decomposition and semantic context. This study, using event-related functional magnetic resonance imaging, investigated the neural basis of this interaction with two speech manipulations, one acoustic (spectral degradation) and the other cognitive (semantic predictability). High compared with low predictability resulted in the greatest improvement in comprehension at an intermediate level of degradation, and this was associated with increased activity in the left angular gyrus, the medial and left lateral prefrontal cortices, and the posterior cingulate gyrus. Functional connectivity between these regions was also increased, particularly with respect to the left angular gyrus. In contrast, activity in both superior temporal sulci and the left inferior frontal gyrus correlated with the amount of spectral detail in the speech signal, regardless of predictability. These results demonstrate that increasing functional connectivity between high-order cortical areas, remote from the auditory cortex, facilitates speech comprehension when the clarity of speech is reduced.},
	issn = {0270-6474},
	URL = {http://www.jneurosci.org/content/27/9/2283},
	eprint = {http://www.jneurosci.org/content/27/9/2283.full.pdf},
	journal = {Journal of Neuroscience}
}


@inproceedings{Guerguiev2017TowardsDL,
  title={Towards deep learning with segregated dendrites},
  author={Jordan Guerguiev and Timothy P. Lillicrap and Blake A. Richards},
  booktitle={eLife},
  year={2017}
}


@article{Lillicrap_2016,
	doi = {10.1038/ncomms13276},
	url = {https://doi.org/10.1038%2Fncomms13276},
	year = 2016,
	month = {nov},
	publisher = {Springer Nature America, Inc},
	volume = {7},
	number = {1},
	author = {Timothy P. Lillicrap and Daniel Cownden and Douglas B. Tweed and Colin J. Akerman},
	title = {Random synaptic feedback weights support error backpropagation for deep learning},
	journal = {Nature Communications}
}



@article {Sur1437,
	author = {Sur, M and Garraghty, PE and Roe, AW},
	title = {Experimentally induced visual projections into auditory thalamus and cortex},
	volume = {242},
	number = {4884},
	pages = {1437--1441},
	year = {1988},
	doi = {10.1126/science.2462279},
	publisher = {American Association for the Advancement of Science},
	abstract = {Retinal cells have been induced to project into the medial geniculate nucleus, the principal auditory thalamic nucleus, in newborn ferrets by reduction of targets of retinal axons in one hemisphere and creation of alternative terminal space for these fibers in the auditory thalamus. Many cells in the medial geniculate nucleus are then visually driven, have large receptive fields, and receive input from retinal ganglion cells with small somata and slow conduction velocities. Visual cells with long conduction latencies and large contralateral receptive fields can also be recorded in primary auditory cortex. Some visual cells in auditory cortex are direction selective or have oriented receptive fields that resemble those of complex cells in primary visual cortex. Thus, functional visual projections can be routed into nonvisual structures in higher mammals, suggesting that the modality of a sensory thalamic nucleus or cortical area may be specified by its inputs during development.},
	issn = {0036-8075},
	URL = {http://science.sciencemag.org/content/242/4884/1437},
	eprint = {http://science.sciencemag.org/content/242/4884/1437.full.pdf},
	journal = {Science}
}


@article{doi:10.1002/(SICI)1096-9861(19981026)400:3<417::AID-CNE10>3.0.CO;2-O,
author = {Angelucci, Alessandra and Clascá, Francisco and Sur, Mriganka},
title = {Brainstem inputs to the ferret medial geniculate nucleus and the effect of early deafferentation on novel retinal projections to the auditory thalamus},
journal = {Journal of Comparative Neurology},
volume = {400},
number = {3},
pages = {417-439},
keywords = {retinal ganglion cells, plasticity, development, subcortical auditory pathways, inferior colliculus},
doi = {10.1002/(SICI)1096-9861(19981026)400:3<417::AID-CNE10>3.0.CO;2-O},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/%28SICI%291096-9861%2819981026%29400%3A3%3C417%3A%3AAID-CNE10%3E3.0.CO%3B2-O},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/%28SICI%291096-9861%2819981026%29400%3A3%3C417%3A%3AAID-CNE10%3E3.0.CO%3B2-O},
abstract = {Abstract Following specific neonatal brain lesions in rodents and ferrets, retinal axons have been induced to innervate the medial geniculate nucleus (MGN). Previous studies have suggested that reduction of normal retinal targets along with deafferentation of the MGN are two concurrent factors required for the induction of novel retino-MGN projections. We have examined, in ferrets, the relative influence of these two factors on the extent of the novel retinal projection. We first characterized the inputs to the normal MGN, and the most effective combination of neonatal lesions to deafferent this nucleus, by injecting retrograde tracers into the MGN of normal and neonatally operated adult ferrets, respectively. In a second group of experiments, newborn ferrets received different combinations of lesions of normal retinal targets and MGN afferents. The resulting extent of retino-MGN projections was estimated for each case at adulthood, by using intraocular injections of anterograde tracers. We found that the extent of retino-MGN projections correlates well with the extent of MGN deafferentation, but not with extent of removal of normal retinal targets. Indeed, the presence of at least some normal retinal targets seems necessary for the formation of retino-MGN connections. The diameters of retino-MGN axons suggest that more than one type of retinal ganglion cells innervate the MGN under a lesion paradigm that spares the visual cortex and lateral geniculate nucleus. We also found that, after extensive deafferentation of MGN, other axonal systems in addition to retinal axons project ectopically to the MGN. These data are consistent with the idea that ectopic retino-MGN projections develop by sprouting of axon collaterals in response to signals arising from the deafferented nucleus, and that these axons compete with other sets of axons for terminal space in the MGN. J. Comp. Neurol. 400:417–439, 1998. © 1998 Wiley-Liss, Inc.},,
year = {1998}
}


@article{Roe1992VisualPR,
  title={Visual projections routed to the auditory pathway in ferrets: receptive fields of visual neurons in primary auditory cortex.},
  author={Anna W. Roe and Sarah L. Pallas and Young Ho Kwon and Mriganka Sur},
  journal={The Journal of neuroscience : the official journal of the Society for Neuroscience},
  year={1992},
  volume={12 9},
  pages={3651-64}
}


@article {Roe818,
	author = {Roe, AW and Pallas, SL and Hahm, JO and Sur, M},
	title = {A map of visual space induced in primary auditory cortex},
	volume = {250},
	number = {4982},
	pages = {818--820},
	year = {1990},
	doi = {10.1126/science.2237432},
	publisher = {American Association for the Advancement of Science},
	abstract = {Maps of sensory surfaces are a fundamental feature of sensory cortical areas of the brain. The relative roles of afferents and targets in forming neocortical maps in higher mammals can be examined in ferrets in which retinal inputs are directed into the auditory pathway. In these animals, the primary auditory cortex contains a systematic representation of the retina (and of visual space) rather than a representation of the cochlea (and of sound frequency). A representation of a two-dimensional sensory epithelium, the retina, in cortex that normally represents a one-dimensional epithelium, the cochlea, suggests that the same cortical area can support different types of maps. Topography in the visual map arises both from thalamocortical projections that are characteristic of the auditory pathway and from patterns of retinal activity that provide the input to the map.},
	issn = {0036-8075},
	URL = {http://science.sciencemag.org/content/250/4982/818},
	eprint = {http://science.sciencemag.org/content/250/4982/818.full.pdf},
	journal = {Science}
}


@article{Sharma2000InductionOV,
  title={Induction of visual orientation modules in auditory cortex},
  author={Jitendra N. Sharma and Alessandra Angelucci and Mriganka Sur},
  journal={Nature},
  year={2000},
  volume={404},
  pages={841-847}
}


@article{mountcastle_1957,
	author = {Mountcastle, V.},
	title = {Modality and topographic properties of cat’s somatic sensory cortex},
	journal = {J. Neurophysiol.},
        year = {1957},
        volume = {20},
        pages = {408–434}
}


@article{doi:10.1098/rstb.1992.0070,
author = {Stuart Rosen  and Robert P. Carlyon  and C. J. Darwin  and Ian John Russell },
title = {Temporal information in speech: acoustic, auditory and linguistic aspects},
journal = {Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences},
volume = {336},
number = {1278},
pages = {367-373},
year = {1992},
doi = {10.1098/rstb.1992.0070}

URL = {https://royalsocietypublishing.org/doi/abs/10.1098/rstb.1992.0070},
eprint = {https://royalsocietypublishing.org/doi/pdf/10.1098/rstb.1992.0070}
,
    abstract = { The temporal properties of speech appear to play a more important role in linguistic contrasts than has hitherto been appreciated. Therefore, a new framework for describing the acoustic structure of speech based purely on temporal aspects has been developed. From this point of view, speech can be said to be comprised of three main temporal features, based on dominant fluctuation rates: envelope, periodicity, and fine-structure. Each feature has distinct acoustic manifestations, auditory and perceptual correlates, and roles in linguistic contrasts. The applicability of this three-featured temporal system is discussed in relation to hearing-impaired and normal listeners. }
}


@misc{dematties_dario_2019_2654939,
  author       = {Dematties, Dario and
                  Thiruvathukal, George K. and
                  Rizzi, Silvio and
                  Wainselboim, Alejandro Javier and
                  Zanutto, Bonifacio Silvano},
  title        = {{Experimental Results and Appendices: Cortical 
                   Spectro-Temporal Model (CSTM).}},
  month        = mar,
  year         = 2019,
  doi          = {10.5281/zenodo.2654939},
  url          = {https://doi.org/10.5281/zenodo.2654939}
}


@misc{dematties_dario_2019_2576130,
  author       = {Dematties, Dario and
                  Thiruvathukal, George K. and
                  Rizzi, Silvio and
                  Wainselboim, Alejandro Javier and
                  Zanutto, Bonifacio Silvano},
  title        = {{Datasets used to train and test the Cortical 
                   Spectro-Temporal Model (CSTM).}},
  month        = feb,
  year         = 2019,
  doi          = {10.5281/zenodo.2576130},
  url          = {https://doi.org/10.5281/zenodo.2576130}
}


@misc{dematties_dario_2019_2580396,
  author       = {Dematties, Dario and
                  Thiruvathukal, George K. and
                  Rizzi, Silvio and
                  Wainselboim, Alejandro Javier and
                  Zanutto, Bonifacio Silvano},
  title        = {neurophon/neurophon: Release for PLOS submission},
  month        = feb,
  year         = 2019,
  doi          = {10.5281/zenodo.2580396},
  url          = {https://doi.org/10.5281/zenodo.2580396}
}

@article{HODGKIN199025,
title = "A quantitative description of membrane current and its application to conduction and excitation in nerve",
journal = "Bulletin of Mathematical Biology",
volume = "52",
number = "1",
pages = "25 - 71",
year = "1990",
issn = "0092-8240",
doi = "https://doi.org/10.1016/S0092-8240(05)80004-7",
url = "http://www.sciencedirect.com/science/article/pii/S0092824005800047",
author = "A.L. Hodgkin and A.F. Huxley",
abstract = "This article concludes a series of papers concerned with the flow of electric current through the surface membrane of a giant nerve fibre (Hodgkinet al., 1952,J. Physiol.116, 424–448; Hodgkin and Huxley, 1952,J. Physiol.116, 449–566). Its general object is to discuss the results of the preceding papers (Section 1), to put them into mathematical form (Section 2) and to whow that they will account for conduction and excitation in quantitative terms (Sections 3–6)."
}


@article{Izhikevich2004SpiketimingDO,
  title={Spike-timing dynamics of neuronal groups.},
  author={Eugene M. Izhikevich and Joseph A. Gally and Gerald M. Edelman},
  journal={Cerebral cortex},
  year={2004},
  volume={14 8},
  pages={933-44}
}


@ARTICLE{1333071,
author={E. M. Izhikevich},
journal={IEEE Transactions on Neural Networks},
title={Which model to use for cortical spiking neurons?},
year={2004},
volume={15},
number={5},
pages={1063-1070},
keywords={neural nets;neurophysiology;cortical spiking neurons;biological plausibility;computational efficiency;bursting neurons;cortical neural networks;Neurons;Biological system modeling;Biological neural networks;Artificial neural networks;Information processing;Fires;Large-scale systems;Frequency;Computational efficiency;Computational modeling;Action Potentials;Animals;Cerebral Cortex;Humans;Models, Neurological;Nerve Net;Neural Pathways;Neurons;Nonlinear Dynamics;Reaction Time;Synapses;Synaptic Transmission},
doi={10.1109/TNN.2004.832719},
ISSN={1045-9227},
month={Sept},
}


@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	language = {en},
	number = {7553},
	urldate = {2018-06-19},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	pages = {436--444},
	annote = {Trends in Deep Learning Research
Unsupervised learning had a catalytic effect in reviving interest indeep learning, but has since been overshadowed by the successes ofpurely supervised learning. Although we have not focused on it in thisReview, we expect unsupervised learning to become far more importantin the longer term. Human and animal learning is largely unsupervised:we discover the structure of the world by observing it, not by being toldthe name of every object.Human vision is an active process that sequentially samples the opticarray in an intelligent, task-specific way using a small, high-resolutionfovea with a large, low-resolution surround. We expect much of thefuture progress in vision to come from systems that are trained end-to-end and combine ConvNets with RNNs that use reinforcement learningto decide where to look. Systems combining deep learning and rein-forcement learning are in their infancy, but they already outperformpassive vision systems99 at classification tasks and produce impressiveresults in learning to play many different video games100.Natural language understanding is another area in which deep learn-ing is poised to make a large impact over the next few years. We expectsystems that use RNNs to understand sentences or whole documentswill become much better when they learn strategies for selectivelyattending to one part at a time.Ultimately, major progress in artificial intelligence will come aboutthrough systems that combine representation learning with complexreasoning. Although deep learning and simple reasoning have beenused for speech and handwriting recognition for a long time, newparadigms are needed to replace rule-based manipulation of symbolicexpressions by operations on large vectors.},
	file = {LeCun et al. - 2015 - Deep learning.pdf:/home/dario/Zotero/storage/3JHKY65E/LeCun et al. - 2015 - Deep learning.pdf:application/pdf}
}


@article{10.1371/journal.pone.0217966,
    author = {Dematties, Dario AND Rizzi, Silvio AND Thiruvathukal, George K. AND Wainselboim, Alejandro AND Zanutto, B. Silvano},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Phonetic acquisition in cortical dynamics, a computational approach},
    year = {2019},
    month = {06},
    volume = {14},
    url = {https://doi.org/10.1371/journal.pone.0217966},
    pages = {1-28},
    abstract = {Many computational theories have been developed to improve artificial phonetic classification performance from linguistic auditory streams. However, less attention has been given to psycholinguistic data and neurophysiological features recently found in cortical tissue. We focus on a context in which basic linguistic units–such as phonemes–are extracted and robustly classified by humans and other animals from complex acoustic streams in speech data. We are especially motivated by the fact that 8-month-old human infants can accomplish segmentation of words from fluent audio streams based exclusively on the statistical relationships between neighboring speech sounds without any kind of supervision. In this paper, we introduce a biologically inspired and fully unsupervised neurocomputational approach that incorporates key neurophysiological and anatomical cortical properties, including columnar organization, spontaneous micro-columnar formation, adaptation to contextual activations and Sparse Distributed Representations (SDRs) produced by means of partial N-Methyl-D-aspartic acid (NMDA) depolarization. Its feature abstraction capabilities show promising phonetic invariance and generalization attributes. Our model improves the performance of a Support Vector Machine (SVM) classifier for monosyllabic, disyllabic and trisyllabic word classification tasks in the presence of environmental disturbances such as white noise, reverberation, and pitch and voice variations. Furthermore, our approach emphasizes potential self-organizing cortical principles achieving improvement without any kind of optimization guidance which could minimize hypothetical loss functions by means of–for example–backpropagation. Thus, our computational model outperforms multiresolution spectro-temporal auditory feature representations using only the statistical sequential structure immerse in the phonotactic rules of the input stream.},
    number = {6},
    doi = {10.1371/journal.pone.0217966}
}


@ARTICLE{10.3389/fncir.2016.00023,
AUTHOR={Hawkins, Jeff and Ahmad, Subutai},
TITLE={Why Neurons Have Thousands of Synapses, a Theory of Sequence Memory in Neocortex},
JOURNAL={Frontiers in Neural Circuits},
VOLUME={10},
PAGES={23},
YEAR={2016},
URL={https://www.frontiersin.org/article/10.3389/fncir.2016.00023},
DOI={10.3389/fncir.2016.00023},
ISSN={1662-5110},
ABSTRACT={Pyramidal neurons represent the majority of excitatory neurons in the neocortex. Each pyramidal neuron receives input from thousands of excitatory synapses that are segregated onto dendritic branches. The dendrites themselves are segregated into apical, basal, and proximal integration zones, which have different properties. It is a mystery how pyramidal neurons integrate the input from thousands of synapses, what role the different dendrites play in this integration, and what kind of network behavior this enables in cortical tissue. It has been previously proposed that non-linear properties of dendrites enable cortical neurons to recognize multiple independent patterns. In this paper we extend this idea in multiple ways. First we show that a neuron with several thousand synapses segregated on active dendrites can recognize hundreds of independent patterns of cellular activity even in the presence of large amounts of noise and pattern variation. We then propose a neuron model where patterns detected on proximal dendrites lead to action potentials, defining the classic receptive field of the neuron, and patterns detected on basal and apical dendrites act as predictions by slightly depolarizing the neuron without generating an action potential. By this mechanism, a neuron can predict its activation in hundreds of independent contexts. We then present a network model based on neurons with these properties that learns time-based sequences. The network relies on fast local inhibition to preferentially activate neurons that are slightly depolarized. Through simulation we show that the network scales well and operates robustly over a wide range of parameters as long as the network uses a sparse distributed code of cellular activations. We contrast the properties of the new network model with several other neural network models to illustrate the relative capabilities of each. We conclude that pyramidal neurons with thousands of synapses, active dendrites, and multiple integration zones create a robust and powerful sequence memory. Given the prevalence and similarity of excitatory neurons throughout the neocortex and the importance of sequence memory in inference and behavior, we propose that this form of sequence memory may be a universal property of neocortical tissue.}
}


@inproceedings{Bahrampour2015ComparativeSO,
  title={Comparative Study of Deep Learning Software Frameworks},
  author={Soheil Bahrampour and Naveen Ramakrishnan and Lukas Schott and Mohak Shah},
  year={2015}
}

@INPROCEEDINGS{7979887,
author={S. Shi and Q. Wang and P. Xu and X. Chu},
booktitle={2016 7th International Conference on Cloud Computing and Big Data (CCBD)},
title={Benchmarking State-of-the-Art Deep Learning Software Tools},
year={2016},
volume={},
number={},
pages={99-104},
abstract={Deep learning has been shown as a successful machine learning method for a variety of tasks, and its popularity results in numerous open-source deep learning software tools coming to public. Training a deep network is usually a very time-consuming process. To address the huge computational challenge in deep learning, many tools exploit hardware features such as multi-core CPUs and many-core GPUs to shorten the training and inference time. However, different tools exhibit different features and running performance when they train different types of deep networks on different hardware platforms, making it difficult for end users to select an appropriate pair of software and hardware. In this paper, we present our attempt to benchmark several state-of-the-art GPU-accelerated deep learning software tools, including Caffe, CNTK, TensorFlow, and Torch. We focus on evaluating the running time performance (i.e., speed) of these tools with three popular types of neural networks on two representative CPU platforms and three representative GPU platforms. Our contribution is two-fold. First, for end users of deep learning software tools, our benchmarking results can serve as a reference to selecting appropriate hardware platforms and software tools. Second, for developers of deep learning software tools, our in-depth analysis points out possible future directions to further optimize the running performance.},
keywords={benchmark testing;feedforward neural nets;graphics processing units;learning (artificial intelligence);microprocessor chips;performance evaluation;recurrent neural nets;software tools;open-source deep learning software tool benchmarking;machine learning method;deep network training;hardware platforms;GPU-accelerated deep learning software tools;Caffe;CNTK;TensorFlow;Torch;running time performance evaluation;CPU platform;Tools;Machine learning;Neural networks;Benchmark testing;Graphics processing units;Instruction sets;Training;Deep Learning;GPU;Feed-forward Neural Networks;Convolutional Neural Networks;Recurrent Neural Networks},
doi={10.1109/CCBD.2016.029},
ISSN={},
month={Nov},
}


@article{doi:10.3109/0954898X.2012.739292,
author = {Helge Ülo Dinkelbach and Julien Vitay and Frederik Beuth and Fred H Hamker},
title = {Comparison of GPU- and CPU-implementations of mean-firing rate neural networks on parallel hardware},
journal = {Network: Computation in Neural Systems},
volume = {23},
number = {4},
pages = {212-236},
year  = {2012},
publisher = {Taylor & Francis},
doi = {10.3109/0954898X.2012.739292},
    note ={PMID: 23140422},

URL = { 
	https://doi.org/10.3109/0954898X.2012.739292
    
},
eprint = { 
	https://doi.org/10.3109/0954898X.2012.739292
    
}
,
    abstract = { Modern parallel hardware such as multi-core processors (CPUs) and graphics processing units (GPUs) have a high computational power which can be greatly beneficial to the simulation of large-scale neural networks. Over the past years, a number of efforts have focused on developing parallel algorithms and simulators best suited for the simulation of spiking neural models. In this article, we aim at investigating the advantages and drawbacks of the CPU and GPU parallelization of mean-firing rate neurons, widely used in systems-level computational neuroscience. By comparing OpenMP, CUDA and OpenCL implementations towards a serial CPU implementation, we show that GPUs are better suited than CPUs for the simulation of very large networks, but that smaller networks would benefit more from an OpenMP implementation. As this performance strongly depends on data organization, we analyze the impact of various factors such as data structure, memory alignment and floating precision. We then discuss the suitability of the different hardware depending on the networks' size and connectivity, as random or sparse connectivities in mean-firing rate networks tend to break parallel performance on GPUs due to the violation of coalescence. }
}


@misc{noauthor_cooley_nodate,
	title = {Cooley {\textbar} {Argonne} {Leadership} {Computing} {Facility}},
	howpublished = {\url{https://www.alcf.anl.gov/user-guides/cooley}},
	note = {Accessed: 2018-11-24},
	file = {Cooley | Argonne Leadership Computing Facility:/home/dario/Zotero/storage/ZIXLEY4Q/cooley.html:text/html}
}


@book{edited_by_stephen_c._cunnane_human_2010,
	title = {Human brain evolution : the influence of freshwater and marine food resources},
	url = {https://search.library.wisc.edu/catalog/9910250984602121},
	abstract = {xix, 213 pages, 4 unnumbered pages of plates : illustrations (some color) ; 26 cm},
	publisher = {Hoboken, N.J. : Wiley-Blackwell, [2010] ©2010},
	author = {edited by Stephen C. Cunnane, Kathlyn M. Stewart},
	year = {2010},
	annote = {Includes bibliographical references and index.}
}


@book{10.2307/j.ctt5vkr1h,
 ISBN = {9780300181111},
 URL = {http://www.jstor.org/stable/j.ctt5vkr1h},
 abstract = {<p>In this classic work, one of the greatest mathematicians of the twentieth century explores the analogies between computing machines and the living human brain. John von Neumann, whose many contributions to science, mathematics, and engineering include the basic organizational framework at the heart of today's computers, concludes that the brain operates both digitally and analogically, but also has its own peculiar statistical language.</p><p>In his foreword to this new edition, Ray Kurzweil, a futurist famous in part for his own reflections on the relationship between technology and intelligence, places von Neumann's work in a historical context and shows how it remains relevant today.</p>},
 author = {JOHN VON NEUMANN},
 publisher = {Yale University Press},
 title = {The Computer and the Brain},
 year = {1986}
}


@article{Azevedo2009EqualNO,
  title={Equal numbers of neuronal and nonneuronal cells make the human brain an isometrically scaled-up primate brain.},
  author={Frederico A. C. Azevedo and Ludmila R B Carvalho and Lea Tenenholz Grinberg and Jos{\'e} Marcelo Farfel and Renata Eloah de Lucena Ferretti and Renata E. P. Leite and Wilson Jacob Filho and Roberto Lent and Suzana Herculano-Houzel},
  journal={The Journal of comparative neurology},
  year={2009},
  volume={513 5},
  pages={532-41}
}




@article{MAASS19971659,
title = "Networks of spiking neurons: The third generation of neural network models",
journal = "Neural Networks",
volume = "10",
number = "9",
pages = "1659 - 1671",
year = "1997",
issn = "0893-6080",
doi = "https://doi.org/10.1016/S0893-6080(97)00011-7",
url = "http://www.sciencedirect.com/science/article/pii/S0893608097000117",
author = "Wolfgang Maass",
keywords = "Spiking neuron, Integrate-and-fire neutron, Computational complexity, Sigmoidal neural nets, Lower bounds",
abstract = "The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology."
}

@InProceedings{10.1007/978-3-642-03156-4_17,
author="Ghosh-Dastidar, Samanwoy
and Adeli, Hojjat",
editor="Yu, Wen
and Sanchez, Edgar N.",
title="Third Generation Neural Networks: Spiking Neural Networks",
booktitle="Advances in Computational Intelligence",
year="2009",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="167--178",
abstract="Artificial Neural Networks (ANNs) are based on highly simplified brain dynamics and have been used as powerful computational tools to solve complex pattern recognition, function estimation, and classification problems. Throughout their development, ANNs have been evolving towards more powerful and more biologically realistic models. In the last decade, the third generation Spiking Neural Networks (SNNs) have been developed which comprise of spiking neurons. Information transfer in these neurons models the information transfer in biological neurons, i.e., via the precise timing of spikes or a sequence of spikes. Addition of the temporal dimension for information encoding in SNNs yields new insight into the dynamics of the human brain and has the potential to result in compact representations of large neural networks. As such, SNNs have great potential for solving complicated time-dependent pattern recognition problems defined by time series because of their inherent dynamic representation. This article presents an overview of the development of spiking neurons and SNNs within the context of feedforward networks, and provides insight into their potential for becoming the next generation neural networks.",
isbn="978-3-642-03156-4"
}



@article{McCulloch1990ALC,
  title={A logical calculus of the ideas immanent in nervous activity. 1943.},
  author={Warren S. McCulloch and Walter Pitts},
  journal={Bulletin of mathematical biology},
  year={1990},
  volume={52 1-2},
  pages={99-115; discussion 73-97}
}




@book{Valiant:1994:CM:199266,
 author = {Valiant, Leslie G.},
 title = {Circuits of the Mind},
 year = {1994},
 isbn = {0-19-508926-X},
 publisher = {Oxford University Press, Inc.},
 address = {New York, NY, USA},
} 

@article{Abeles1993SpatiotemporalFP,
  title={Spatiotemporal firing patterns in the frontal cortex of behaving monkeys.},
  author={M. Abeles and Hagai Bergman and E Margalit and Eilon Vaadia},
  journal={Journal of neurophysiology},
  year={1993},
  volume={70 4},
  pages={1629-38}
}

@article{bair1994,
  title={Reliable temporal modulation in cortical spike trains in the awake monkey.},
  author={W. Bair and C. Koch and W. T. Newsoine and K. H. Britten},
  journal={Proceeding of Dynamics of Neural Processing},
  year={1994},
  pages={84-88}
}





@ARTICLE{10.3389/fninf.2015.00019,
AUTHOR={Vitay, Julien and Dinkelbach, Helge and Hamker, Fred},
TITLE={ANNarchy: a code generation approach to neural simulations on parallel hardware},
JOURNAL={Frontiers in Neuroinformatics},
VOLUME={9},
PAGES={19},
YEAR={2015},
URL={https://www.frontiersin.org/article/10.3389/fninf.2015.00019},
DOI={10.3389/fninf.2015.00019},
ISSN={1662-5196},
ABSTRACT={Many modern neural simulators focus on the simulation of networks of spiking neurons on parallel hardware. Another important framework in computational neuroscience, rate-coded neural networks, is mostly difficult or impossible to implement using these simulators. We present here the ANNarchy (Artificial Neural Networks architect) neural simulator, which allows to easily define and simulate rate-coded and spiking networks, as well as combinations of both. The interface in Python has been designed to be close to the PyNN interface, while the definition of neuron and synapse models can be specified using an equation-oriented mathematical description similar to the Brian neural simulator. This information is used to generate C++ code that will efficiently perform the simulation on the chosen parallel hardware (multi-core system or graphical processing unit). Several numerical methods are available to transform ordinary differential equations into an efficient C++ code. We compare the parallel performance of the simulator to existing solutions.}
}


@article{HUQQANI2013349,
title = "Multicore and GPU Parallelization of Neural Networks for Face Recognition",
journal = "Procedia Computer Science",
volume = "18",
pages = "349 - 358",
year = "2013",
note = "2013 International Conference on Computational Science",
issn = "1877-0509",
doi = "https://doi.org/10.1016/j.procs.2013.05.198",
url = "http://www.sciencedirect.com/science/article/pii/S1877050913003414",
author = "Altaf Ahmad Huqqani and Erich Schikuta and Sicen Ye and Peng Chen",
keywords = "Parallel Simulation, Multicores, GPUs, OpenMP, CUDA, Artificial Neural Network",
abstract = "Training of Artificial Neural Networks for large data sets is a time consuming task. Various approaches have been proposed to reduce the efforts, many of them by applying parallelization techniques. In this paper we develop and analyze two novel parallel training approaches for Backpropagation neural networks for face recognition. We focus on two specific paralleliza- tion environments, using on the one hand OpenMP on a conventional multithreaded CPU and CUDA on a GPU. Based on our findings we give guidelines for the efficient parallelization of Backpropagation neural networks on multicore and GPU architectures. Additionally, we present a traversal method finding the best combination of learning rate and momentum term by varying the number of hidden neurons supporting the parallelization efforts."
}


@inproceedings{Schuessler:2011:PTA:1997052.1997062,
 author = {Schuessler, Olena and Loyola, Diego},
 title = {Parallel Training of Artificial Neural Networks Using Multithreaded and Multicore CPUs},
 booktitle = {Proceedings of the 10th International Conference on Adaptive and Natural Computing Algorithms - Volume Part I},
 series = {ICANNGA'11},
 year = {2011},
 isbn = {978-3-642-20281-0},
 location = {Ljubljana, Slovenia},
 pages = {70--79},
 numpages = {10},
 url = {http://dl.acm.org/citation.cfm?id=1997052.1997062},
 acmid = {1997062},
 publisher = {Springer-Verlag},
 address = {Berlin, Heidelberg},
 keywords = {multithreading and multicore, neural network training, pthreads and openMP parallelization},
} 

@inproceedings{Strey2003ACO,
  title={A comparison of OpenMP and MPI for neural network simulations on a SunFire 6800},
  author={Alfred Strey},
  booktitle={PARCO},
  year={2003}
}


@INPROCEEDINGS{6232827,
author={B. P. Gonzalez and G. G. Sánchez and J. P. Donate and P. Cortez and A. S. de Miguel},
booktitle={2012 IEEE Conference on Evolving and Adaptive Intelligent Systems},
title={Parallelization of an evolving Artificial Neural Networks system to Forecast Time Series using OPENMP and MPI},
year={2012},
volume={},
number={},
pages={186-191},
abstract={Time Series Forecasting (TSF) is a key tool to support decision making, for instance by producing better estimates to be used when planning production resources. Artificial Neural Networks (ANN) are innate candidates for TSF due to advantages such as nonlinear learning and noise tolerance. The search for the best ANN is a complex task that strongly affects the forecasting performance while often requiring a high computational time. However, obtaining fast predictions is a relevant issue in several real-world scenarios, such as real-time and control systems. In this work, we present an Evolutionary (EANN) approach for TSF based on Estimation Distribution Algorithm (EDA) that evolves fully connected Artificial Neural Network (EANN). To speed up such approach, we propose the use of two parallel programming standards: Message Passing Interface (MPI) and Open Multi-Processing (OpenMP). Several experiments were held, using five real-world time series with different characteristics and from distinct domains, in order to compare with sequential EANN approach with the MPI and OpenMP parallel variants, under a number of cores that ranged from 1 to 6. Overall, the EANN results are competitive when compared with the popular ForecastPro tool. Moreover, the setup that included the MPI parallelization method and the use of 5 cores lead to the lowest execution times, while making a reasonable use of the available computational resources.},
keywords={decision making;message passing;neural nets;parallel programming;production planning;time series;parallelization;artificial neural networks system;time series forecasting;TSF;decision making;production resources planning;EANN;estimation distribution algorithm;EDA;parallel programming;message passing interface;MPI;open multi-processing;OpenMP;Standards;Yttrium;Biological cells;Artificial neural networks;Computational modeling},
doi={10.1109/EAIS.2012.6232827},
ISSN={},
month={May},
}

@INPROCEEDINGS{6511739,
author={M. Joldos and I. L. Muntean},
booktitle={2013 11th RoEduNet International Conference},
title={Multicore asynchronous simulation of spiking neural networks on the grid},
year={2013},
volume={},
number={},
pages={1-4},
abstract={Dynamics analysis studies of spiking neural network behavior entails the computation of a large number of simulation scenarios. Moreover, when the simulated neural micro-circuits are fairly large, the use of multiple cores in a simulation tends to be beneficial. In this paper we focus on the parallelization of an asynchronous simulation strategy with OpenMP threads and on the management of the neural simulations on compute grids. The first results show a parallel efficiency in the ranges of 0.31-0.98, with hyper-threading disabled or enabled, resp. By using the grid as underlying middleware for performing the simulations, we could generate and manage hundreds of simulation scenarios in an easy way.},
keywords={biology computing;digital simulation;grid computing;message passing;middleware;multiprocessing systems;multi-threading;neural nets;neurophysiology;parallel processing;multicore asynchronous simulation;dynamics analysis;spiking neural network behavior;simulation scenario;neural microcircuit simulation;asynchronous simulation strategy parallelization;OpenMP thread;neural simulation management;compute grid;parallel efficiency;hyperthreading;middleware;Computational modeling;Neurons;Integrated circuit modeling;Instruction sets;Biological system modeling;Multicore processing;Neural microtechnology;Grid computing;Spiking Neural Networks;Asynchronous Simulation;Globus Toolkit;Globus Online Client},
doi={10.1109/RoEduNet.2013.6511739},
ISSN={2068-1038},
month={Jan},
}


@inproceedings{Chatzikonstantis:2016:FID:2903150.2903477,
 author = {Chatzikonstantis, George and Rodopoulos, Dimitrios and Nomikou, Sofia and Strydis, Christos and De Zeeuw, Chris I. and Soudris, Dimitrios},
 title = {First Impressions from Detailed Brain Model Simulations on a Xeon/Xeon-Phi Node},
 booktitle = {Proceedings of the ACM International Conference on Computing Frontiers},
 series = {CF '16},
 year = {2016},
 isbn = {978-1-4503-4128-8},
 location = {Como, Italy},
 pages = {361--364},
 numpages = {4},
 url = {http://doi.acm.org/10.1145/2903150.2903477},
 doi = {10.1145/2903150.2903477},
 acmid = {2903477},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {MPI, OpenMP, neuron modeling, performance},
} 


@INPROCEEDINGS{6821186,
author={M. Joldos and O. Vinteler and I. R. Peter and I. L. Muntean},
booktitle={2013 15th International Symposium on Symbolic and Numeric Algorithms for Scientific Computing},
title={MPI-Based Asynchronous Simulation of Spiking Neural Networks on the Grid},
year={2013},
volume={},
number={},
pages={481-487},
abstract={Brain microcircuits exhibit an almost chaotic behavior. This is of high interest in developing new computational models or designing high capacity storage systems. Therefore, the simulation of such microcircuits must preserve the brain dynamic behavior. But investigating the dynamics analysis of such systems is a complex computational task due to the large number of neurons and synapses in the network, the large number of simulation scenarios that need to be computed, and to their model representation. To address the first challenge, we propose in this paper an MPI-based parallelization scheme of the asynchronous spiking neural network simulation algorithm. Due to the partitioning method, we can compute scenarios with more than 50.000 neurons and 300 millions synapses. The proposed solution has been evaluated on production HPC systems, employing up to 512 parallel processes. We contribute to the second challenge by extending the fACIBiNET framework with client-side capabilities for the Globus Online service. As such, scenarios with both high-throughput and high-performance computing requirements are managed in an efficient manner from within the framework, using grid technologies.},
keywords={grid computing;integrated circuit noise;integrated circuits;message passing;neural nets;MPI-based asynchronous simulation;spiking neural networks;brain microcircuits;chaotic behavior;computational models;high capacity storage systems;brain dynamic behavior;dynamics analysis;complex computational task;neurons;synapses;MPI-based parallelization scheme;asynchronous spiking neural network simulation algorithm;partitioning method;production HPC systems;fACIBiNET framework;client-side capabilities;Globus Online service;high performance computing requirements;grid technologies;Neurons;Computational modeling;Biological system modeling;Brain modeling;Delays;Mathematical model;Neural microtechnology;Grid computing;Spiking Neural Networks;Asynchronous Simulation},
doi={10.1109/SYNASC.2013.69},
ISSN={},
month={Sept},
}


@article{Pastorelli2015ScalingT1,
  title={Scaling to 1024 software processes and hardware cores of the distributed simulation of a spiking neural network including up to 20G synapses},
  author={Elena Pastorelli and Pier Stanislao Paolucci and Roberto Ammendola and Andrea Biagioni and Ottorino Frezza and Francesca Lo Cicero and Alessandro Lonardo and Michele Martinelli and Francesco Simula and Piero Vicini},
  journal={CoRR},
  year={2015},
  volume={abs/1511.09325}
}


@book{hu2012biophysically,
  title={Biophysically Accurate Brain Modeling and Simulation Using Hybrid MPI/OpenMP Parallel Processing},
  author={Hu, J.},
  url={https://books.google.com.ar/books?id=F3pVAQAACAAJ},
  year={2012},
  publisher={Texas A \& M University}
}


@INPROCEEDINGS{7733347,
author={S. Ristov and R. Prodan and M. Gusev and K. Skala},
booktitle={2016 Federated Conference on Computer Science and Information Systems (FedCSIS)},
title={Superlinear speedup in HPC systems: Why and when?},
year={2016},
volume={},
number={},
pages={889-898},
abstract={The speedup is usually limited by two main laws in high-performance computing, that is, the Amdahl's and Gustafson's laws. However, the speedup sometimes can reach far beyond the limited linear speedup, known as superlinear speedup, which means that the speedup is greater than the number of processors that are used. Although the superlinear speedup is not a new concept and many authors have already reported its existence, most of them reported it as a side effect, without explaining why and how it is happening. In this paper, we analyze several different superlinear speedup types and define a taxonomy for them. Additionally, we present several explanations and cases of superlinearity existence for different types of granular algorithms (tasks), which means that they can be divided into many sub-tasks and scattered to the processors for execution. Apart from frequent explanation that having more cache memory in parallel execution is the main reason, we summarize other different effects that cause the superlinearity, including the superlinear speedup in cloud virtual environment for both vertical and horizontal scaling.},
keywords={cache storage;cloud computing;multiprocessing systems;parallel processing;superlinear speedup;HPC systems;granular algorithms;cache memory;parallel execution;cloud virtual environment;high-performance computing;Program processors;Cloud computing;Cache memory;Clocks;Complexity theory;Throughput;Synchronization;Cache memory;load;parallel and distributed processing;performance},
doi={},
ISSN={},
month={Sept},
}


@article{DBLP:journals/corr/AhmadH15,
  author    = {Subutai Ahmad and
               Jeff Hawkins},
  title     = {Properties of Sparse Distributed Representations and their Application
               to Hierarchical Temporal Memory},
  journal   = {CoRR},
  volume    = {abs/1503.07469},
  year      = {2015},
  url       = {http://arxiv.org/abs/1503.07469},
  archivePrefix = {arXiv},
  eprint    = {1503.07469},
  timestamp = {Mon, 13 Aug 2018 01:00:00 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/AhmadH15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article {Saffran12874,
	author = {Saffran, Jenny R. and Senghas, Ann and Trueswell, John C.},
	title = {The acquisition of language by children},
	volume = {98},
	number = {23},
	pages = {12874--12875},
	year = {2001},
	doi = {10.1073/pnas.231498898},
	publisher = {National Academy of Sciences},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/98/23/12874},
	eprint = {https://www.pnas.org/content/98/23/12874.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@article{Romberg2010StatisticalLA,
  title={Statistical learning and language acquisition.},
  author={Alexa R. Romberg and Jenny R. Saffran},
  journal={Wiley interdisciplinary reviews. Cognitive science},
  year={2010},
  volume={1 6},
  pages={
          906-914
        }
}

@article{10.1371/journal.pone.0177794,
    author = {Lopopolo, Alessandro AND Frank, Stefan L. AND van den Bosch, Antal AND Willems, Roel M.},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Using stochastic language models (SLM) to map lexical, syntactic, and phonological information processing in the brain},
    year = {2017},
    month = {05},
    volume = {12},
    url = {https://doi.org/10.1371/journal.pone.0177794},
    pages = {1-18},
    abstract = {Language comprehension involves the simultaneous processing of information at the phonological, syntactic, and lexical level. We track these three distinct streams of information in the brain by using stochastic measures derived from computational language models to detect neural correlates of phoneme, part-of-speech, and word processing in an fMRI experiment. Probabilistic language models have proven to be useful tools for studying how language is processed as a sequence of symbols unfolding in time. Conditional probabilities between sequences of words are at the basis of probabilistic measures such as surprisal and perplexity which have been successfully used as predictors of several behavioural and neural correlates of sentence processing. Here we computed perplexity from sequences of words and their parts of speech, and their phonemic transcriptions. Brain activity time-locked to each word is regressed on the three model-derived measures. We observe that the brain keeps track of the statistical structure of lexical, syntactic and phonological information in distinct areas.},
    number = {5},
    doi = {10.1371/journal.pone.0177794}
}

@article{doi:10.1111/infa.12094,
author = {van Heugten, Marieke and Christophe, Anne},
title = {Infants' Acquisition of Grammatical Gender Dependencies},
journal = {Infancy},
volume = {20},
number = {6},
pages = {675-683},
doi = {10.1111/infa.12094},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/infa.12094},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/infa.12094},
abstract = {To successfully understand spoken language, listeners need to determine how words within sentences relate to one another. Although the ability to compute relationships between word categories is known to develop early in life, little research has been conducted on infants' early sensitivity to subcategorical dependencies, such as those evoked by grammatical gender (where the article form is dictated by the noun's gender). This study therefore examines whether French-learning 18-month-olds track such relationships. Using the Visual Fixation Procedure, infants were presented with article–noun sequences in which the gender-marked article either matched (e.g., laFEM poussetteFEM “the stroller”) or mismatched (e.g., leMASC poussetteFEM) the gender of the noun. A clear preference for correct over incorrect co-occurrences was observed, suggesting that by 18 months of age, children's storage and access of words is sufficiently sophisticated to include the means to track subcategorical dependencies. This early sensitivity to gender information may be greatly beneficial for constraining lexical access during online language processing.},
year = {2015}
}

@article{doi:10.1111/j.1467-8624.2012.01869.x,
author = {Cyr, Marilyn and Shi, Rushen},
title = {Development of Abstract Grammatical Categorization in Infants},
journal = {Child Development},
volume = {84},
number = {2},
pages = {617-629},
doi = {10.1111/j.1467-8624.2012.01869.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-8624.2012.01869.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-8624.2012.01869.x},
abstract = {This study examined abstract syntactic categorization in infants, using the case of grammatical gender. Ninety-six French-learning 14-, 17-, 20-, and 30-month-olds completed the study. In a preferential looking procedure infants were tested on their generalized knowledge of grammatical gender involving pseudonouns and gender-marking determiners. The pseudonouns were controlled to contain no phonological or acoustical cues to gender. The determiner gender feature was the only information available. During familiarization, some pseudonouns followed a masculine determiner and others a feminine determiner. Test trials presented the same pseudonouns with different determiners in correct (consistent with familiarization gender pairing) versus incorrect gender agreement. Twenty-month-olds showed emerging knowledge of gender categorization and agreement. This knowledge was robust in 30-month-olds. These findings demonstrate that abstract, productive grammatical representations are present early in acquisition.},
year = {2013}
}

@article{Gibson1998-GIBCOS,
	pages = {262--268},
	title = {Constraints on Sentence Comprehension},
	volume = {2},
	journal = {Trends in Cognitive Sciences},
	author = {Edward Gibson and Neal J. Pearlmutter},
	number = {7},
	doi = {10.1016/s1364-6613(98)01187-5},
	year = {1998}
}

@article{Rego1993TheCB,
  title={The connection between phonological, syntactic and semantic skills and children’s reading and spelling},
  author={L{\'u}cia Lins Browne Rego and Peter Elwood Bryant},
  journal={European Journal of Psychology of Education},
  year={1993},
  volume={8},
  pages={235-246}
}

@article{Hagoort2005OnBB,
  title={On Broca, brain, and binding: a new framework},
  author={Peter Hagoort},
  journal={Trends in Cognitive Sciences},
  year={2005},
  volume={9},
  pages={416-423}
}

@article{doi:10.1080/00437956.1954.11659520,
author = {Zellig S. Harris},
title = {Distributional Structure},
journal = {WORD},
volume = {10},
number = {2-3},
pages = {146-162},
year  = {1954},
publisher = {Routledge},
doi = {10.1080/00437956.1954.11659520},

URL = { 
        https://doi.org/10.1080/00437956.1954.11659520
    
},
eprint = { 
        https://doi.org/10.1080/00437956.1954.11659520
    
}

}


@article{doi:10.1207/s15327078in1002_5,
author = { Rushen   Shi  and  Janet F.   Werker  and  Anne   Cutler },
title = {Recognition and Representation of Function Words in English-Learning Infants},
journal = {Infancy},
volume = {10},
number = {2},
pages = {187-198},
year  = {2006},
publisher = {Routledge},
doi = {10.1207/s15327078in1002_5},

URL = { 
        https://www.tandfonline.com/doi/abs/10.1207/s15327078in1002_5
    
},
eprint = { 
        https://www.tandfonline.com/doi/pdf/10.1207/s15327078in1002_5
    
}

}

@article{lohmann_phonological_2017,
	title = {Phonological properties of word classes and directionality in conversion},
	volume = {10},
	issn = {1750-1245},
	url = {https://www.euppublishing.com/doi/abs/10.3366/word.2017.0108},
	doi = {10.3366/word.2017.0108},
	abstract = {In the study of the word-formation process of conversion, one particularly difficult task is to determine the directionality of the process, that is, to decide which word represents the base and wh...},
	number = {2},
	urldate = {2019-11-15},
	journal = {Word Structure},
	author = {Lohmann, Arne},
	month = oct,
	year = {2017},
	keywords = {conversion, directionality, English, phonology, word formation, zero-derivation},
	pages = {204--234},
	file = {EUP Snapshot:/home/dario/Zotero/storage/DD49CQIN/word.2017.html:text/html}
}

@article{10.1371/journal.pone.0017920,
    author = {Friederici, Angela D. AND Mueller, Jutta L. AND Oberecker, Regine},
    journal = {PLOS ONE},
    publisher = {Public Library of Science},
    title = {Precursors to Natural Grammar Learning: Preliminary Evidence from 4-Month-Old Infants},
    year = {2011},
    month = {03},
    volume = {6},
    url = {https://doi.org/10.1371/journal.pone.0017920},
    pages = {1-7},
    abstract = {When learning a new language, grammar—although difficult—is very important, as grammatical rules determine the relations between the words in a sentence. There is evidence that very young infants can detect rules determining the relation between neighbouring syllables in short syllable sequences. A critical feature of all natural languages, however, is that many grammatical rules concern the dependency relation between non-neighbouring words or elements in a sentence i.e. between an auxiliary and verb inflection as in is singing. Thus, the issue of when and how children begin to recognize such non-adjacent dependencies is fundamental to our understanding of language acquisition. Here, we use brain potential measures to demonstrate that the ability to recognize dependencies between non-adjacent elements in a novel natural language is observable by the age of 4 months. Brain responses indicate that 4-month-old German infants discriminate between grammatical and ungrammatical dependencies in auditorily presented Italian sentences after only brief exposure to correct sentences of the same type. As the grammatical dependencies are realized by phonologically distinct syllables the present data most likely reflect phonologically based implicit learning mechanisms which can serve as a precursor to later grammar learning.},
    number = {3},
    doi = {10.1371/journal.pone.0017920}
}

@article {Pallier2522,
	author = {Pallier, Christophe and Devauchelle, Anne-Dominique and Dehaene, Stanislas},
	title = {Cortical representation of the constituent structure of sentences},
	volume = {108},
	number = {6},
	pages = {2522--2527},
	year = {2011},
	doi = {10.1073/pnas.1018711108},
	publisher = {National Academy of Sciences},
	abstract = {Linguistic analyses suggest that sentences are not mere strings of words but possess a hierarchical structure with constituents nested inside each other. We used functional magnetic resonance imaging (fMRI) to search for the cerebral mechanisms of this theoretical construct. We hypothesized that the neural assembly that encodes a constituent grows with its size, which can be approximately indexed by the number of words it encompasses. We therefore searched for brain regions where activation increased parametrically with the size of linguistic constituents, in response to a visual stream always comprising 12 written words or pseudowords. The results isolated a network of left-hemispheric regions that could be dissociated into two major subsets. Inferior frontal and posterior temporal regions showed constituent size effects regardless of whether actual content words were present or were replaced by pseudowords (jabberwocky stimuli). This observation suggests that these areas operate autonomously of other language areas and can extract abstract syntactic frames based on function words and morphological information alone. On the other hand, regions in the temporal pole, anterior superior temporal sulcus and temporo-parietal junction showed constituent size effect only in the presence of lexico-semantic information, suggesting that they may encode semantic constituents. In several inferior frontal and superior temporal regions, activation was delayed in response to the largest constituent structures, suggesting that nested linguistic structures take increasingly longer time to be computed and that these delays can be measured with fMRI.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/108/6/2522},
	eprint = {https://www.pnas.org/content/108/6/2522.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@article{doi:10.1152/physrev.00006.2011,
author = {Friederici, Angela D.},
title = {The Brain Basis of Language Processing: From Structure to Function},
journal = {Physiological Reviews},
volume = {91},
number = {4},
pages = {1357-1392},
year = {2011},
doi = {10.1152/physrev.00006.2011},
    note ={PMID: 22013214},

URL = { 
        https://doi.org/10.1152/physrev.00006.2011
    
},
eprint = { 
        https://doi.org/10.1152/physrev.00006.2011
    
}
,
    abstract = { Language processing is a trait of human species. The knowledge about its neurobiological basis has been increased considerably over the past decades. Different brain regions in the left and right hemisphere have been identified to support particular language functions. Networks involving the temporal cortex and the inferior frontal cortex with a clear left lateralization were shown to support syntactic processes, whereas less lateralized temporo-frontal networks subserve semantic processes. These networks have been substantiated both by functional as well as by structural connectivity data. Electrophysiological measures indicate that within these networks syntactic processes of local structure building precede the assignment of grammatical and semantic relations in a sentence. Suprasegmental prosodic information overtly available in the acoustic language input is processed predominantly in a temporo-frontal network in the right hemisphere associated with a clear electrophysiological marker. Studies with patients suffering from lesions in the corpus callosum reveal that the posterior portion of this structure plays a crucial role in the interaction of syntactic and prosodic information during language processing. }
}

@article{doi:10.1146/annurev-neuro-071013-013847,
author = {Hagoort, Peter and Indefrey, Peter},
title = {The Neurobiology of Language Beyond Single Words},
journal = {Annual Review of Neuroscience},
volume = {37},
number = {1},
pages = {347-362},
year = {2014},
doi = {10.1146/annurev-neuro-071013-013847},
    note ={PMID: 24905595},

URL = { 
        https://doi.org/10.1146/annurev-neuro-071013-013847
    
},
eprint = { 
        https://doi.org/10.1146/annurev-neuro-071013-013847
    
}
,
    abstract = { A hallmark of human language is that we combine lexical building blocks retrieved from memory in endless new ways. This combinatorial aspect of language is referred to as unification. Here we focus on the neurobiological infrastructure for syntactic and semantic unification. Unification is characterized by a high-speed temporal profile including both prediction and integration of retrieved lexical elements. A meta-analysis of numerous neuroimaging studies reveals a clear dorsal/ventral gradient in both left inferior frontal cortex and left posterior temporal cortex, with dorsal foci for syntactic processing and ventral foci for semantic processing. In addition to core areas for unification, further networks need to be recruited to realize language-driven communication to its full extent. One example is the theory of mind network, which allows listeners and readers to infer the intended message (speaker meaning) from the coded meaning of the linguistic utterance. This indicates that sensorimotor simulation cannot handle all of language processing. }
}

@article{GOUCHA2015294,
title = "The language skeleton after dissecting meaning: A functional segregation within Broca's Area",
journal = "NeuroImage",
volume = "114",
pages = "294 - 302",
year = "2015",
issn = "1053-8119",
doi = "https://doi.org/10.1016/j.neuroimage.2015.04.011",
url = "http://www.sciencedirect.com/science/article/pii/S1053811915003018",
author = "Tomás Goucha and Angela D. Friederici",
keywords = "Language processing, fMRI, Broca's area, Syntax, Morphology, Pseudowords",
abstract = "Broca's area is proposed as a crucial brain area for linguistic computations. Language processing goes beyond word-level processing, also implying the integration of meaningful information (semantics) with the underlying structural skeleton (syntax). There is an on-going debate about the specialisation of the subregions of Broca's area—Brodmann areas (BA) 44 and 45—regarding the latter aspects. Here, we tested if syntactic information is specifically processed in BA 44, whereas BA 45 is mainly recruited for semantic processing. We contrasted conditions with sentence structure against conditions with random order in two fMRI experiments. Besides, in order to disentangle these processes, we systematically removed the amount of semantic information available in the stimuli. This was achieved in Experiment 1 by replacing meaningful words (content words) by pseudowords. Within real word conditions we found broad activation in the left hemisphere, including the inferior frontal gyrus (BA 44/45/47), the anterior temporal lobe and posterior superior temporal gyrus (pSTG) and sulcus (pSTS). For pseudowords we found a similar activation pattern, still involving BA 45. Among the pseudowords in Experiment 1, we kept those word elements that convey meaning like un- in unhappy or -hood in brotherhood (i.e. derivational morphology). In Experiment 2 we tested whether the activation in BA 45 was due to their presence. We therefore further removed derivational morphology, only leaving word elements that determine syntactic structure (i.e. inflectional morphology, e.g. the verb ending -s in he paints). Now, in the absence of all semantic cues, including derivational morphology, only BA 44 was active. Additional analyses showed a selective responsiveness of this area to syntax-relevant cues. These findings confirm BA 44 as a core area for the processing of pure syntactic information. This furthermore suggests that the brain represents structural and meaningful aspects of language separately."
}

@article{DECARLI2007933,
title = "Identification of activated regions during a language task",
journal = "Magnetic Resonance Imaging",
volume = "25",
number = "6",
pages = "933 - 938",
year = "2007",
note = "Proceedings of the International School on Magnetic Resonance and Brain Function",
issn = "0730-725X",
doi = "https://doi.org/10.1016/j.mri.2007.03.031",
url = "http://www.sciencedirect.com/science/article/pii/S0730725X07002585",
author = "Diego De Carli and Girolamo Garreffa and Claudio Colonnese and Giovanni Giulietti and Ludovica Labruna and Ennio Briselli and Soléakhéna Ken and Maria Antonietta Macrì and Bruno Maraviglia",
keywords = "Functional magnetic resonance imaging (fMRI), Language, Negative BOLD response",
abstract = "Functional magnetic resonance imaging (fMRI) techniques are based on the assumption that changes in neural activity are accompanied by modulation in the blood-oxygenation-level-dependent (BOLD) signal. In addition to conventional increases in BOLD signals, sustained negative BOLD signal changes are occasionally observed in many fMRI experiments, which show regions of cortex that seem to respond in antiphase with primary stimulus. The existence of this so-called negative BOLD response (NBR) has been observed and investigated in many functional studies. Several theoretical mechanisms have been proposed to account for it, but its origin has never been fully explained. In this study, the variability of fMRI activation, including the sources of the negative BOLD signal, during phonological and semantic language tasks, was investigated in six right-handed healthy subjects. We found significant activations in the brain regions, mainly in the left hemisphere, involved in the language stimuli [prominent in the inferior frontal gyrus, approximately Brodmann Areas (BA)7, BA44, BA45 and BA47, and in the precuneus]. Moreover, we observed activations in motor regions [precentral gyrus and supplementary motor area (SMA)], a result that suggests a specific role of these areas (particularly the SMA) in language processing. Functional analysis have also shown that certain brain regions, including the posterior cingulate cortex and the anterior cingulate cortex, have consistently greater activity during resting states compared to states of performing cognitive tasks. In our study, we observed diffuse NBR at the cortical level and a stronger negative response in correspondence to the main sinuses. These phenomena seem to be unrelated to a specific neural activity, appearing to be expressions of a mechanical variation in hemodynamics. We discussed about the importance of these responses that are anticorrelated with the stimulus. Our data suggest that particular care must be considered in the interpretation of fMRI findings, especially in the case of presurgical studies."
}

@article {PMID:15528098,
	Title = {Semantic processing of Chinese in left inferior prefrontal cortex studied with reversible words},
	Author = {Zhang, John X and Zhuang, Jie and Ma, Lifei and Yu, Wei and Peng, Danling and Ding, Guosheng and Zhang, Zhaoqi and Weng, Xuchu},
	DOI = {10.1016/j.neuroimage.2004.07.008},
	Number = {3},
	Volume = {23},
	Month = {November},
	Year = {2004},
	Journal = {NeuroImage},
	ISSN = {1053-8119},
	Pages = {975—982},
	URL = {https://doi.org/10.1016/j.neuroimage.2004.07.008},
}

@article{NEWMAN201051,
title = "The effect of semantic relatedness on syntactic analysis: An fMRI study",
journal = "Brain and Language",
volume = "113",
number = "2",
pages = "51 - 58",
year = "2010",
issn = "0093-934X",
doi = "https://doi.org/10.1016/j.bandl.2010.02.001",
url = "http://www.sciencedirect.com/science/article/pii/S0093934X10000453",
author = "Sharlene D. Newman and Toshikazu Ikuta and Thomas Burns",
keywords = "Semantics, Syntax",
abstract = "The sentences we process in normal conversation tend to refer to information that we are familiar with rather than abstract, unrelated information. This allows for the use of knowledge stores to help facilitate comprehension processes. In many sentence comprehension studies, the stimuli are designed such that the use of world knowledge is limited. Here, we investigated how the semantic relatedness of sentence constituents influences sentence processing. A three factor design was employed in which processing phase (sentence vs. probe), syntactic complexity (object-relative vs. conjoined active) and the semantic relatedness of the nouns within the sentence was examined. We found a differential effect in two sub-regions of the left inferior frontal gyrus (LIFG). BA 44 revealed an effect of syntactic complexity while inferior portions of the LIFG (BA 47) revealed an effect of relatedness as well as an interaction between complexity and relatedness during the probe phase. In addition, significant differences in activation were observed when comparing the sentence processing and probe phases with the sentence phase eliciting stronger semantic related activation while the probe phase elicited stronger working memory related activation."
}

@article {Lee3942,
	author = {Lee, Yune-Sang and Turkeltaub, Peter and Granger, Richard and Raizada, Rajeev D. S.},
	title = {Categorical Speech Processing in Broca{\textquoteright}s Area: An fMRI Study Using Multivariate Pattern-Based Analysis},
	volume = {32},
	number = {11},
	pages = {3942--3948},
	year = {2012},
	doi = {10.1523/JNEUROSCI.3814-11.2012},
	publisher = {Society for Neuroscience},
	abstract = {Although much effort has been directed toward understanding the neural basis of speech processing, the neural processes involved in the categorical perception of speech have been relatively less studied, and many questions remain open. In this functional magnetic resonance imaging (fMRI) study, we probed the cortical regions mediating categorical speech perception using an advanced brain-mapping technique, whole-brain multivariate pattern-based analysis (MVPA). Normal healthy human subjects (native English speakers) were scanned while they listened to 10 consonant{\textendash}vowel syllables along the /ba/{\textendash}/da/ continuum. Outside of the scanner, individuals{\textquoteright} own category boundaries were measured to divide the fMRI data into /ba/ and /da/ conditions per subject. The whole-brain MVPA revealed that Broca{\textquoteright}s area and the left pre-supplementary motor area evoked distinct neural activity patterns between the two perceptual categories (/ba/ vs /da/). Broca{\textquoteright}s area was also found when the same analysis was applied to another dataset (Raizada and Poldrack, 2007), which previously yielded the supramarginal gyrus using a univariate adaptation{\textendash}fMRI paradigm. The consistent MVPA findings from two independent datasets strongly indicate that Broca{\textquoteright}s area participates in categorical speech perception, with a possible role of translating speech signals into articulatory codes. The difference in results between univariate and multivariate pattern-based analyses of the same data suggest that processes in different cortical areas along the dorsal speech perception stream are distributed on different spatial scales.},
	issn = {0270-6474},
	URL = {https://www.jneurosci.org/content/32/11/3942},
	eprint = {https://www.jneurosci.org/content/32/11/3942.full.pdf},
	journal = {Journal of Neuroscience}
}

@article {PMID:27381836,
	Title = {Roles of Supplementary Motor Areas in Auditory Processing and Auditory Imagery},
	Author = {Lima, César F and Krishnan, Saloni and Scott, Sophie K},
	DOI = {10.1016/j.tins.2016.06.003},
	Number = {8},
	Volume = {39},
	Month = {August},
	Year = {2016},
	Journal = {Trends in neurosciences},
	ISSN = {0166-2236},
	Pages = {527—542},
	URL = {http://europepmc.org/articles/PMC5441995},
}

@article{HEIM2003285,
title = "Phonological processing during language production: fMRI evidence for a shared production-comprehension network",
journal = "Cognitive Brain Research",
volume = "16",
number = "2",
pages = "285 - 296",
year = "2003",
issn = "0926-6410",
doi = "https://doi.org/10.1016/S0926-6410(02)00284-7",
url = "http://www.sciencedirect.com/science/article/pii/S0926641002002847",
author = "St. Heim and B. Opitz and K. Müller and A.D. Friederici",
keywords = "Broca’s area, Language production, Language comprehension, Phonology, Inferior frontal gyrus (IFG), Production-perception network, fMRI",
abstract = "Studies of phonological processes during language comprehension consistently report activation of the superior portion of Broca’s area. In the domain of language production, however, there is no unequivocal evidence for the contribution of Broca’s area to phonological processing. The present event-related fMRI study investigated the existence of a common neural network for phonological decisions in comprehension and production by using production tasks most comparable to those previously used in comprehension. Subjects performed two decision tasks on the initial phoneme of German picture names (/b/ or not? Vowel or not?). A semantic decision task served as a baseline for both phonological tasks. The contrasts between each phonological task and the semantic task were calculated, and a conjunction analysis was performed. There was significant activation in the superior portion of Broca’s area (Brodmann’s area (BA) 44) in the conjunction analysis, also present in each single contrast. In addition, further left frontal (BA 45/46) and temporal (posterior superior temporal gyrus) areas known to support phonological processing in both production and comprehension were activated. The results suggest the existence of a shared fronto-temporal neural network engaged in the processing of phonological information in both perception and production."
}

@article {PMID:18296070,
	Title = {Specialisation in Broca's region for semantic, phonological, and syntactic fluency?},
	Author = {Heim, Stefan and Eickhoff, Simon B and Amunts, Katrin},
	DOI = {10.1016/j.neuroimage.2008.01.009},
	Number = {3},
	Volume = {40},
	Month = {April},
	Year = {2008},
	Journal = {NeuroImage},
	ISSN = {1053-8119},
	Pages = {1362—1368},
	URL = {https://doi.org/10.1016/j.neuroimage.2008.01.009},
}

@article{AMUNTS200442,
title = "Analysis of neural mechanisms underlying verbal fluency in cytoarchitectonically defined stereotaxic space—The roles of Brodmann areas 44 and 45",
journal = "NeuroImage",
volume = "22",
number = "1",
pages = "42 - 56",
year = "2004",
issn = "1053-8119",
doi = "https://doi.org/10.1016/j.neuroimage.2003.12.031",
url = "http://www.sciencedirect.com/science/article/pii/S1053811904000059",
author = "Katrin Amunts and Peter H. Weiss and Hartmut Mohlberg and Peter Pieperhoff and Simon Eickhoff and Jennifer M. Gurd and John C. Marshall and Nadim J. Shah and Gereon R. Fink and Karl Zilles",
keywords = "Brodmann's areas, fMRI, Cytoarchitectonic study, Verbal fluency",
abstract = "We investigated neural activations underlying a verbal fluency task and cytoarchitectonic probabilistic maps of Broca's speech region (Brodmann's areas 44 and 45). To do so, we reanalyzed data from a previous functional magnetic resonance imaging (fMRI) [Brain 125 (2002) 1024] and from a cytoarchitectonic study [J. Comp. Neurol. 412 (1999) 319] and developed a method to combine both data sets. In the fMRI experiment, verbal fluency was investigated in 11 healthy volunteers, who covertly produced words from predefined categories. A factorial design was used with factors verbal class (semantic vs. overlearned fluency) and switching between categories (no vs. yes). fMRI data analysis employed SPM99 (Statistical Parametric Mapping). Cytoarchitectonic maps of areas 44 and 45 were derived from histologic sections of 10 postmortem brains. Both the in vivo fMRI and postmortem MR data were warped to a common reference brain using a new elastic warping tool. Cytoarchitectonic probability maps with stereotaxic information about intersubject variability were calculated for both areas and superimposed on the functional data, which showed the involvement of left hemisphere areas with verbal fluency relative to the baseline. Semantic relative to overlearned fluency showed greater involvement of left area 45 than of 44. Thus, although both areas participate in verbal fluency, they do so differentially. Left area 45 is more involved in semantic aspects of language processing, while area 44 is probably involved in high-level aspects of programming speech production per se. The combination of functional data analysis with a new elastic warping tool and cytoarchitectonic maps opens new perspectives for analyzing the cortical networks involved in language."
}

@article{BORNKESSELSCHLESEWSKY200855,
title = "An alternative perspective on “semantic P600” effects in language comprehension",
journal = "Brain Research Reviews",
volume = "59",
number = "1",
pages = "55 - 73",
year = "2008",
issn = "0165-0173",
doi = "https://doi.org/10.1016/j.brainresrev.2008.05.003",
url = "http://www.sciencedirect.com/science/article/pii/S0165017308000519",
author = "Ina Bornkessel-Schlesewsky and Matthias Schlesewsky",
keywords = "Language comprehension, N400, P600, Syntax, Semantics, Semantic reversal anomalies, Semantic P600, Animacy, Plausibility, Extended Argument Dependency Model",
abstract = "The literature on the electrophysiology of language comprehension has recently seen a very prominent discussion of “semantic P600” effects, which have been observed, for example, in sentences involving an implausible thematic role assignment to an argument that would be a highly plausible filler for a different thematic role of the same verb. These findings have sparked a discussion about underlying properties of the language comprehension architecture, as they have generally been viewed as a challenge to established models of language processing and specifically to the notion that syntax precedes semantics in the comprehension process. In this paper, we review the literature on semantic P600 effects and discuss a number of challenges – both conceptual and empirical – to existing approaches in this domain. We then provide a new perspective on these effects by showing how they can be derived within an independently motivated, hierarchically organised neurocognitive model of language comprehension in which syntactic structuring precedes argument interpretation (the extended Argument Dependency Model, eADM; Bornkessel and Schlesewsky, 2006). In addition to straightforwardly deriving the phenomenon of a “semantic P600,” the basic architectural properties of the eADM account for existing empirical puzzles within the semantic P600 literature."
}

@article{doi:10.1111/j.1749-818X.2008.00099.x,
author = {Bornkessel-Schlesewsky, Ina and Schlesewsky, Matthias},
title = {The Role of Prominence Information in the Real-Time Comprehension of Transitive Constructions: A Cross-Linguistic Approach},
journal = {Language and Linguistics Compass},

volume = {3},
number = {1},
pages = {19-58},
doi = {10.1111/j.1749-818X.2008.00099.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1749-818X.2008.00099.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1749-818X.2008.00099.x},
abstract = {Abstract Approaches to language processing have traditionally been formulated with reference to general cognitive concepts (e.g. working memory limitations) or have based their representational assumptions on concepts from generative linguistic theory (e.g. structure determines interpretation). Thus, many well-established generalisations about language that have emerged from cross-linguistic/typological research have not as yet had a major influence in shaping ideas about online processing. Here, we examine the viability of using typologically motivated concepts to account for phenomena in online language comprehension. In particular, we focus on the comprehension of simple transitive sentences (i.e. sentences involving two arguments/event participants) and cross-linguistic similarities and differences in how they are processed. We argue that incremental argument interpretation in these structures is best explained with reference to a range of cross-linguistically motivated, hierarchically ordered information types termed ‘prominence scales’ (e.g. animacy, definiteness/specificity, case marking and linear order). We show that the assumption of prominence-based argument processing can capture a wide range of recent neurocognitive findings, as well as deriving well-known behavioural results.},
year = {2009}
}

@article{FRIEDERICI200278,
title = "Towards a neural basis of auditory sentence processing",
journal = "Trends in Cognitive Sciences",
volume = "6",
number = "2",
pages = "78 - 84",
year = "2002",
issn = "1364-6613",
doi = "https://doi.org/10.1016/S1364-6613(00)01839-8",
url = "http://www.sciencedirect.com/science/article/pii/S1364661300018398",
author = "Angela D. Friederici",
keywords = "syntax, semantics, prosody, Broca's area, neuroimaging",
abstract = "Functional dissociations within the neural basis of auditory sentence processing are difficult to specify because phonological, syntactic and semantic information are all involved when sentences are perceived. In this review I argue that sentence processing is supported by a temporo–frontal network. Within this network, temporal regions subserve aspects of identification and frontal regions the building of syntactic and semantic relations. Temporal analyses of brain activation within this network support syntax-first models because they reveal that building of syntactic structure precedes semantic processes and that these interact only during a later stage."
}

@article{Spruston2008PyramidalND,
  title={Pyramidal neurons: dendritic structure and synaptic integration},
  author={Nelson Spruston},
  journal={Nature Reviews Neuroscience},
  year={2008},
  volume={9},
  pages={206-221}
}

@misc{news_hidden_2018,
	title = {Hidden {Structure} of {Enigmatic} {Backwards} {Neural} {Connections} {Discovered}},
	url = {https://neurosciencenews.com/backwards-neural-connections-8818/},
	abstract = {Summary: Study may provide key insights into the visual system and perception.Source: Champalimaud Center for the Unknown.For decades the neuroscience community has been baffled by the existen},
	urldate = {2019-08-21},
	journal = {Neuroscience News},
	author = {News, Neuroscience},
	month = apr,
	year = {2018},
	file = {Snapshot:/home/dario/Zotero/storage/2ENI3EYP/backwards-neural-connections-8818.html:text/html}
}

@article{marques_functional_2018,
	title = {The functional organization of cortical feedback inputs to primary visual cortex},
	volume = {21},
	copyright = {2018 The Author(s)},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-018-0135-z},
	doi = {10.1038/s41593-018-0135-z},
	abstract = {The authors measured the organization of cortical feedback inputs in mouse primary visual cortex. They found that the locations in visual cortex targeted by feedback axons relate to their tuning properties according to a simple geometrical rule.},
	number = {5},
	urldate = {2019-08-21},
	journal = {Nature Neuroscience},
	author = {Marques, Tiago and Nguyen, Julia and Fioreze, Gabriela and Petreanu, Leopoldo},
	month = may,
	year = {2018},
	pages = {757--764},
	file = {Snapshot:/home/dario/Zotero/storage/ITU68JMN/s41593-018-0135-z.html:text/html}
}

@article{Chen2009ForwardAB,
  title={Forward and backward connections in the brain: A DCM study of functional asymmetries},
  author={Chun-Chuan Chen and Richard N. A. Henson and Klaas Enno Stephan and James Kilner and Karl J. Friston},
  journal={NeuroImage},
  year={2009},
  volume={45},
  pages={453-462}
}

@article{WHITTINGTON2019235,
title = "Theories of Error Back-Propagation in the Brain",
journal = "Trends in Cognitive Sciences",
volume = "23",
number = "3",
pages = "235 - 250",
year = "2019",
issn = "1364-6613",
doi = "https://doi.org/10.1016/j.tics.2018.12.005",
url = "http://www.sciencedirect.com/science/article/pii/S1364661319300129",
author = "James C.R. Whittington and Rafal Bogacz",
keywords = "deep learning, neural networks, predictive coding, synaptic plasticity",
abstract = "This review article summarises recently proposed theories on how neural circuits in the brain could approximate the error back-propagation algorithm used by artificial neural networks. Computational models implementing these theories achieve learning as efficient as artificial neural networks, but they use simple synaptic plasticity rules based on activity of presynaptic and postsynaptic neurons. The models have similarities, such as including both feedforward and feedback connections, allowing information about error to propagate throughout the network. Furthermore, they incorporate experimental evidence on neural connectivity, responses, and plasticity. These models provide insights on how brain networks might be organised such that modification of synaptic weights on multiple levels of cortical hierarchy leads to improved performance on tasks."
}

@article {10.7554/eLife.22901,
article_type = {journal},
title = {Towards deep learning with segregated dendrites},
author = {Guerguiev, Jordan and Lillicrap, Timothy P and Richards, Blake A},
editor = {Latham, Peter},
volume = 6,
year = 2017,
month = {dec},
pub_date = {2017-12-05},
pages = {e22901},
citation = {eLife 2017;6:e22901},
doi = {10.7554/eLife.22901},
url = {https://doi.org/10.7554/eLife.22901},
abstract = {Deep learning has led to significant advances in artificial intelligence, in part, by adopting strategies motivated by neurophysiology. However, it is unclear whether deep learning could occur in the real brain. Here, we show that a deep learning algorithm that utilizes multi-compartment neurons might help us to understand how the neocortex optimizes cost functions. Like neocortical pyramidal neurons, neurons in our model receive sensory information and higher-order feedback in electrotonically segregated compartments. Thanks to this segregation, neurons in different layers of the network can coordinate synaptic weight updates. As a result, the network learns to categorize images better than a single layer network. Furthermore, we show that our algorithm takes advantage of multilayer architectures to identify useful higher-order representations—the hallmark of deep learning. This work demonstrates that deep learning can be achieved using segregated dendritic compartments, which may help to explain the morphology of neocortical pyramidal neurons.},
keywords = {deep learning, dendritic morphology, neocortex, credit assignment, feedback alignment, target propagation},
journal = {eLife},
issn = {2050-084X},
publisher = {eLife Sciences Publications, Ltd},
}

@article{LILLICRAP201982,
title = "Backpropagation through time and the brain",
journal = "Current Opinion in Neurobiology",
volume = "55",
pages = "82 - 89",
year = "2019",
note = "Machine Learning, Big Data, and Neuroscience",
issn = "0959-4388",
doi = "https://doi.org/10.1016/j.conb.2019.01.011",
url = "http://www.sciencedirect.com/science/article/pii/S0959438818302009",
author = "Timothy P Lillicrap and Adam Santoro",
abstract = "It has long been speculated that the backpropagation-of-error algorithm (backprop) may be a model of how the brain learns. Backpropagation-through-time (BPTT) is the canonical temporal-analogue to backprop used to assign credit in recurrent neural networks in machine learning, but there's even less conviction about whether BPTT has anything to do with the brain. Even in machine learning the use of BPTT in classic neural network architectures has proven insufficient for some challenging temporal credit assignment (TCA) problems that we know the brain is capable of solving. Nonetheless, recent work in machine learning has made progress in solving difficult TCA problems by employing novel memory-based and attention-based architectures and algorithms, some of which are brain inspired. Importantly, these recent machine learning methods have been developed in the context of, and with reference to BPTT, and thus serve to strengthen BPTT's position as a useful normative guide for thinking about temporal credit assignment in artificial and biological systems alike."
}

@article {Nasreaav7903,
	author = {Nasr, Khaled and Viswanathan, Pooja and Nieder, Andreas},
	title = {Number detectors spontaneously emerge in a deep neural network designed for visual object recognition},
	volume = {5},
	number = {5},
	elocation-id = {eaav7903},
	year = {2019},
	doi = {10.1126/sciadv.aav7903},
	publisher = {American Association for the Advancement of Science},
	abstract = {Humans and animals have a {\textquotedblleft}number sense,{\textquotedblright} an innate capability to intuitively assess the number of visual items in a set, its numerosity. This capability implies that mechanisms to extract numerosity indwell the brain{\textquoteright}s visual system, which is primarily concerned with visual object recognition. Here, we show that network units tuned to abstract numerosity, and therefore reminiscent of real number neurons, spontaneously emerge in a biologically inspired deep neural network that was merely trained on visual object recognition. These numerosity-tuned units underlay the network{\textquoteright}s number discrimination performance that showed all the characteristics of human and animal number discriminations as predicted by the Weber-Fechner law. These findings explain the spontaneous emergence of the number sense based on mechanisms inherent to the visual system.},
	URL = {https://advances.sciencemag.org/content/5/5/eaav7903},
	eprint = {https://advances.sciencemag.org/content/5/5/eaav7903.full.pdf},
	journal = {Science Advances}
}

@inproceedings{Mikolov:2013:DRW:2999792.2999959,
 author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
 title = {Distributed Representations of Words and Phrases and Their Compositionality},
 booktitle = {Proceedings of the 26th International Conference on Neural Information Processing Systems - Volume 2},
 series = {NIPS'13},
 year = {2013},
 location = {Lake Tahoe, Nevada},
 pages = {3111--3119},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=2999792.2999959},
 acmid = {2999959},
 publisher = {Curran Associates Inc.},
 address = {USA},
} 

@InProceedings{mikolov2013linguistic,
author = {Mikolov, Tomas and Yih, Scott Wen-tau and Zweig, Geoffrey},
title = {Linguistic Regularities in Continuous Space Word Representations},
booktitle = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-2013)},
year = {2013},
month = {May},
abstract = {Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, “King - Man + Woman” results in a vector very close to “Queen.” We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40\% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems.},
publisher = {Association for Computational Linguistics},
url = {https://www.microsoft.com/en-us/research/publication/linguistic-regularities-in-continuous-space-word-representations/},
edition = {Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT-2013)},
}


@article{journals/corr/abs-1301-3781,
  added-at = {2013-10-23T23:02:12.000+0200},
  author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  biburl = {https://www.bibsonomy.org/bibtex/29665b85e8756834ac29fcbd2c6ad0837/wool},
  ee = {http://arxiv.org/abs/1301.3781},
  interhash = {e92df552b17e9f952226a893b84ad739},
  intrahash = {9665b85e8756834ac29fcbd2c6ad0837},
  journal = {CoRR},
  keywords = {nlp},
  timestamp = {2013-10-23T23:02:12.000+0200},
  title = {Efficient Estimation of Word Representations in Vector Space},
  url = {http://dblp.uni-trier.de/db/journals/corr/corr1301.html#abs-1301-3781},
  volume = {abs/1301.3781},
  year = 2013
}

@article {Carlo1488,
	author = {Carlo, C. Nikoosh and Stevens, Charles F.},
	title = {Structural uniformity of neocortex, revisited},
	volume = {110},
	number = {4},
	pages = {1488--1493},
	year = {2013},
	doi = {10.1073/pnas.1221398110},
	publisher = {National Academy of Sciences},
	abstract = {The number of neurons under a square millimeter of cortical surface has been reported to be the same across five cortical areas and five species [Rockel et al. (1980) Brain 103(2):221{\textendash}244] despite differences in cortical thickness between the areas. Although the accuracy of this result has been the subject of sharp debate since its publication approximately 30 y ago, the experiments of Rockel et al. have never been directly replicated with modern stereological methods. We have replicated these experiments and confirm the accuracy of the original report. In addition, we have observed that the number of glial cells under a square millimeter of cortical surface depends on cortical thickness, but not on cortical area or species.},
	issn = {0027-8424},
	URL = {https://www.pnas.org/content/110/4/1488},
	eprint = {https://www.pnas.org/content/110/4/1488.full.pdf},
	journal = {Proceedings of the National Academy of Sciences}
}

@article{Antic2010TheDO,
  title={The decade of the dendritic NMDA spike.},
  author={Srdjan D Antic and Wen-liang Zhou and Anna R Moore and Shaina M Short and Katerina D Ikonomu},
  journal={Journal of neuroscience research},
  year={2010},
  volume={88 14},
  pages={
          2991-3001
        }
}


@article{Major2013ActivePO,
  title={Active properties of neocortical pyramidal neuron dendrites.},
  author={Guy Major and Matthew Evan Larkum and Jackie Schiller},
  journal={Annual review of neuroscience},
  year={2013},
  volume={36},
  pages={
          1-24
        }
}

@article{10.1093/cercor/bhh065,
    author = {Larkum, Matthew E. and Senn, Walter and Lüscher, Hans-R.},
    title = "{Top-down Dendritic Input Increases the Gain of Layer 5 Pyramidal Neurons}",
    journal = {Cerebral Cortex},
    volume = {14},
    number = {10},
    pages = {1059-1070},
    year = {2004},
    month = {10},
    abstract = "{The cerebral cortex is organized so that an important component of feedback input from higher to lower cortical areas arrives at the distal apical tufts of pyramidal neurons. Yet, distal inputs are predicted to have much less impact on firing than proximal inputs. Here we show that even weak asynchronous dendritic input to the distal tuft region can significantly increase the gain of layer 5 pyramidal neurons and thereby the output of columns in the primary somatosensory cortex of the rat. Noisy currents injected in ramps at different dendritic locations showed that the initial slope of the frequency–current (f/I) relationship increases with the distance of the current injection from the soma. The increase was due to the interaction of dendritic depolarization with back-propagating APs which activated dendritic calcium conductances. Gain increases were accompanied by a change of firing mode from isolated spikes to bursting where the timing of bursts coded the presence of coincident somatic and dendritic inputs. We propose that this dendritic gain modulation and the timing of bursts may serve to associate top-down and bottom-up input on different time scales.}",
    issn = {1047-3211},
    doi = {10.1093/cercor/bhh065},
    url = {https://doi.org/10.1093/cercor/bhh065},
    eprint = {https://academic.oup.com/cercor/article-pdf/14/10/1059/771961/bhh065.pdf},
}

@article{BANNISTER200595,
title = "Inter- and intra-laminar connections of pyramidal cells in the neocortex",
journal = "Neuroscience Research",
volume = "53",
number = "2",
pages = "95 - 103",
year = "2005",
issn = "0168-0102",
doi = "https://doi.org/10.1016/j.neures.2005.06.019",
url = "http://www.sciencedirect.com/science/article/pii/S0168010205001884",
author = "A. Peter Bannister",
keywords = "Neocortex, Pyramidal cell, Axon, Selective innervation, Synapse, Microcircuitry, Excitatory pathways, Paired recording",
abstract = "The flow of excitation through cortical columns has long since been predicted by studying the axonal projection patterns of excitatory neurones situated within different laminae. In grossly simplified terms and assuming random connectivity, such studies predict that input from the thalamus terminates primarily in layer 4, is relayed ‘forward’ to layer 3, then to layers 5 and 6 from where the modified signal may exit the cortex. Projection patterns also indicate ‘back’ projections from layer 5 to 3 and layer 6 to 4. More recently it has become clear that the interconnections between these layers are not random; forward projections primarily contact specific pyramidal subclasses and intracortical back projections innervate interneurones. This indicates that presynaptic axons or postsynaptic dendrites are capable of selecting their synaptic partners and that this selectivity is layer dependent. For the past decade, we and others have studied pyramidal cell targeting in circuits both within, and between laminae using paired intracellular recordings with biocytin filling and have begun to identify further levels of selectivity through the preferential targeting of electrophysiologically and/or morphologically distinct pyramidal subtypes. Presented here, therefore, is a brief overview of current thinking on the layer and subclass specific connectivity of neocortical principle excitatory cells."
}

@article{THOMSON1998669,
title = "Postsynaptic pyramidal target selection by descending layer III pyramidal axons: dual intracellular recordings and biocytin filling in slices of rat neocortex",
journal = "Neuroscience",
volume = "84",
number = "3",
pages = "669 - 683",
year = "1998",
issn = "0306-4522",
doi = "https://doi.org/10.1016/S0306-4522(97)00557-5",
url = "http://www.sciencedirect.com/science/article/pii/S0306452297005575",
author = "A.M Thomson and A.P Bannister",
keywords = "cortex, synapse, pyramidal cell, axon, EPSP, paired recording",
abstract = "Paired intracellular recordings in slices of adult rat neocortex with biocytin filling of synaptically connected neurons were used to investigate the pyramidal targets, in layer V, of layer III pyramidal axons. The time-course and sensitivity of excitatory postsynaptic potentials to current injected at the soma, and locations of close appositions between presynaptic axons and postsynaptic dendrites, indicated that the majority of contributory synapses were located in layer V. Within a “column” of tissue, radius ≤250μm, the probability that a randomly selected layer III pyramid innervated a layer V pyramid was 1 in 4 if the target cell was a burst firing pyramid with an apical dendritic tuft in layers II/I. If, however, the potential target was a regular spiking pyramid, the probability of connectivity was only 1 in 40, and none of the 13 anatomically identified postsynaptic layer V targets had a slender apical dendrite terminating in layers IV/III. Morphological reconstructions indicated that layer III pyramids select target layer V cells whose apical dendrites pass within 50–100μm of the soma of the presynaptic pyramid in layer III and which have overlapping apical dendritic tufts in the superficial layers. The probability that a layer V cell would innervate a layer III pyramid lying within 250μm of its apical dendrite was much lower (one in 58). Both presynaptic layer III pyramids and their large postsynaptic layer V targets could therefore access similar inputs in layers I/II, while small layer V pyramids could not. One prediction from the present data would be that neither descending layer V inputs to the striatum or thalamus, nor transcallosal connections would be readily activated by longer distance cortico-cortical “feedback” connections that terminated in layers I/II. These could, however, activate corticofugal pathways to the superior colliculus or pons, both directly and via layer III."
}

@article{Cui:2016:COS:3030654.3030660,
 author = {Cui, Yuwei and Ahmad, Subutai and Hawkins, Jeff},
 title = {Continuous Online Sequence Learning with an Unsupervised Neural Network Model},
 journal = {Neural Comput.},
 issue_date = {November 2016},
 volume = {28},
 number = {11},
 month = nov,
 year = {2016},
 issn = {0899-7667},
 pages = {2474--2504},
 numpages = {31},
 url = {https://doi.org/10.1162/NECO_a_00893},
 doi = {10.1162/NECO\_a\_00893},
 acmid = {3030660},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@article {Yoshimura1931,
	author = {Yoshimura, Yumiko and Sato, Hiromichi and Imamura, Kazuyuki and Watanabe, Yasuyoshi},
	title = {Properties of Horizontal and Vertical Inputs to Pyramidal Cells in the Superficial Layers of the Cat Visual Cortex},
	volume = {20},
	number = {5},
	pages = {1931--1940},
	year = {2000},
	doi = {10.1523/JNEUROSCI.20-05-01931.2000},
	publisher = {Society for Neuroscience},
	abstract = {The purpose of this study is to elucidate the integrative input mechanisms of pyramidal cells receiving horizontally projecting axon collaterals (horizontal projection) and vertical input from layer IV. We performed whole-cell recordings from pyramidal cells in layer II/III and focally activated other single pyramidal cells monosynaptically connected via long-distance horizontal (LH) projections (the distance between presynaptic and postsynaptic cells was 350{\textendash}1200 μm) in slice preparations of the kitten primary visual cortex. In addition, presynaptic single fibers in layer IV (vertical input) and/or short-distance horizontal (SH) inputs from neighboring single pyramidal cells (distance within 100 μm) in layer II/III were activated. Unitary EPSPs evoked by the activation of LH and SH connections had smaller amplitude and larger coefficient of variation than those evoked by stimulating the vertical input. Paired-pulse stimulation of the LH and SH inputs caused the depression of the second EPSP, whereas that of vertical inputs caused either facilitation or depression of the second EPSP. The EPSPs evoked by simultaneous activation of LH and vertical inputs summated linearly at the resting membrane potential. However, the EPSPs evoked by stimulation of the two inputs were nonlinearly (supralinearly) summated when the postsynaptic membrane was depolarized to a certain level. Similar EPSP interaction was observed in response to simultaneous activation of the LH and SH inputs.},
	issn = {0270-6474},
	URL = {https://www.jneurosci.org/content/20/5/1931},
	eprint = {https://www.jneurosci.org/content/20/5/1931.full.pdf},
	journal = {Journal of Neuroscience}
}

@article{doi:10.1113/jphysiol.2001.012959,
author = {Feldmeyer, Dirk and Lübke, Joachim and Silver, R. Angus and Sakmann, Bert},
title = {Synaptic connections between layer 4 spiny neurone- layer 2/3 pyramidal cell pairs in juvenile rat barrel cortex: physiology and anatomy of interlaminar signalling within a cortical column},
journal = {The Journal of Physiology},

volume = {538},
number = {3},
pages = {803-822},
doi = {10.1113/jphysiol.2001.012959},
url = {https://physoc.onlinelibrary.wiley.com/doi/abs/10.1113/jphysiol.2001.012959},
eprint = {https://physoc.onlinelibrary.wiley.com/doi/pdf/10.1113/jphysiol.2001.012959},
abstract = {Whole-cell voltage recordings were obtained from 64 synaptically coupled excitatory layer 4 (L4) spiny neurones and L2/3 pyramidal cells in acute slices of the somatosensory cortex (‘barrel’ cortex) of 17- to 23-days-old rats. Single action potentials (APs) in the L4 spiny neurone evoked single unitary EPSPs in the L2/3 pyramidal cell with a peak amplitude of 0.7 ± 0.6 mV. The average latency was 2.1 ± 0.6 ms, the rise time was 0.8 ± 0.3 ms and the decay time constant was 12.7 ± 3.5 ms. The percentage of failures of an AP in a L4 spiny neurone to evoke a unitary EPSP in the L2/3 pyramidal cell was 4.9 ± 8.8 \% and the coefficient of variation (c.v.) of the unitary EPSP amplitude was 0.27 ± 0.13. Both c.v. and percentage of failures decreased with increased average EPSP amplitude. Postsynaptic glutamate receptors (GluRs) in L2/3 pyramidal cells were of the N-methyl-d-aspartate (NMDA) receptor (NMDAR) and the non-NMDAR type. At −60 mV in the presence of extracellular Mg2+ (1 mm), 29 ± 15 \% of the EPSP voltage-time integral was blocked by NMDAR antagonists. In 0 Mg2+, the NMDAR/AMPAR ratio of the EPSC was 0.50 ± 0.29, about half the value obtained for L4 spiny neurone connections. Burst stimulation of L4 spiny neurones showed that EPSPs in L2/3 pyramidal cells depressed over a wide range of frequencies (1–100 s−1). However, at higher frequencies (30 s−1) EPSP summation overcame synaptic depression so that the summed EPSP was larger than the first EPSP amplitude in the train. The number of putative synaptic contacts established by the axonal collaterals of the L4 projection neurone with the target neurone in layer 2/3 varied between 4 and 5, with an average of 4.5 ± 0.5 (n= 13 pairs). Synapses were established on basal dendrites of the pyramidal cell. Their mean geometric distance from the pyramidal cell soma was 67 ± 34 μm (range, 16–196 μm). The results suggest that each connected L4 spiny neurone produces a weak but reliable EPSP in the pyramidal cell. Therefore transmission of signals to layer 2/3 is likely to have a high threshold requiring simultaneous activation of many L4 neurons, implying that L4 spiny neurone to L2/3 pyramidal cell synapses act as a gate for the lateral spread of excitation in layer 2/3.},
year = {2002}
}

@article{harrison_l.m_stochastic_2005,
	title = {Stochastic models of neuronal dynamics},
	volume = {360},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2005.1648},
	doi = {10.1098/rstb.2005.1648},
	abstract = {Cortical activity is the product of interactions among neuronal populations. Macroscopic electrophysiological phenomena are generated by these interactions. In principle, the mechanisms of these interactions afford constraints on biologically plausible models of electrophysiological responses. In other words, the macroscopic features of cortical activity can be modelled in terms of the microscopic behaviour of neurons. An evoked response potential (ERP) is the mean electrical potential measured from an electrode on the scalp, in response to some event. The purpose of this paper is to outline a population density approach to modelling ERPs.We propose a biologically plausible model of neuronal activity that enables the estimation of physiologically meaningful parameters from electrophysiological data. The model encompasses four basic characteristics of neuronal activity and organization: (i) neurons are dynamic units, (ii) driven by stochastic forces, (iii) organized into populations with similar biophysical properties and response characteristics and (iv) multiple populations interact to form functional networks. This leads to a formulation of population dynamics in terms of the Fokker–Planck equation. The solution of this equation is the temporal evolution of a probability density over state-space, representing the distribution of an ensemble of trajectories. Each trajectory corresponds to the changing state of a neuron. Measurements can be modelled by taking expectations over this density, e.g. mean membrane potential, firing rate or energy consumption per neuron. The key motivation behind our approach is that ERPs represent an average response over many neurons. This means it is sufficient to model the probability density over neurons, because this implicitly models their average state. Although the dynamics of each neuron can be highly stochastic, the dynamics of the density is not. This means we can use Bayesian inference and estimation tools that have already been established for deterministic systems. The potential importance of modelling density dynamics (as opposed to more conventional neural mass models) is that they include interactions among the moments of neuronal states (e.g. the mean depolarization may depend on the variance of synaptic currents through nonlinear mechanisms).Here, we formulate a population model, based on biologically informed model-neurons with spike-rate adaptation and synaptic dynamics. Neuronal sub-populations are coupled to form an observation model, with the aim of estimating and making inferences about coupling among sub-populations using real data. We approximate the time-dependent solution of the system using a bi-orthogonal set and first-order perturbation expansion. For didactic purposes, the model is developed first in the context of deterministic input, and then extended to include stochastic effects. The approach is demonstrated using synthetic data, where model parameters are identified using a Bayesian estimation scheme we have described previously.},
	number = {1457},
	urldate = {2019-07-24},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {{Harrison L.M} and {David O} and {Friston K.J}},
	month = may,
	year = {2005},
	pages = {1075--1091}
}


@incollection{poirazi_dendritic_2015,
	address = {New York, NY},
	title = {Dendritic {Computation}},
	isbn = {978-1-4614-6675-8},
	url = {https://doi.org/10.1007/978-1-4614-6675-8_125},
	booktitle = {Encyclopedia of {Computational} {Neuroscience}},
	publisher = {Springer New York},
	author = {Poirazi, Panayiota},
	editor = {Jaeger, Dieter and Jung, Ranu},
	year = {2015},
	doi = {10.1007/978-1-4614-6675-8_125},
	pages = {992--997}
}

@article{PAYEUR201978,
title = "Classes of dendritic information processing",
journal = "Current Opinion in Neurobiology",
volume = "58",
pages = "78 - 85",
year = "2019",
issn = "0959-4388",
doi = "https://doi.org/10.1016/j.conb.2019.07.006",
url = "http://www.sciencedirect.com/science/article/pii/S0959438818302162",
author = "Alexandre Payeur and Jean-Claude Béïque and Richard Naud",
abstract = "Dendrites are much more than passive neuronal components. Mounting experimental evidence and decades of computational work have decisively shown that dendrites leverage a host of nonlinear biophysical phenomena and actively participate in sophisticated computations, at the level of the single neuron and at the level of the network. However, a coherent view of their processing power is still lacking and dendrites are largely neglected in neural network models. Here, we describe four classes of dendritic information processing and delineate their implications at the algorithmic level. We propose that beyond the well-known spatiotemporal filtering of their inputs, dendrites are capable of selecting, routing and multiplexing information. By separating dendritic processing from axonal outputs, neuron networks gain a degree of freedom with implications for perception and learning."
}

@article {Gidon83,
	author = {Gidon, Albert and Zolnik, Timothy Adam and Fidzinski, Pawel and Bolduan, Felix and Papoutsi, Athanasia and Poirazi, Panayiota and Holtkamp, Martin and Vida, Imre and Larkum, Matthew Evan},
	title = {Dendritic action potentials and computation in human layer 2/3 cortical neurons},
	volume = {367},
	number = {6473},
	pages = {83--87},
	year = {2020},
	doi = {10.1126/science.aax6239},
	publisher = {American Association for the Advancement of Science},
	abstract = {A special developmental program in the human brain drives the disproportionate thickening of cortical layer 2/3. This suggests that the expansion of layer 2/3, along with its numerous neurons and their large dendrites, may contribute to what makes us human. Gidon et al. thus investigated the dendritic physiology of layer 2/3 pyramidal neurons in slices taken from surgically resected brain tissue in epilepsy patients. Dual somatodendritic recordings revealed previously unknown classes of action potentials in the dendrites of these neurons, which make their activity far more complex than has been previously thought. These action potentials allow single neurons to solve two long-standing computational problems in neuroscience that were considered to require multilayer neural networks.Science, this issue p. 83The active electrical properties of dendrites shape neuronal input and output and are fundamental to brain function. However, our knowledge of active dendrites has been almost entirely acquired from studies of rodents. In this work, we investigated the dendrites of layer 2 and 3 (L2/3) pyramidal neurons of the human cerebral cortex ex vivo. In these neurons, we discovered a class of calcium-mediated dendritic action potentials (dCaAPs) whose waveform and effects on neuronal output have not been previously described. In contrast to typical all-or-none action potentials, dCaAPs were graded; their amplitudes were maximal for threshold-level stimuli but dampened for stronger stimuli. These dCaAPs enabled the dendrites of individual human neocortical pyramidal neurons to classify linearly nonseparable inputs{\textemdash}a computation conventionally thought to require multilayered networks.},
	issn = {0036-8075},
	URL = {https://science.sciencemag.org/content/367/6473/83},
	eprint = {https://science.sciencemag.org/content/367/6473/83.full.pdf},
	journal = {Science}
}

@article{barth_experimental_2012,
	title = {Experimental evidence for sparse firing in the neocortex},
	volume = {35},
	issn = {0166-2236},
	url = {http://www.sciencedirect.com/science/article/pii/S0166223612000513},
	doi = {https://doi.org/10.1016/j.tins.2012.03.008},
	abstract = {The advent of unbiased recording and imaging techniques to evaluate firing activity across neocortical neurons has revealed substantial heterogeneity in response properties in vivo, and that a minority of neurons are responsible for the majority of spikes. Despite the computational advantages to sparsely firing populations, experimental data defining the fraction of responsive neurons and the range of firing rates have not been synthesized. Here we review data about the distribution of activity across neuronal populations in primary sensory cortex. Overall, the firing output of granular and infragranular layers is highest. Although subthreshold activity across supragranular neurons is decidedly non-sparse, spikes are much less frequent and some cells are silent. Superficial layers of the cortex may employ specific cell and circuit mechanisms to increase sparseness.},
	number = {6},
	journal = {Trends in Neurosciences},
	author = {Barth, Alison L. and Poulet, James F. A.},
	year = {2012},
	keywords = {coding, kurtosis, optimality, pyramidal neuron, silent neurons},
	pages = {345 -- 355}
}

@misc{noauthor_google_nodate,
    author = {Google},
	title = {Google {Code} {Archive} - {Long}-term storage for {Google} {Code} {Project} {Hosting}.},
	year = {2019},
	url = {https://code.google.com/archive/p/word2vec/},
	urldate = {2019-07-27},
	file = {Google Code Archive - Long-term storage for Google Code Project Hosting.:/home/dario/Zotero/storage/GI6DYEGQ/word2vec.html:text/html}
}

@ARTICLE{10.3389/neuro.01.1.1.002.2007,
  
AUTHOR={Thomson, Alex and Lamy, Christophe},   
	 
TITLE={Functional maps of neocortical local circuitry},      
	
JOURNAL={Frontiers in Neuroscience},      
	
VOLUME={1},      

PAGES={2},     
	
YEAR={2007},      
	  
URL={https://www.frontiersin.org/article/10.3389/neuro.01.1.1.002.2007},       
	
DOI={10.3389/neuro.01.1.1.002.2007},      
	
ISSN={1662-453X}   
   
}

@article{Lbke2007ExcitatorySF,
  title={Excitatory signal flow and connectivity in a cortical column: focus on barrel cortex},
  author={Joachim L{\"u}bke and Dirk Feldmeyer},
  journal={Brain Structure and Function},
  year={2007},
  volume={212},
  pages={3-17}
}

@article{doi:10.1177/1073858407305201,
author = {Jay Hegdé and Daniel J. Felleman},
title ={Reappraising the Functional Implications of the Primate Visual Anatomical Hierarchy},
journal = {The Neuroscientist},
volume = {13},
number = {5},
pages = {416-421},
year = {2007},
doi = {10.1177/1073858407305201},
    note ={PMID: 17901251},

URL = { 
        https://doi.org/10.1177/1073858407305201
    
},
eprint = { 
        https://doi.org/10.1177/1073858407305201
    
}
,
    abstract = { The primate visual system has been shown to be organized into an anatomical hierarchy by the application of a few principled criteria. It has been widely assumed that cortical visual processing is also hierarchical, with the anatomical hierarchy providing a defined substrate for clear levels of hierarchical function. A large body of empirical evidence seemed to support this assumption, including the general observations that functional properties of visual neurons grow progressively more complex at progressively higher levels of the anatomical hierarchy. However, a growing body of evidence, including recent direct experimental comparisons of functional properties at two or more levels of the anatomical hierarchy, indicates that visual processing neither is hierarchical nor parallels the anatomical hierarchy. Recent results also indicate that some of the pathways of visual information flow are not hierarchical, so that the anatomical hierarchy cannot be taken as a strict flowchart of visual information either. Thus, while the sustaining strength of the notion of hierarchical processing may be that it is rather simple, its fatal flaw is that it is overly simplistic. NEUROSCIENTIST 13(5):416—421, 2007. DOI: 10.1177/1073858407305201 }
}

@incollection{Kohonen:1988:SFT:65669.104428,
 author = {Kohonen, Teuvo},
 chapter = {Self-organized Formation of Topologically Correct Feature Maps},
 title = {Neurocomputing: Foundations of Research},
 editor = {Anderson, James A. and Rosenfeld, Edward},
 year = {1988},
 isbn = {0-262-01097-6},
 pages = {509--521},
 numpages = {13},
 url = {http://dl.acm.org/citation.cfm?id=65669.104428},
 acmid = {104428},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@inproceedings{Kohonen1989SelforganizationAA,
  title={Self-organization and associative memory: 3rd edition},
  author={Teuvo Kohonen},
  year={1989}
}

@Article{Haueis2016,
author="Haueis, Philipp",
title="The life of the cortical column: opening the domain of functional architecture of the cortex (1955--1981)",
journal="History and Philosophy of the Life Sciences",
year="2016",
month="Jun",
day="17",
volume="38",
number="3",
pages="2",
abstract="The concept of the cortical column refers to vertical cell bands with similar response properties, which were initially observed by Vernon Mountcastle's mapping of single cell recordings in the cat somatic cortex. It has subsequently guided over 50 years of neuroscientific research, in which fundamental questions about the modularity of the cortex and basic principles of sensory information processing were empirically investigated. Nevertheless, the status of the column remains controversial today, as skeptical commentators proclaim that the vertical cell bands are a functionally insignificant by-product of ontogenetic development. This paper inquires how the column came to be viewed as an elementary unit of the cortex from Mountcastle's discovery in 1955 until David Hubel and Torsten Wiesel's reception of the Nobel Prize in 1981. I first argue that Mountcastle's vertical electrode recordings served as criteria for applying the column concept to electrophysiological data. In contrast to previous authors, I claim that this move from electrophysiological data to the phenomenon of columnar responses was concept-laden, but not theory-laden. In the second part of the paper, I argue that Mountcastle's criteria provided Hubel Wiesel with a conceptual outlook, i.e. it allowed them to anticipate columnar patterns in the cat and macaque visual cortex. I argue that in the late 1970s, this outlook only briefly took a form that one could call a `theory' of the cerebral cortex, before new experimental techniques started to diversify column research. I end by showing how this account of early column research fits into a larger project that follows the conceptual development of the column into the present.",
issn="1742-6316",
doi="10.1007/s40656-016-0103-4",
url="https://doi.org/10.1007/s40656-016-0103-4"
}

@article{doi:10.1002/cne.901580305,
author = {Hubel, David H. and Wiesel, Torsten N.},
title = {Uniformity of monkey striate cortex: A parallel relationship between field size, scatter, and magnification factor},
journal = {Journal of Comparative Neurology},

volume = {158},
number = {3},
pages = {295-305},
doi = {10.1002/cne.901580305},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cne.901580305},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/cne.901580305},
abstract = {Abstract This paper is concerned with the relationship between orientation columns, ocular-dominance columns, the topographic mapping of visual fields onto cortex, and receptive-field size and scatter. Although the orientation columns are an order of magnitude smaller than the ocular-dominance columns, the horizontal distance corresponding to a complete cycle of orientation columns, representing a rotation through 180°, seems to be roughly the same size as a left-plus-right ocular dominance set, with a thickness of about 0.5–1 mm, independent of eccentricity at least out to 15°. We use the term hypercolumn to refer to a complete set of either type (180°, or left-plus-right eyes). In the macaque monkey several penetrations were made at various eccentricities in various parts of the striate cortex subserving the fovea, parafovea and midperiphery. As observed many times previously, in any vertical penetration there was an apparently random scatter in receptive-field positions, which was of the same order of magnitude as the individual receptive fields in that part of the cortex; the field size and the scatter increased in parallel fashion with eccentricity. The movement through the visual field corresponding to a 1 mm horizontal movement along the cortex (the reciprocal of the magnification factor) also increased with eccentricity, in a manner that was strikingly parallel with the increase in receptive field size and scatter. In parts of the cortex representing retina, at least out to about 22° from the fovea, a movement along the cortical surface of about 1 mm was enough to displace the fields so that the new position they collectively occupied half overlapped the old. Such an overlap was thus produced by moving along the cortex a distance about equal to the thickness of a left-plus-right set of ocular-dominance columns, or a complete 180° array of orientation columns. It therefore seems that, independent of eccentricity, a 2 mm × 2 mm block of cortex contains by a comfortable margin the machinery needed to analyze a region of visual field roughly equal to the local field size plus scatter. A movement of 2–3 mm corresponds to a new visual field region and to several new sets of hypercolumns. The cortex thus seems remarkably uniform physiologically, just as it is anatomically.},
year = {1974}
}

@article{doi:10.1152/jn.1957.20.4.408,
author = {Mountcastle, Vernon B.},
title = {MODALITY AND TOPOGRAPHIC PROPERTIES OF SINGLE NEURONS OF CAT'S SOMATIC SENSORY CORTEX},
journal = {Journal of Neurophysiology},
volume = {20},
number = {4},
pages = {408-434},
year = {1957},
doi = {10.1152/jn.1957.20.4.408},
    note ={PMID: 13439410},

URL = { 
        https://doi.org/10.1152/jn.1957.20.4.408
    
},
eprint = { 
        https://doi.org/10.1152/jn.1957.20.4.408
    
}

}

@inproceedings{Mountcastle1978AnOP,
  title={An organizing principle for cerebral function : the unit module and the distributed system},
  author={Vernon B. Mountcastle},
  year={1978}
}

@article{10.1093/brain/120.4.701,
    author = {Mountcastle, V B},
    title = "{The columnar organization of the neocortex.}",
    journal = {Brain},
    volume = {120},
    number = {4},
    pages = {701-722},
    year = {1997},
    month = {04},
    abstract = "{The modular organization of nervous systems is a widely documented principle of design for both vertebrate and invertebrate brains of which the columnar organization of the neocortex is an example. The classical cytoarchitectural areas of the neocortex are composed of smaller units, local neural circuits repeated iteratively within each area. Modules may vary in cell type and number, in internal and external connectivity, and in mode of neuronal processing between different large entities; within any single large entity they have a basic similarity of internal design and operation. Modules are most commonly grouped into entities by sets of dominating external connections. This unifying factor is most obvious for the heterotypical sensory and motor areas of the neocortex. Columnar defining factors in homotypical areas are generated, in part, within the cortex itself. The set of all modules composing such an entity may be fractionated into different modular subsets by different extrinsic connections. Linkages between them and subsets in other large entities form distributed systems. The neighborhood relations between connected subsets of modules in different entities result in nested distributed systems that serve distributed functions. A cortical area defined in classical cytoarchitectural terms may belong to more than one and sometimes to several distributed systems. Columns in cytoarchitectural areas located at some distance from one another, but with some common properties, may be linked by long-range, intracortical connections.}",
    issn = {0006-8950},
    doi = {10.1093/brain/120.4.701},
    url = {https://doi.org/10.1093/brain/120.4.701},
    eprint = {https://academic.oup.com/brain/article-pdf/120/4/701/17863573/1200701.pdf},
}

@ARTICLE{557663,
author={T. {Villmann} and R. {Der} and M. {Herrmann} and T. M. {Martinetz}},
journal={IEEE Transactions on Neural Networks},
title={Topology preservation in self-organizing feature maps: exact definition and measurement},
year={1997},
volume={8},
number={2},
pages={256-266},
keywords={self-organising feature maps;topology;topology preservation;self-organizing feature maps;neighborhood preservation;Kohonen map;dimensional conflict;qualitative approaches;quantitative approaches;synaptic weight vectors;nonlinear data manifolds;induced receptive fields;topographic function;Lattices;Network topology;Vectors;Neural networks;Information processing;Speech processing;Image processing;Robots;Informatics;Research and development},
doi={10.1109/72.557663},
ISSN={1045-9227},
month={March},}

@article{key:article,
	author = {Chelsea Boling and Kumer Das},
	title = {Article: Reducing Dimensionality of Text Documents using Latent Semantic Analysis},
	journal = {International Journal of Computer Applications},
	year = {2015},
	volume = {112},
	number = {5},
	pages = {9-12},
	month = {February},
	note = {Full text available}
}

@article{huth_natural_2016,
	title = {Natural speech reveals the semantic maps that tile human cerebral cortex},
	volume = {532},
	url = {https://doi.org/10.1038/nature17637},
	journal = {Nature},
	author = {Huth, Alexander G. and de Heer, Wendy A. and Griffiths, Thomas L. and Theunissen, Frédéric E. and Gallant, Jack L.},
	month = apr,
	year = {2016},
	pages = {453}
}



@article{DBLP:journals/corr/Webber15,
  author    = {Francisco De Sousa Webber},
  title     = {Semantic Folding Theory And its Application in Semantic Fingerprinting},
  journal   = {CoRR},
  volume    = {abs/1511.08855},
  year      = {2015},
  url       = {http://arxiv.org/abs/1511.08855},
  archivePrefix = {arXiv},
  eprint    = {1511.08855},
  timestamp = {Mon, 13 Aug 2018 16:46:10 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/Webber15},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{pub.1005704802,
 author = {Huth, Alexander G. and Nishimoto, Shinji and Vu, An T. and Gallant, Jack L.},
 doi = {10.1016/j.neuron.2012.10.014},
 journal = {Neuron},
 keywords = {},
 number = {6},
 pages = {1210-1224},
 title = {A Continuous Semantic Space Describes the Representation of Thousands of Object and Action Categories across the Human Brain},
 url = {https://app.dimensions.ai/details/publication/pub.1005704802 and https://doi.org/10.1016/j.neuron.2012.10.014},
 volume = {76},
 year = {2012}
}

@ARTICLE{10.3389/fncir.2018.00121,
AUTHOR={Hawkins, Jeff and Lewis, Marcus and Klukas, Mirko and Purdy, Scott and Ahmad, Subutai},   
TITLE={A Framework for Intelligence and Cortical Function Based on Grid Cells in the Neocortex},      
JOURNAL={Frontiers in Neural Circuits},      
VOLUME={12},      
PAGES={121},     
YEAR={2019},      
URL={https://www.frontiersin.org/article/10.3389/fncir.2018.00121},       
DOI={10.3389/fncir.2018.00121},      
ISSN={1662-5110},   
ABSTRACT={How the neocortex works is a mystery. In this paper we propose a novel framework for understanding its function. Grid cells are neurons in the entorhinal cortex that represent the location of an animal in its environment. Recent evidence suggests that grid cell-like neurons may also be present in the neocortex. We propose that grid cells exist throughout the neocortex, in every region and in every cortical column. They define a location-based framework for how the neocortex functions. Whereas grid cells in the entorhinal cortex represent the location of one thing, the body relative to its environment, we propose that cortical grid cells simultaneously represent the location of many things. Cortical columns in somatosensory cortex track the location of tactile features relative to the object being touched and cortical columns in visual cortex track the location of visual features relative to the object being viewed. We propose that mechanisms in the entorhinal cortex and hippocampus that evolved for learning the structure of environments are now used by the neocortex to learn the structure of objects. Having a representation of location in each cortical column suggests mechanisms for how the neocortex represents object compositionality and object behaviors. It leads to the hypothesis that every part of the neocortex learns complete models of objects and that there are many models of each object distributed throughout the neocortex. The similarity of circuitry observed in all cortical regions is strong evidence that even high-level cognitive tasks are learned and represented in a location-based framework.}
}

@article{Srivastava2014DropoutAS,
  title={Dropout: a simple way to prevent neural networks from overfitting},
  author={Nitish Srivastava and Geoffrey E. Hinton and Alex Krizhevsky and Ilya Sutskever and Ruslan Salakhutdinov},
  journal={J. Mach. Learn. Res.},
  year={2014},
  volume={15},
  pages={1929-1958}
}

@article{10.1371/journal.pcbi.1000532,
    author = {George, Dileep AND Hawkins, Jeff},
    journal = {PLOS Computational Biology},
    publisher = {Public Library of Science},
    title = {Towards a Mathematical Theory of Cortical Micro-circuits},
    year = {2009},
    month = {10},
    volume = {5},
    url = {https://doi.org/10.1371/journal.pcbi.1000532},
    pages = {1-26},
    abstract = {Author Summary Understanding the computational and information processing roles of cortical circuitry is one of the outstanding problems in neuroscience. In this paper, we work from a theory of neocortex that models it as a spatio-temporal hierarchical system to derive a biological cortical circuit. This is achieved by combining the computational constraints provided by the inference equations for this spatio-temporal hierarchy with anatomical data. The result is a mathematically consistent biological circuit that can be mapped to the cortical laminae and matches many prominent features of the mammalian neocortex. The mathematical model can serve as a starting point for the construction of machines that work like the brain. The resultant biological circuit can be used for modeling physiological phenomena and for deriving testable predictions about the brain.},
    number = {10},
    doi = {10.1371/journal.pcbi.1000532}
}

@article{HIRSCH2006377,
title = "Laminar processing in the visual cortical column",
journal = "Current Opinion in Neurobiology",
volume = "16",
number = "4",
pages = "377 - 384",
year = "2006",
note = "Sensory systems",
issn = "0959-4388",
doi = "https://doi.org/10.1016/j.conb.2006.06.014",
url = "http://www.sciencedirect.com/science/article/pii/S0959438806000894",
author = "Judith A Hirsch and Luis M Martinez",
abstract = "Sensory regions of neocortex are organized as arrays of vertical columns composed of cells that share similar response properties, with the orientation columns of the cat's visual cortex being the best known example. Interest in how sensitivity to different stimulus features first emerges in the columns and how this selectivity is refined by subsequent processing has fueled decades of research. A natural starting point in approaching these issues is anatomy. Each column traverses the six cortical layers and each layer has a unique pattern of inputs, intrinsic connections and outputs. Thus, it makes sense to explore the possibility of corresponding laminar differences in sensory function, that is, to examine relationships between morphology and physiology. In addition, to help identify general patterns of cortical organization, it is useful to compare results obtained from different sensory systems and diverse species. The picture that emerges from such comparisons is that each cortical layer serves a distinct role in sensory function. Furthermore, different cortices appear to share some common strategies for processing information but also have specialized mechanisms adapted for the demands of specific sensory tasks."
}

@article{10.1093/cercor/13.1.15,
    author = {Lund, Jennifer S. and Angelucci, Alessandra and Bressloff, Paul C.},
    title = "{Anatomical Substrates for Functional Columns in Macaque Monkey Primary Visual Cortex}",
    journal = {Cerebral Cortex},
    volume = {13},
    number = {1},
    pages = {15-24},
    year = {2003},
    month = {01},
    abstract = "{In this review we re-examine the concept of a cortical column in macaque primary visual cortex, and consider to what extent a functionally defined column reflects any sort of anatomical entity that subdivides cortical territory. Functional studies have shown that columns relating to different response properties are mapped in cortex at different spatial scales. We suggest that these properties first emerge in mid-layer 4C through a combination of thalamic afferent inputs and local intracortical circuitry, and are then transferred to other layers in a columnar fashion, via interlaminar relays, where additional processing occurs. However, several properties are not strictly columnar since they do not appear in all cortical layers. In contrast to the functional column, an anatomically based cortical column is defined most clearly in terms of the reciprocal connections it makes, both via intra-areal lateral connections and inter-areal feedback/feedforward pathways. The column boundaries are reinforced by interplay between lateral inhibition spreading beyond the column boundary and disinhibition within the column. The anatomical column acts as a functionally tuned unit and point of information collation from laterally offset regions and feedback pathways. Thalamic inputs provide the highcontrast receptive field sizes of the column’s neurons, intra-areal lateral connections provide their low contrast summation field sizes, and feedback pathways provide surround modulation of receptive fields responses.}",
    issn = {1047-3211},
    doi = {10.1093/cercor/13.1.15},
    url = {https://doi.org/10.1093/cercor/13.1.15},
    eprint = {https://academic.oup.com/cercor/article-pdf/13/1/15/9752435/1300015.pdf},
}

@inproceedings{Shi1995PerceptualCO,
  title={Perceptual correlates of content words and function words in early language input},
  author={Rushen Shi},
  year={1995}
}

@article{shi_morgan_allopenna_1998, title={Phonological and acoustic bases for earliest grammatical category assignment: a cross-linguistic perspective}, volume={25}, DOI={10.1017/S0305000997003395}, number={1}, journal={Journal of Child Language}, publisher={Cambridge University Press}, author={SHI, RUSHEN and MORGAN, JAMES L. and ALLOPENNA, PAUL}, year={1998}, pages={169–201}}

@article{shi_newborn_1999,
	title = {Newborn infants’ sensitivity to perceptual cues to lexical and grammatical words},
	volume = {72},
	issn = {0010-0277},
	url = {http://www.sciencedirect.com/science/article/pii/S0010027799000475},
	doi = {https://doi.org/10.1016/S0010-0277(99)00047-5},
	abstract = {In our study newborn infants were presented with lists of lexical and grammatical words prepared from natural maternal speech. The results show that newborns are able to categorically discriminate these sets of words based on a constellation of perceptual cues that distinguish them. This general ability to detect and categorically discriminate sets of words on the basis of multiple acoustic and phonological cues may provide a perceptual base that can help older infants bootstrap into the acquisition of grammatical categories and syntactic structure.},
	number = {2},
	journal = {Cognition},
	author = {Shi, Rushen and Werker, Janet F. and Morgan, James L.},
	year = {1999},
	keywords = {Infancy, Language acquisition, Lexical and grammatical words, Speech perception, Syntactic categories},
	pages = {B11 -- B21}
}

@inproceedings{Yusuke:2002:MEE:1289189.1289214,
 author = {Yusuke, Miyao and Jun'ichi, Tsujii},
 title = {Maximum Entropy Estimation for Feature Forests},
 booktitle = {Proceedings of the Second International Conference on Human Language Technology Research},
 series = {HLT '02},
 year = {2002},
 location = {San Diego, California},
 pages = {292--297},
 numpages = {6},
 url = {http://dl.acm.org/citation.cfm?id=1289189.1289214},
 acmid = {1289214},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
} 

@misc{noauthor_2_nodate,
    author = {Yusuke Miyao},
	title = {(2) {Probabilistic} modeling of argument structures including non-local dependencies {\textbar} {Yusuke} {Miyao}},
	url = {https://www.researchgate.net/publication/239725877_Probabilistic_modeling_of_argument_structures_including_non-local_dependencies},
	abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
	year = {2002},
	urldate = {2019-07-31},
	journal = {ResearchGate}
}

@inproceedings{Miyao2004CorpusOrientedGD,
  title={Corpus-Oriented Grammar Development for Acquiring a Head-Driven Phrase Structure Grammar from the Penn Treebank},
  author={Yusuke Miyao and Takashi Ninomiya and Jun'ichi Tsujii},
  booktitle={IJCNLP},
  year={2004}
}

@inproceedings{Miyao:2005:PDM:1219840.1219851,
 author = {Miyao, Yusuke and Tsujii, Jun'ichi},
 title = {Probabilistic Disambiguation Models for Wide-coverage HPSG Parsing},
 booktitle = {Proceedings of the 43rd Annual Meeting on Association for Computational Linguistics},
 series = {ACL '05},
 year = {2005},
 location = {Ann Arbor, Michigan},
 pages = {83--90},
 numpages = {8},
 url = {https://doi.org/10.3115/1219840.1219851},
 doi = {10.3115/1219840.1219851},
 acmid = {1219851},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
} 

@inproceedings{Ninomiya:2006:ELM:1610075.1610100,
 author = {Ninomiya, Takashi and Matsuzaki, Takuya and Tsuruoka, Yoshimasa and Miyao, Yusuke and Tsujii, Jun'ichi},
 title = {Extremely Lexicalized Models for Accurate and Fast HPSG Parsing},
 booktitle = {Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing},
 series = {EMNLP '06},
 year = {2006},
 isbn = {1-932432-73-6},
 location = {Sydney, Australia},
 pages = {155--163},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=1610075.1610100},
 acmid = {1610100},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
}

@inproceedings{Ninomiya:2007:LMN:1621410.1621418,
 author = {Ninomiya, Takashi and Matsuzaki, Takuya and Miyao, Yusuke and Tsujii, Jun'ichi},
 title = {A Log-linear Model with an N-gram Reference Distribution for Accurate HPSG Parsing},
 booktitle = {Proceedings of the 10th International Conference on Parsing Technologies},
 series = {IWPT '07},
 year = {2007},
 isbn = {978-1-932432-90-9},
 location = {Prague, Czech REpublic},
 pages = {60--68},
 numpages = {9},
 url = {http://dl.acm.org/citation.cfm?id=1621410.1621418},
 acmid = {1621418},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
}

@article{Miyao:2008:FFM:1350986.1350988,
 author = {Miyao, Yusuke and Tsujii, Jun'ichi},
 title = {Feature Forest Models for Probabilistic Hpsg Parsing},
 journal = {Comput. Linguist.},
 issue_date = {March 2008},
 volume = {34},
 number = {1},
 month = mar,
 year = {2008},
 issn = {0891-2017},
 pages = {35--80},
 numpages = {46},
 url = {http://dx.doi.org/10.1162/coli.2008.34.1.35},
 doi = {10.1162/coli.2008.34.1.35},
 acmid = {1350988},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
}

@INPROCEEDINGS{tsuruoka:2004b,
     author = {Tsuruoka, Y. and Miyao, Y. and Tsujii, J.},
      title = {Towards efficient probabilistic HPSG parsing: integrating semantic and syntactic preference to guide the parsing},
  booktitle = {Proceedings of IJCNLP-04 Workshop: Beyond shallow analyses - Formalisms and statistical modeling for deep analyses},
       year = {2004},
    address = {Hainan Island, China}
}

@inproceedings{Ninomiya:2005:EBT:1654494.1654505,
 author = {Ninomiya, Takashi and Tsuruoka, Yoshimasa and Miyao, Yusuke and Tsujii, Jun'ichi},
 title = {Efficacy of Beam Thresholding, Unification Filtering and Hybrid Parsing in Probabilistic HPSG Parsing},
 booktitle = {Proceedings of the Ninth International Workshop on Parsing Technology},
 series = {Parsing '05},
 year = {2005},
 location = {Vancouver, British Columbia, Canada},
 pages = {103--114},
 numpages = {12},
 url = {http://dl.acm.org/citation.cfm?id=1654494.1654505},
 acmid = {1654505},
 publisher = {Association for Computational Linguistics},
 address = {Stroudsburg, PA, USA},
}

@article{bc22fe91f8a743269f26f92abfd79790,
title = "Fast and Scalable HPSG Parsing",
author = "T NINOMIYA and Yoshimasa Tsuruoka and Y MIYAO and K TAURA and J TSUJII",
year = "2006",
volume = "46(2)",
journal = "Traitement automatique des langues (TAL)",

}

@inproceedings{Matsuzaki:2007:EHP:1625275.1625546,
 author = {Matsuzaki, Takuya and Miyao, Yusuke and Tsujii, Jun'ichi},
 title = {Efficient HPSG Parsing with Supertagging and CFG-filtering},
 booktitle = {Proceedings of the 20th International Joint Conference on Artifical Intelligence},
 series = {IJCAI'07},
 year = {2007},
 location = {Hyderabad, India},
 pages = {1671--1676},
 numpages = {6},
 url = {http://dl.acm.org/citation.cfm?id=1625275.1625546},
 acmid = {1625546},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = {San Francisco, CA, USA},
}

@misc{noauthor_english_2019,
    author = {Enju},
	title = {English {HPSG} parser},
	copyright = {View license},
	url = {https://github.com/mynlp/enju},
	urldate = {2019-07-31},
	publisher = {mynlp},
	month = jun,
	year = {2019},
	note = {original-date: 2016-09-16T06:56:38Z}
}

@InProceedings{BothaEtAl2018,
  title = {{Learning To Split and Rephrase From Wikipedia Edit History}},
  author = {Botha, Jan A and Faruqui, Manaal and Alex, John and Baldridge, Jason and Das, Dipanjan},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  pages = {to appear},
  note = {arXiv preprint arXiv:1808.09468},
  year = {2018}
}

@article{TOMASELLO2017111,
title = "Brain connections of words, perceptions and actions: A neurobiological model of spatio-temporal semantic activation in the human cortex",
journal = "Neuropsychologia",
volume = "98",
pages = "111 - 129",
year = "2017",
note = "Special Issue: The Neural Basis of Language Learning",
issn = "0028-3932",
doi = "https://doi.org/10.1016/j.neuropsychologia.2016.07.004",
url = "http://www.sciencedirect.com/science/article/pii/S0028393216302421",
author = "Rosario Tomasello and Max Garagnani and Thomas Wennekers and Friedemann Pulvermüller",
keywords = "Word acquisition, Semantic grounding, Hebbian cell assembly, Biologically inspired neural network, Word recognition EEG-MEG responses, Cortical connectivity",
abstract = "Neuroimaging and patient studies show that different areas of cortex respectively specialize for general and selective, or category-specific, semantic processing. Why are there both semantic hubs and category-specificity, and how come that they emerge in different cortical regions? Can the activation time-course of these areas be predicted and explained by brain-like network models? In this present work, we extend a neurocomputational model of human cortical function to simulate the time-course of cortical processes of understanding meaningful concrete words. The model implements frontal and temporal cortical areas for language, perception, and action along with their connectivity. It uses Hebbian learning to semantically ground words in aspects of their referential object- and action-related meaning. Compared with earlier proposals, the present model incorporates additional neuroanatomical links supported by connectivity studies and downscaled synaptic weights in order to control for functional between-area differences purely due to the number of in- or output links of an area. We show that learning of semantic relationships between words and the objects and actions these symbols are used to speak about, leads to the formation of distributed circuits, which all include neuronal material in connector hub areas bridging between sensory and motor cortical systems. Therefore, these connector hub areas acquire a role as semantic hubs. By differentially reaching into motor or visual areas, the cortical distributions of the emergent ‘semantic circuits’ reflect aspects of the represented symbols’ meaning, thus explaining category-specificity. The improved connectivity structure of our model entails a degree of category-specificity even in the ‘semantic hubs’ of the model. The relative time-course of activation of these areas is typically fast and near-simultaneous, with semantic hubs central to the network structure activating before modality-preferential areas carrying semantic information."
}

@article{WENNEKERS200616,
title = "Language models based on Hebbian cell assemblies",
journal = "Journal of Physiology-Paris",
volume = "100",
number = "1",
pages = "16 - 30",
year = "2006",
note = "Theoretical and Computational Neuroscience: Understanding Brain Functions",
issn = "0928-4257",
doi = "https://doi.org/10.1016/j.jphysparis.2006.09.007",
url = "http://www.sciencedirect.com/science/article/pii/S0928425706000581",
author = "Thomas Wennekers and Max Garagnani and Friedemann Pulvermüller",
keywords = "Neo-cortex, Hebbian cell assembly, Associative memory, Attractor network, Synfire chain, Cortical micro-circuit, Language model",
abstract = "This paper demonstrates how associative neural networks as standard models for Hebbian cell assemblies can be extended to implement language processes in large-scale brain simulations. To this end the classical auto- and hetero-associative paradigms of attractor nets and synfire chains (SFCs) are combined and complemented by conditioned associations as a third principle which allows for the implementation of complex graph-like transition structures between assemblies. We show example simulations of a multiple area network for object-naming, which categorises objects in a visual hierarchy and generates different specific syntactic motor sequences (“words”) in response. The formation of cell assemblies due to ongoing plasticity in a multiple area network for word learning is studied afterwards. Simulations show how assemblies can form by means of percolating activity across auditory and motor-related language areas, a process supported by rhythmic, synchronized propagating waves through the network. Simulations further reproduce differences in own EEG&MEG experiments between responses to word- versus non-word stimuli in human subjects."
}

@article{doi:10.1002/1097-4679(195007)6:3<307::AID-JCLP2270060338>3.0.CO;2-K,
title = {Organization of behavior},
author = {Hebb, D. O.},
journal = {Journal of Clinical Psychology},
volume = {6},
number = {3},
pages = {307-307},
doi = {10.1002/1097-4679(195007)6:3<307::AID-JCLP2270060338>3.0.CO;2-K},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/1097-4679%28195007%296%3A3%3C307%3A%3AAID-JCLP2270060338%3E3.0.CO%3B2-K},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/1097-4679%28195007%296%3A3%3C307%3A%3AAID-JCLP2270060338%3E3.0.CO%3B2-K},
year = {1950}
}

@article{rabovsky_modelling_2018,
	title = {Modelling the {N}400 brain potential as change in a probabilistic representation of meaning},
	volume = {2},
	copyright = {2018 The Author(s)},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-018-0406-4},
	doi = {10.1038/s41562-018-0406-4},
	abstract = {The N400 evoked potential is a window to meaning in the brain, but it remains incompletely understood. The authors provide a unified explanation of the N400 in a neural network model that avoids the commitments of traditional approaches to meaning in language.},
	number = {9},
	urldate = {2019-08-03},
	journal = {Nature Human Behaviour},
	author = {Rabovsky, Milena and Hansen, Steven S. and McClelland, James L.},
	month = sep,
	year = {2018},
	pages = {693--705},
	file = {Snapshot:/home/dario/Zotero/storage/CX8ZK3N3/s41562-018-0406-4.html:text/html}
}

@article{Dominey2009NeuralNP,
  title={Neural network processing of natural language: II. Towards a unified model of corticostriatal function in learning sentence comprehension and non-linguistic sequencing},
  author={Peter Ford Dominey and Toshio Inui and Michel Hoen},
  journal={Brain and Language},
  year={2009},
  volume={109},
  pages={80-92}
}

@article{michalon_meaning-driven_2019,
	title = {Meaning-driven syntactic predictions in a parallel processing architecture: {Theory} and algorithmic modeling of {ERP} effects},
	volume = {131},
	issn = {0028-3932},
	url = {http://www.sciencedirect.com/science/article/pii/S0028393219301162},
	doi = {https://doi.org/10.1016/j.neuropsychologia.2019.05.009},
	abstract = {Syntactic and semantic information processing can interact selectively during language comprehension. However, the nature and extent of the interactions, in particular of semantic effects on syntax, remain to some extent elusive. We revisit an influential ERP result by Kim and Osterhout (2005), later replicated by Kim and Sikos (2011), that the verb in sentences such as ‘The hearty meal was devouring … ’ evokes a P600 effect—a signature of syntactic processing difficulty—even though all stimuli were grammatically well-formed. We view this effect as a manifestation of a conflict in the assignment of grammatical subject and object roles to the verb's arguments as performed independently by a semantic system (predicting that meal should be the object) and by a syntactic system (labeling meal as the subject). More specifically, we develop an explicit algorithmic implementation of a parallel processing architecture that supports (i) meaning-based prediction of grammatical role labels, using either a probabilistic label guesser or a neural network, and (ii) comparison of the predicted labels with labels assigned by a state-of-the-art dependency parser. We demonstrate that the system can classify sentences from the Kim and Osterhout (2005) corpus with adequate accuracy, and can detect labeling conflicts as intended. Some implications of our results for models of prediction in language processing are discussed.},
	journal = {Neuropsychologia},
	author = {Michalon, Olivier and Baggio, Giosuè},
	year = {2019},
	keywords = {ERP, NLP, P600, Parallel architecture, Parsing, Prediction, Semantics, Syntax},
	pages = {171 -- 183}
}

@article{STJOHN1990217,
title = "Learning and applying contextual constraints in sentence comprehension",
journal = "Artificial Intelligence",
volume = "46",
number = "1",
pages = "217 - 257",
year = "1990",
issn = "0004-3702",
doi = "https://doi.org/10.1016/0004-3702(90)90008-N",
url = "http://www.sciencedirect.com/science/article/pii/000437029090008N",
author = "Mark F. St. John and James L. McClelland",
abstract = "A parallel distributed processing model is described that learns to comprehend single clause sentences. Specifically, it assigns thematic roles to sentence constituents, disambiguates ambiguous words, instantiates vague words, and elaborates implied roles. The sentences are pre-segmented into constituent phrases. Each constituent is processed in turn to update an evolving representation of the event described by the sentence. The model uses the information derived from each constituent to revise its ongoing interpretation of the sentence and to anticipate additional constituents. The network learns to perform these tasks through practice on processing example sentence/event pairs. The learning procedure allows the model to take a statistical approach to solving the bootstrapping problem of learning the syntax and semantics of a language from the same data. The model performs very well on the corpus of sentences on which it was trained, and generalizes to sentences on which it was not trained, but learns slowly."
}

@article{gadagkar_dopamine_2016,
	title = {Dopamine neurons encode performance error in singing birds},
	volume = {354},
	issn = {0036-8075},
	url = {https://science.sciencemag.org/content/354/6317/1278},
	doi = {10.1126/science.aah6837},
	abstract = {How do birds know that a song that they hear is from a member of their own species, and how do they learn their songs in the first place? Araki et al. identified two types of brain cells involved in how finches learn their songs (see the Perspective by Tchernichovski and Lipkind). When zebra finches were raised by Bengalese finch foster parents, they learned a song whose morphology resembled that of their foster father. However, the temporal structure remained zebra finch–specific, suggesting that it is innate. Gadagkar et al. recorded activity in specific dopamine neurons in singing zebra finches while controlling perceived song quality with distorted auditory feedback. This distorted feedback represented worse performance than predicted and resulted in negative prediction errors. These findings suggest again that finches have an innate internal goal for their learned songs.Science, this issue p. 1282, p. 1234; see also p. 1278Many behaviors are learned through trial and error by matching performance to internal goals. Yet neural mechanisms of performance evaluation remain poorly understood. We recorded basal ganglia–projecting dopamine neurons in singing zebra finches as we controlled perceived song quality with distorted auditory feedback. Dopamine activity was phasically suppressed after distorted syllables, consistent with a worse-than-predicted outcome, and was phasically activated at the precise moment of the song when a predicted distortion did not occur, consistent with a better-than-predicted outcome. Error response magnitude depended on distortion probability. Thus, dopaminergic error signals can evaluate behaviors that are not learned for reward and are instead learned by matching performance outcomes to internal goals.},
	number = {6317},
	journal = {Science},
	author = {Gadagkar, Vikram and Puzerey, Pavel A. and Chen, Ruidong and Baird-Daniel, Eliza and Farhang, Alexander R. and Goldberg, Jesse H.},
	year = {2016},
	pages = {1278--1282}
}

@article{DBLP:journals/corr/abs-1810-04805,
  author    = {Jacob Devlin and
               Ming{-}Wei Chang and
               Kenton Lee and
               Kristina Toutanova},
  title     = {{BERT:} Pre-training of Deep Bidirectional Transformers for Language
               Understanding},
  journal   = {CoRR},
  volume    = {abs/1810.04805},
  year      = {2018},
  url       = {http://arxiv.org/abs/1810.04805},
  archivePrefix = {arXiv},
  eprint    = {1810.04805},
  timestamp = {Tue, 30 Oct 2018 20:39:56 +0100},
  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1810-04805},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@article{radford_language_nodate,
	title = {Language {Models} are {Unsupervised} {Multitask} {Learners}},
	abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspeciﬁc datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underﬁts WebText. Samples from the model reﬂect these improvements and contain coherent paragraphs of text. These ﬁndings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
	author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	pages = {24},
	file = {Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:/home/dario/Zotero/storage/MWA4YYIF/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf:application/pdf},
	year = {2019}
}

@article{DBLP:journals/corr/VaswaniSPUJGKP17,
  author    = {Ashish Vaswani and
               Noam Shazeer and
               Niki Parmar and
               Jakob Uszkoreit and
               Llion Jones and
               Aidan N. Gomez and
               Lukasz Kaiser and
               Illia Polosukhin},
  title     = {Attention Is All You Need},
  journal   = {CoRR},
  volume    = {abs/1706.03762},
  year      = {2017},
  url       = {http://arxiv.org/abs/1706.03762},
  archivePrefix = {arXiv},
  eprint    = {1706.03762},
  timestamp = {Mon, 13 Aug 2018 16:48:37 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/VaswaniSPUJGKP17},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{doi:10.1080/15475441003769411,
author = { Caroline   F.   Rowland  and  Claire   L.   Noble },
title = {The Role of Syntactic Structure in Children's Sentence Comprehension: Evidence From the Dative},
journal = {Language Learning and Development},
volume = {7},
number = {1},
pages = {55-75},
year  = {2010},
publisher = {Routledge},
doi = {10.1080/15475441003769411},

URL = { 
        https://doi.org/10.1080/15475441003769411
    
},
eprint = { 
        https://doi.org/10.1080/15475441003769411
    
}

}

@book{skinner1957verbal,
  title={Verbal behavior},
  author={Skinner, Burrhus Frederic},
  year={1957},
  publisher={New York: Appleton-Century-Crofts}
}

@article{doi:10.1111/j.1460-6984.2011.00086.x,
author = {Van Horne, Amanda J. Owen},
title = {Child Language Acquisition: Contrasting Theoretical Approaches by Ben Ambridge and Elena V. M. Lieven},
journal = {International Journal of Language \& Communication Disorders},
volume = {47},
number = {1},
pages = {112-114},
doi = {10.1111/j.1460-6984.2011.00086.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1460-6984.2011.00086.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1460-6984.2011.00086.x},
year = {2012}
}

@inbook{10.2307/j.ctt17kk81z.1,
 ISBN = {9780262527408},
 URL = {http://www.jstor.org/stable/j.ctt17kk81z.1},
 author = {Noam Chomsky},
 booktitle = {Aspects of the Theory of Syntax},
 edition = {50},
 pages = {i--vi},
 publisher = {The MIT Press},
 title = {Front Matter},
 year = {1965}
}

@article{pine_conti-ramsden_joseph_lieven_serratrice_2008, title={Tense over time: testing the Agreement/Tense Omission Model as an account of the pattern of tense-marking provision in early child English}, volume={35}, DOI={10.1017/S0305000907008252}, number={1}, journal={Journal of Child Language}, publisher={Cambridge University Press}, author={PINE, JULIAN M. and CONTI-RAMSDEN, GINA and JOSEPH, KATE L. and LIEVEN, ELENA V. M. and SERRATRICE, LUDOVICA}, year={2008}, pages={55–75}}

@article{theakston_lieven_2005, title={The acquisition of auxiliaries BE and HAVE: an elicitation study*}, volume={32}, DOI={10.1017/S0305000905006872}, number={3}, journal={Journal of Child Language}, publisher={Cambridge University Press}, author={THEAKSTON, ANNA L. and LIEVEN, ELENA V. M.}, year={2005}, pages={587–616}}



@book{bishop2007,
  added-at = {2009-06-02T09:46:22.000+0200},
  asin = {0387310738},
  author = {Bishop, Christopher M.},
  biburl = {https://www.bibsonomy.org/bibtex/2d21de30a3a67c0f9f3c96bd6eec3267a/midtiby},
  description = {Amazon.com: Pattern Recognition and Machine Learning (Information Science and Statistics): Christopher M. Bishop: Books},
  dewey = {006.4},
  ean = {9780387310732},
  edition = 1,
  interhash = {f6fec2ccd82dec0dcd63825e301662cf},
  intrahash = {d21de30a3a67c0f9f3c96bd6eec3267a},
  isbn = {0387310738},
  keywords = {algorithms machinelearning patternrecognition statistics},
  publisher = {Springer},
  timestamp = {2009-06-02T15:22:29.000+0200},
  title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
  url = {http://www.amazon.com/Pattern-Recognition-Learning-Information-Statistics/dp/0387310738%3FSubscriptionId%3D13CT5CVB80YFWJEPWS02%26tag%3Dws%26linkCode%3Dxm2%26camp%3D2025%26creative%3D165953%26creativeASIN%3D0387310738},
  year = 2007
}

@book{dudaHart1973,
  added-at = {2011-09-14T15:29:26.000+0200},
  address = {New Yotk},
  author = {Duda, R. O. and Hart, P. E.},
  biburl = {https://www.bibsonomy.org/bibtex/2098812b16c7081c0db102c1a37615dd3/jil},
  interhash = {b67a0d8f706c8cec908d83df62d3cf2f},
  intrahash = {098812b16c7081c0db102c1a37615dd3},
  keywords = {book classic k k-means kmeans means},
  publisher = {John Willey \& Sons},
  timestamp = {2013-11-23T20:11:51.000+0100},
  title = {Pattern Classification and Scene Analysis},
  year = 1973
}

